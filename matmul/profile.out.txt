/usr/local/cuda/bin/ncu --set full ./bin/matmul
Running in PROFILING mode (1 iteration, no warmup, no verification).
==PROF== Connected to process 2981867 (/home/ahmads/github/cuda_study/matmul/bin/matmul)
==WARNING== Unable to access the following 6 metrics: ctc__rx_bytes_data_user.sum, ctc__rx_bytes_data_user.sum.pct_of_peak_sustained_elapsed, ctc__rx_bytes_data_user.sum.per_second, ctc__tx_bytes_data_user.sum, ctc__tx_bytes_data_user.sum.pct_of_peak_sustained_elapsed, ctc__tx_bytes_data_user.sum.per_second.


==PROF== Profiling "matmul_naive" - 0: 0%
==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.
....50%....100% - 39 passes
==PROF== Profiling "matmul_tiled" - 1: 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_tiled_reg" - 2: 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_tiled_vec" - 3: 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_tiled_pipe" - 4: 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_simple" - 5: 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_smem" - 6: 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_smem" - 7: 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_wgmma" - 8: 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_wgmma_pipe" - 9: 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_wgmma_pipe" - 10: 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_wgmma_tma" - 11: 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_wgmma_tma" - 12: 0%....50%....100% - 39 passes
==PROF== Profiling "device_kernel" - 13: 0%....50%....100% - 39 passes
==PROF== Profiling "Kernel2" - 14: 0%....50%....100% - 39 passes
==PROF== Profiling "sm90_xmma_gemm_f32f32_tf32f32..." - 15: 0%....50%....100% - 39 passes
==PROF== Disconnected from process 2981867
[2981867] matmul@127.0.0.1
  matmul_naive(const float *, const float *, float *, int, int, int) (512, 256, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- --------------
    Metric Name             Metric Unit   Metric Value
    ----------------------- ----------- --------------
    DRAM Frequency                  Ghz           1.59
    SM Frequency                    Ghz           1.35
    Elapsed Cycles                cycle    399,893,383
    Memory Throughput                 %          97.65
    DRAM Throughput                   %          18.98
    Duration                         ms         296.22
    L1/TEX Cache Throughput           %          97.73
    L2 Cache Throughput               %          21.64
    SM Active Cycles              cycle 399,558,502.81
    Compute (SM) Throughput           %          65.10
    ----------------------- ----------- --------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.
          Start by analyzing L1 in the Memory Workload Analysis section.

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 8%
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline
          analysis.

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        93.00
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           96
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.49
    Executed Ipc Elapsed  inst/cycle         1.49
    Issue Slots Busy               %        37.18
    Issued Ipc Active     inst/cycle         1.49
    SM Busy                        %        37.18
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (25.5%) based on active cycles, taking into account the rates of its
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)
          operations. It is well-utilized, but should not be a bottleneck.

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       464.44
    Mem Busy                               %        97.65
    Max Bandwidth                          %        65.99
    L1/TEX Hit Rate                        %        87.54
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    5,352,422
    L2 Hit Rate                            %        39.99
    Mem Pipes Busy                         %        65.10
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 18.95%
          Out of the 171277504.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To
          increase this success rate, consider marking only those memory regions as compressible that contain the most
          zero values and/or expose the most homogeneous values.

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 42.72%
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between
          threads. Check the Source Counters section for uncoalesced global loads.

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        37.18
    Issued Warp Per Scheduler                        0.37
    No Eligible                            %        62.82
    Active Warps Per Scheduler          warp        15.96
    Eligible Warps Per Scheduler        warp         1.45
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 2.348%
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only
          issues an instruction every 2.7 cycles. This might leave hardware resources underutilized and may lead to
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average
          of 15.96 active warps per scheduler, but only an average of 1.45 warps were eligible per cycle. Eligible
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        42.92
    Warp Cycles Per Executed Instruction           cycle        42.92
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    32.00
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 2.348%
          On average, each warp of this workload spends 28.1 cycles being stalled waiting for a scoreboard dependency
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently
          used data to shared memory. This stall type represents about 65.6% of the total average of 42.9 cycles
          between issuing two instructions.
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on
          sampling data. The Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details
          on each stall reason.

    Section: Instruction Statistics
    ---------------------------------------- ----------- --------------
    Metric Name                              Metric Unit   Metric Value
    ---------------------------------------- ----------- --------------
    Avg. Executed Instructions Per Scheduler        inst 148,550,252.61
    Executed Instructions                           inst 78,434,533,376
    Avg. Issued Instructions Per Scheduler          inst 148,550,280.54
    Issued Instructions                             inst 78,434,548,123
    ---------------------------------------- ----------- --------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                131,072
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread      33,554,432
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                              124.12
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.75
    Achieved Active Warps Per SM           warp        63.84
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ---------------
    Metric Name                Metric Unit    Metric Value
    -------------------------- ----------- ---------------
    Average DRAM Active Cycles       cycle      89,566,590
    Total DRAM Elapsed Cycles        cycle  22,649,972,224
    Average L1 Active Cycles         cycle  399,558,502.81
    Total L1 Elapsed Cycles          cycle  52,785,862,904
    Average L2 Active Cycles         cycle  399,761,410.65
    Total L2 Elapsed Cycles          cycle  38,384,454,240
    Average SM Active Cycles         cycle  399,558,502.81
    Total SM Elapsed Cycles          cycle  52,785,862,904
    Average SMSP Active Cycles       cycle  399,548,693.58
    Total SMSP Elapsed Cycles        cycle 211,143,451,616
    -------------------------- ----------- ---------------

    Section: Source Counters
    ------------------------- ----------- -------------
    Metric Name               Metric Unit  Metric Value
    ------------------------- ----------- -------------
    Branch Instructions Ratio           %          0.01
    Branch Instructions              inst 1,083,179,008
    Branch Efficiency                   %           100
    Avg. Divergent Branches                           0
    ------------------------- ----------- -------------

  void matmul_tiled<32>(const float *, const float *, float *, int, int, int) (256, 128, 1)x(32, 32, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- --------------
    Metric Name             Metric Unit   Metric Value
    ----------------------- ----------- --------------
    DRAM Frequency                  Ghz           1.59
    SM Frequency                    Ghz           1.35
    Elapsed Cycles                cycle    238,654,941
    Memory Throughput                 %          89.34
    DRAM Throughput                   %          16.12
    Duration                         ms         176.78
    L1/TEX Cache Throughput           %          89.59
    L2 Cache Throughput               %          14.15
    SM Active Cycles              cycle 237,962,531.30
    Compute (SM) Throughput           %          75.00
    ----------------------- ----------- --------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.
          Start by analyzing L1 in the Memory Workload Analysis section.

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 14%
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline
          analysis.

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       111.21
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           48
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.63
    Executed Ipc Elapsed  inst/cycle         1.62
    Issue Slots Busy               %        40.63
    Issued Ipc Active     inst/cycle         1.63
    SM Busy                        %        41.04
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.61%
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       394.41
    Mem Busy                               %        89.34
    Max Bandwidth                          %        85.22
    L1/TEX Hit Rate                        %         0.00
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    4,274,807
    L2 Hit Rate                            %        38.81
    Mem Pipes Busy                         %        75.00
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 16.07%
          Out of the 136793824.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To
          increase this success rate, consider marking only those memory regions as compressible that contain the most
          zero values and/or expose the most homogeneous values.

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.29%
          The memory access pattern for shared stores might not be optimal and causes on average a 1.1 - way bank
          conflict across all 1073741824 shared store requests.This results in 154748114 bank conflicts,  which
          represent 12.60% of the overall 1228489938 wavefronts for shared stores. Check the Source Counters section
          for uncoalesced shared stores.

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        40.63
    Issued Warp Per Scheduler                        0.41
    No Eligible                            %        59.37
    Active Warps Per Scheduler          warp        15.97
    Eligible Warps Per Scheduler        warp         3.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.66%
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only
          issues an instruction every 2.5 cycles. This might leave hardware resources underutilized and may lead to
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average
          of 15.97 active warps per scheduler, but only an average of 3.07 warps were eligible per cycle. Eligible
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        39.30
    Warp Cycles Per Executed Instruction           cycle        39.30
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    32.00
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.66%
          On average, each warp of this workload spends 19.0 cycles being stalled waiting for the MIO (memory
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline
          pressure. This stall type represents about 48.3% of the total average of 39.3 cycles between issuing two
          instructions.
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on
          sampling data. The Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details
          on each stall reason.

    Section: Instruction Statistics
    ---------------------------------------- ----------- --------------
    Metric Name                              Metric Unit   Metric Value
    ---------------------------------------- ----------- --------------
    Avg. Executed Instructions Per Scheduler        inst  96,675,529.70
    Executed Instructions                           inst 51,044,679,680
    Avg. Issued Instructions Per Scheduler          inst  96,675,552.92
    Issued Instructions                             inst 51,044,691,941
    ---------------------------------------- ----------- --------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                 1,024
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 32,768
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            8.19
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread      33,554,432
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                              124.12
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.79
    Achieved Active Warps Per SM           warp        63.87
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ---------------
    Metric Name                Metric Unit    Metric Value
    -------------------------- ----------- ---------------
    Average DRAM Active Cycles       cycle   45,393,079.83
    Total DRAM Elapsed Cycles        cycle  13,517,422,080
    Average L1 Active Cycles         cycle  237,962,531.30
    Total L1 Elapsed Cycles          cycle  31,500,907,504
    Average L2 Active Cycles         cycle  238,368,088.64
    Total L2 Elapsed Cycles          cycle  22,893,503,424
    Average SM Active Cycles         cycle  237,962,531.30
    Total SM Elapsed Cycles          cycle  31,500,907,504
    Average SMSP Active Cycles       cycle  237,963,820.14
    Total SMSP Elapsed Cycles        cycle 126,003,630,016
    -------------------------- ----------- ---------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst  540,016,640
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void matmul_tiled_reg<128, 128, 8, 8, 8>(const float *, const float *, float *, int, int, int) (64, 32, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          1.59
    SM Frequency                    Ghz          1.35
    Elapsed Cycles                cycle    91,388,803
    Memory Throughput                 %         52.75
    DRAM Throughput                   %          5.51
    Duration                         ms         67.70
    L1/TEX Cache Throughput           %         54.47
    L2 Cache Throughput               %          8.35
    SM Active Cycles              cycle 88,545,995.43
    Compute (SM) Throughput           %         43.24
    ----------------------- ----------- -------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential
          reasons.

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 36%
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline
          analysis.

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        86.11
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           24
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.79
    Executed Ipc Elapsed  inst/cycle         1.73
    Issue Slots Busy               %        44.66
    Issued Ipc Active     inst/cycle         1.79
    SM Busy                        %        44.66
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (38.5%) based on active cycles, taking into account the rates of its
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)
          operations. It is well-utilized, but should not be a bottleneck.

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       134.72
    Mem Busy                               %        52.75
    Max Bandwidth                          %        28.91
    L1/TEX Hit Rate                        %         0.27
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector   33,429,481
    L2 Hit Rate                            %        61.51
    Mem Pipes Busy                         %        13.35
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 5.28%
          Out of the 1069743392.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To
          increase this success rate, consider marking only those memory regions as compressible that contain the most
          zero values and/or expose the most homogeneous values.

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 46.16%
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.0 of the 32
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between
          threads. Check the Source Counters section for uncoalesced global stores.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.79%
          The memory access pattern for shared loads might not be optimal and causes on average a 5.0 - way bank
          conflict across all 1073741824 shared load requests.This results in 2147483648 bank conflicts,  which
          represent 40.00% of the overall 5368709120 wavefronts for shared loads. Check the Source Counters section
          for uncoalesced shared loads.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.058%
          The memory access pattern for shared stores might not be optimal and causes on average a 1.1 - way bank
          conflict across all 268435456 shared store requests.This results in 33589764 bank conflicts,  which
          represent 11.12% of the overall 302025220 wavefronts for shared stores. Check the Source Counters section
          for uncoalesced shared stores.

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.64
    Issued Warp Per Scheduler                        0.45
    No Eligible                            %        55.36
    Active Warps Per Scheduler          warp         2.00
    Eligible Warps Per Scheduler        warp         0.72
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.25%
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only
          issues an instruction every 2.2 cycles. This might leave hardware resources underutilized and may lead to
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average
          of 2.00 active warps per scheduler, but only an average of 0.72 warps were eligible per cycle. Eligible
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         4.48
    Warp Cycles Per Executed Instruction           cycle         4.48
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    32.00
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- --------------
    Metric Name                              Metric Unit   Metric Value
    ---------------------------------------- ----------- --------------
    Avg. Executed Instructions Per Scheduler        inst  39,541,791.03
    Executed Instructions                           inst 20,878,065,664
    Avg. Issued Instructions Per Scheduler          inst  39,541,801.03
    Issued Instructions                             inst 20,878,070,944
    ---------------------------------------- ----------- --------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             162
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            8.19
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         524,288
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               15.52
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.50
    Achieved Active Warps Per SM           warp         8.00
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 47.25%
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required
          registers, and the required amount of shared memory.

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle   5,937,326.17
    Total DRAM Elapsed Cycles        cycle  5,176,271,360
    Average L1 Active Cycles         cycle  88,545,995.43
    Total L1 Elapsed Cycles          cycle 12,069,721,684
    Average L2 Active Cycles         cycle  87,637,434.12
    Total L2 Elapsed Cycles          cycle  8,760,200,544
    Average SM Active Cycles         cycle  88,545,995.43
    Total SM Elapsed Cycles          cycle 12,069,721,684
    Average SMSP Active Cycles       cycle  88,586,894.56
    Total SMSP Elapsed Cycles        cycle 48,278,886,736
    -------------------------- ----------- --------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.00
    Branch Instructions              inst   33,849,344
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 2.546%
          This kernel has uncoalesced global accesses resulting in a total of 29360128 excessive sectors (3% of the
          total 1107296256 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional
          information on reducing uncoalesced device memory accesses.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 36.89%
          This kernel has uncoalesced shared accesses resulting in a total of 2147483648 excessive wavefronts (38% of
          the total 5637144576 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source
          locations. The CUDA Best Practices Guide
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.

  void matmul_tiled_vec<128, 128, 8, 8, 8>(const float *, const float *, float *, int, int, int) (64, 32, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          1.59
    SM Frequency                    Ghz          1.35
    Elapsed Cycles                cycle    67,729,294
    Memory Throughput                 %         74.02
    DRAM Throughput                   %          8.81
    Duration                         ms         50.17
    L1/TEX Cache Throughput           %         76.22
    L2 Cache Throughput               %         12.04
    SM Active Cycles              cycle 65,828,406.60
    Compute (SM) Throughput           %         53.93
    ----------------------- ----------- -------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L1
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or
          whether there are values you can (re)compute.

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 48%
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline
          analysis.

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       128.19
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           12
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.22
    Executed Ipc Elapsed  inst/cycle         2.16
    Issue Slots Busy               %        55.53
    Issued Ipc Active     inst/cycle         2.22
    SM Busy                        %        55.53
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (50.0%) based on active cycles, taking into account the rates of its
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)
          operations. It is well-utilized, but should not be a bottleneck.

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       215.49
    Mem Busy                               %        74.02
    Max Bandwidth                          %        39.06
    L1/TEX Hit Rate                        %         1.46
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector   21,099,666
    L2 Hit Rate                            %        57.73
    Mem Pipes Busy                         %        14.26
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 8.556%
          Out of the 675189312.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To
          increase this success rate, consider marking only those memory regions as compressible that contain the most
          zero values and/or expose the most homogeneous values.

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 64.77%
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.0 of the 32
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between
          threads. Check the Source Counters section for uncoalesced global stores.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 30.49%
          The memory access pattern for shared loads might not be optimal and causes on average a 5.0 - way bank
          conflict across all 1073741824 shared load requests.This results in 2147607969 bank conflicts,  which
          represent 40.00% of the overall 5368833441 wavefronts for shared loads. Check the Source Counters section
          for uncoalesced shared loads.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 39.6%
          The memory access pattern for shared stores might not be optimal and causes on average a 4.2 - way bank
          conflict across all 134217728 shared store requests.This results in 290303861 bank conflicts,  which
          represent 51.96% of the overall 558739317 wavefronts for shared stores. Check the Source Counters section
          for uncoalesced shared stores.

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        55.53
    Issued Warp Per Scheduler                        0.56
    No Eligible                            %        44.47
    Active Warps Per Scheduler          warp         3.90
    Eligible Warps Per Scheduler        warp         1.31
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.98%
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average
          of 3.90 active warps per scheduler, but only an average of 1.31 warps were eligible per cycle. Eligible
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         7.03
    Warp Cycles Per Executed Instruction           cycle         7.03
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    32.00
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- --------------
    Metric Name                              Metric Unit   Metric Value
    ---------------------------------------- ----------- --------------
    Avg. Executed Instructions Per Scheduler        inst  36,552,207.52
    Executed Instructions                           inst 19,299,565,568
    Avg. Issued Instructions Per Scheduler          inst  36,552,226.52
    Issued Instructions                             inst 19,299,575,600
    ---------------------------------------- ----------- --------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             128
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           10.37
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         524,288
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                7.76
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            5
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.29
    Achieved Active Warps Per SM           warp        15.54
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.98%
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required
          registers.

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle   7,038,487.83
    Total DRAM Elapsed Cycles        cycle  3,836,203,008
    Average L1 Active Cycles         cycle  65,828,406.60
    Total L1 Elapsed Cycles          cycle  8,947,238,114
    Average L2 Active Cycles         cycle  67,219,614.95
    Total L2 Elapsed Cycles          cycle  6,496,081,536
    Average SM Active Cycles         cycle  65,828,406.60
    Total SM Elapsed Cycles          cycle  8,947,238,114
    Average SMSP Active Cycles       cycle  65,823,986.13
    Total SMSP Elapsed Cycles        cycle 35,788,952,456
    -------------------------- ----------- --------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst  168,067,072
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 2.634%
          This kernel has uncoalesced global accesses resulting in a total of 29360128 excessive sectors (3% of the
          total 1107296256 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional
          information on reducing uncoalesced device memory accesses.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 39.73%
          This kernel has uncoalesced shared accesses resulting in a total of 2415919104 excessive wavefronts (41% of
          the total 5905580032 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source
          locations. The CUDA Best Practices Guide
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.

  void matmul_tiled_pipe<128, 128, 8, 8, 8>(const float *, const float *, float *, int, int, int) (64, 32, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          1.59
    SM Frequency                    Ghz          1.35
    Elapsed Cycles                cycle    98,999,424
    Memory Throughput                 %         49.58
    DRAM Throughput                   %          5.08
    Duration                         ms         73.35
    L1/TEX Cache Throughput           %         51.12
    L2 Cache Throughput               %          7.57
    SM Active Cycles              cycle 95,891,216.24
    Compute (SM) Throughput           %         37.55
    ----------------------- ----------- -------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential
          reasons.

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 33%
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline
          analysis.

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        93.13
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           24
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.55
    Executed Ipc Elapsed  inst/cycle         1.50
    Issue Slots Busy               %        38.71
    Issued Ipc Active     inst/cycle         1.55
    SM Busy                        %        38.71
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.5%) based on active cycles, taking into account the rates of its
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)
          operations. It is well-utilized, but should not be a bottleneck.

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       124.34
    Mem Busy                               %        49.58
    Max Bandwidth                          %        26.77
    L1/TEX Hit Rate                        %         0.19
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector   33,568,564
    L2 Hit Rate                            %        50.76
    Mem Pipes Busy                         %         9.26
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 4.839%
          Out of the 1074194048.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To
          increase this success rate, consider marking only those memory regions as compressible that contain the most
          zero values and/or expose the most homogeneous values.

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 43.38%
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.0 of the 32
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between
          threads. Check the Source Counters section for uncoalesced global stores.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.45%
          The memory access pattern for shared loads might not be optimal and causes on average a 5.0 - way bank
          conflict across all 1073741824 shared load requests.This results in 2147483648 bank conflicts,  which
          represent 40.00% of the overall 5368709120 wavefronts for shared loads. Check the Source Counters section
          for uncoalesced shared loads.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18%
          The memory access pattern for shared stores might not be optimal and causes on average a 6.2 - way bank
          conflict across all 67108864 shared store requests.This results in 145853455 bank conflicts,  which
          represent 35.21% of the overall 414288911 wavefronts for shared stores. Check the Source Counters section
          for uncoalesced shared stores.

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        38.73
    Issued Warp Per Scheduler                        0.39
    No Eligible                            %        61.27
    Active Warps Per Scheduler          warp         2.00
    Eligible Warps Per Scheduler        warp         0.60
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.42%
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only
          issues an instruction every 2.6 cycles. This might leave hardware resources underutilized and may lead to
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average
          of 2.00 active warps per scheduler, but only an average of 0.60 warps were eligible per cycle. Eligible
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         5.16
    Warp Cycles Per Executed Instruction           cycle         5.16
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.94
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 37.34%
          On average, each warp of this workload spends 1.9 cycles being stalled waiting for a scoreboard dependency on
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently
          used data to shared memory. This stall type represents about 37.3% of the total average of 5.2 cycles
          between issuing two instructions.
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on
          sampling data. The Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details
          on each stall reason.

    Section: Instruction Statistics
    ---------------------------------------- ----------- --------------
    Metric Name                              Metric Unit   Metric Value
    ---------------------------------------- ----------- --------------
    Avg. Executed Instructions Per Scheduler        inst  37,123,940.85
    Executed Instructions                           inst 19,601,440,768
    Avg. Issued Instructions Per Scheduler          inst  37,123,948.85
    Issued Instructions                             inst 19,601,444,992
    ---------------------------------------- ----------- --------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             130
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           20.74
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         524,288
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               15.52
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.50
    Achieved Active Warps Per SM           warp         8.00
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 50.42%
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required
          registers, and the required amount of shared memory.

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle      5,937,660
    Total DRAM Elapsed Cycles        cycle  5,608,435,712
    Average L1 Active Cycles         cycle  95,891,216.24
    Total L1 Elapsed Cycles          cycle 13,051,912,374
    Average L2 Active Cycles         cycle  47,167,228.80
    Total L2 Elapsed Cycles          cycle  9,412,888,896
    Average SM Active Cycles         cycle  95,891,216.24
    Total SM Elapsed Cycles          cycle 13,051,912,374
    Average SMSP Active Cycles       cycle  95,864,311.26
    Total SMSP Elapsed Cycles        cycle 52,207,649,496
    -------------------------- ----------- --------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst  201,621,504
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 1.276%
          This kernel has uncoalesced global accesses resulting in a total of 29360128 excessive sectors (3% of the
          total 1107296256 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional
          information on reducing uncoalesced device memory accesses.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 38.34%
          This kernel has uncoalesced shared accesses resulting in a total of 2281701376 excessive wavefronts (40% of
          the total 5771362304 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source
          locations. The CUDA Best Practices Guide
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.

  void matmul_cute_simple<128, 128, 8, cute::TiledMMA<cute::MMA_Atom<cute::SM80_16x8x8_F32TF32TF32F32_TN>, cute::Layout<cute::tuple<cute::C<2>, cute::C<2>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<2>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>>(const float *, const float *, float *, int, int, int, T4) (32, 64, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          1.59
    SM Frequency                    Ghz          1.24
    Elapsed Cycles                cycle    32,214,173
    Memory Throughput                 %         76.15
    DRAM Throughput                   %          4.40
    Duration                         ms         26.00
    L1/TEX Cache Throughput           %         82.37
    L2 Cache Throughput               %         30.42
    SM Active Cycles              cycle 29,704,740.42
    Compute (SM) Throughput           %         18.99
    ----------------------- ----------- -------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L1
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or
          whether there are values you can (re)compute.

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline
          analysis.

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       135.59
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            6
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.73
    Executed Ipc Elapsed  inst/cycle         0.67
    Issue Slots Busy               %        18.20
    Issued Ipc Active     inst/cycle         0.73
    SM Busy                        %        20.54
    -------------------- ----------- ------------

    INF   Shared is the highest-utilized pipeline (20.5%) based on active cycles, taking into account the rates of its
          different instructions. It is the logical sum of several other pipelines which can't achieve full
          utilization on their own. It executes 64-bit floating point and tensor operations. It's dominated by its
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       107.69
    Mem Busy                               %        76.15
    Max Bandwidth                          %        27.94
    L1/TEX Hit Rate                        %        67.59
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    6,996,488
    L2 Hit Rate                            %        91.84
    Mem Pipes Busy                         %        12.68
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 4.165%
          Out of the 223887616.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To
          increase this success rate, consider marking only those memory regions as compressible that contain the most
          zero values and/or expose the most homogeneous values.

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 25.38%
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 21.3 of the 32
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between
          threads. Check the Source Counters section for uncoalesced global loads.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 38.08%
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between
          threads. Check the Source Counters section for uncoalesced global stores.

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.25
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.75
    Active Warps Per Scheduler          warp         2.90
    Eligible Warps Per Scheduler        warp         0.22
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.85%
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average
          of 2.90 active warps per scheduler, but only an average of 0.22 warps were eligible per cycle. Eligible
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        15.91
    Warp Cycles Per Executed Instruction           cycle        15.91
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    32.00
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 23.85%
          On average, each warp of this workload spends 11.7 cycles being stalled waiting for a scoreboard dependency
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently
          used data to shared memory. This stall type represents about 73.3% of the total average of 15.9 cycles
          between issuing two instructions.
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on
          sampling data. The Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details
          on each stall reason.

    Section: Instruction Statistics
    ---------------------------------------- ----------- -------------
    Metric Name                              Metric Unit  Metric Value
    ---------------------------------------- ----------- -------------
    Avg. Executed Instructions Per Scheduler        inst  5,406,254.55
    Executed Instructions                           inst 2,854,502,400
    Avg. Issued Instructions Per Scheduler          inst  5,406,263.55
    Issued Instructions                             inst 2,854,507,152
    ---------------------------------------- ----------- -------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             166
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         262,144
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                5.17
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %        18.75
    Achieved Occupancy                        %        17.67
    Achieved Active Warps Per SM           warp        11.31
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 23.85%
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required
          registers.

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle   1,822,989.33
    Total DRAM Elapsed Cycles        cycle  1,988,184,064
    Average L1 Active Cycles         cycle  29,704,740.42
    Total L1 Elapsed Cycles          cycle  4,241,064,458
    Average L2 Active Cycles         cycle  32,389,139.97
    Total L2 Elapsed Cycles          cycle  3,352,475,040
    Average SM Active Cycles         cycle  29,704,740.42
    Total SM Elapsed Cycles          cycle  4,241,064,458
    Average SMSP Active Cycles       cycle  29,627,645.06
    Total SMSP Elapsed Cycles        cycle 16,964,257,832
    -------------------------- ----------- --------------

    OPT   Est. Speedup: 7.11%
          One or more SMs have a much higher number of active cycles than the average number of active cycles.
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.
          Maximum instance value is 7.69% above the average, while the minimum instance value is 8.80% below the
          average.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.384%
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active
          cycles. Maximum instance value is 8.01% above the average, while the minimum instance value is 8.53% below
          the average.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.11%
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active
          cycles. Maximum instance value is 7.69% above the average, while the minimum instance value is 8.80% below
          the average.

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst   16,793,600
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 30.96%
          This kernel has uncoalesced global accesses resulting in a total of 1077936128 excessive sectors (33% of the
          total 3229614080 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional
          information on reducing uncoalesced device memory accesses.

  void matmul_cute_smem<128, 128, 8, cute::TiledMMA<cute::MMA_Atom<cute::SM80_16x8x8_F32TF32TF32F32_TN>, cute::Layout<cute::tuple<cute::C<2>, cute::C<2>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<2>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<64>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<256>, cute::C<1>>, cute::C<64>>>, cute::tuple<cute::C<64>, cute::C<8>>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::C<128>, cute::C<4>>, cute::tuple<cute::C<4>, cute::C<1>>>, cute::tuple<cute::C<128>, cute::C<4>>>, cute::Layout<cute::tuple<cute::C<128>, cute::C<8>>, cute::tuple<cute::C<12>, cute::C<1>>>, cute::Layout<cute::tuple<cute::C<128>, cute::C<8>>, cute::tuple<cute::C<1>, cute::C<132>>>>(const float *, const float *, float *, int, int, int, T4, T5, T6, T7, T8) (32, 64, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          1.59
    SM Frequency                    Ghz          1.25
    Elapsed Cycles                cycle    43,067,306
    Memory Throughput                 %         86.89
    DRAM Throughput                   %         22.25
    Duration                         ms         34.48
    L1/TEX Cache Throughput           %         75.45
    L2 Cache Throughput               %         88.04
    SM Active Cycles              cycle 40,377,891.89
    Compute (SM) Throughput           %         31.36
    ----------------------- ----------- -------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.
          Start by analyzing L2 in the Memory Workload Analysis section.

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline
          analysis.

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        89.59
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           12
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.79
    Executed Ipc Elapsed  inst/cycle         0.74
    Issue Slots Busy               %        19.84
    Issued Ipc Active     inst/cycle         0.79
    SM Busy                        %        19.84
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.89%
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.

    Section: Memory Workload Analysis
    ---------------------------- ----------- -------------
    Metric Name                  Metric Unit  Metric Value
    ---------------------------- ----------- -------------
    Memory Throughput                Gbyte/s        544.44
    Mem Busy                               %         55.38
    Max Bandwidth                          %         86.89
    L1/TEX Hit Rate                        %         79.20
    L2 Compression Success Rate            %             0
    L2 Compression Ratio                   %             0
    L2 Compression Input Sectors      sector 3,227,839,100
    L2 Hit Rate                            %         93.08
    Mem Pipes Busy                         %         31.36
    ---------------------------- ----------- -------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 10.29%
          Out of the 103290851200.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To
          increase this success rate, consider marking only those memory regions as compressible that contain the most
          zero values and/or expose the most homogeneous values.

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 35.1%
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between
          threads. Check the Source Counters section for uncoalesced global stores.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 68%
          The memory access pattern for local loads from L1TEX might not be optimal. On average, only 1.0 of the 32
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between
          threads. Check the Source Counters section for uncoalesced local loads.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 68%
          The memory access pattern for local stores to L1TEX might not be optimal. On average, only 1.0 of the 32
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between
          threads. Check the Source Counters section for uncoalesced local stores.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 26.57%
          The memory access pattern for shared loads might not be optimal and causes on average a 1.5 - way bank
          conflict across all 536870912 shared load requests.This results in 291898721 bank conflicts,  which
          represent 35.22% of the overall 828769633 wavefronts for shared loads. Check the Source Counters section for
          uncoalesced shared loads.

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        19.84
    Issued Warp Per Scheduler                        0.20
    No Eligible                            %        80.16
    Active Warps Per Scheduler          warp         3.88
    Eligible Warps Per Scheduler        warp         0.24
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 13.11%
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only
          issues an instruction every 5.0 cycles. This might leave hardware resources underutilized and may lead to
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average
          of 3.88 active warps per scheduler, but only an average of 0.24 warps were eligible per cycle. Eligible
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.55
    Warp Cycles Per Executed Instruction           cycle        19.55
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.37
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 13.11%
          On average, each warp of this workload spends 9.6 cycles being stalled waiting for a scoreboard dependency on
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently
          used data to shared memory. This stall type represents about 49.0% of the total average of 19.5 cycles
          between issuing two instructions.
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on
          sampling data. The Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details
          on each stall reason.

    Section: Instruction Statistics
    ---------------------------------------- ----------- -------------
    Metric Name                              Metric Unit  Metric Value
    ---------------------------------------- ----------- -------------
    Avg. Executed Instructions Per Scheduler        inst  8,012,256.97
    Executed Instructions                           inst 4,230,471,680
    Avg. Issued Instructions Per Scheduler          inst  8,012,260.97
    Issued Instructions                             inst 4,230,473,792
    ---------------------------------------- ----------- -------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             128
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           10.34
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         262,144
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                3.88
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 465 thread blocks.
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for
          up to 25.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for
          more details on launch configurations.

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.21
    Achieved Active Warps Per SM           warp        15.50
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 13.11%
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required
          registers.

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle  12,220,928.33
    Total DRAM Elapsed Cycles        cycle  2,636,349,440
    Average L1 Active Cycles         cycle  40,377,891.89
    Total L1 Elapsed Cycles          cycle  5,728,794,900
    Average L2 Active Cycles         cycle  45,778,154.94
    Total L2 Elapsed Cycles          cycle  4,468,311,648
    Average SM Active Cycles         cycle  40,377,891.89
    Total SM Elapsed Cycles          cycle  5,728,794,900
    Average SMSP Active Cycles       cycle  40,385,777.03
    Total SMSP Elapsed Cycles        cycle 22,915,179,600
    -------------------------- ----------- --------------

    OPT   Est. Speedup: 6.22%
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum
          instance value is 6.69% above the average, while the minimum instance value is 15.01% below the average.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.216%
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum
          instance value is 6.68% above the average, while the minimum instance value is 12.32% below the average.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.22%
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.
          Maximum instance value is 6.69% above the average, while the minimum instance value is 15.01% below the
          average.

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.00
    Branch Instructions              inst   16,793,600
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 32.87%
          This kernel has uncoalesced global accesses resulting in a total of 541065216 excessive sectors (33% of the
          total 1619001344 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional
          information on reducing uncoalesced device memory accesses.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 54.87%
          This kernel has uncoalesced shared accesses resulting in a total of 1157627904 excessive wavefronts (59% of
          the total 1962934272 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source
          locations. The CUDA Best Practices Guide
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.

  void matmul_cute_smem<128, 128, 32, cute::TiledMMA<cute::MMA_Atom<cute::SM80_16x8x8_F32TF32TF32F32_TN>, cute::Layout<cute::tuple<cute::C<2>, cute::C<2>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<2>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<64>, cute::C<1>>, cute::C<16>>>, cute::tuple<cute::C<16>, cute::C<32>>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::C<128>, cute::C<4>>, cute::tuple<cute::C<4>, cute::C<1>>>, cute::tuple<cute::C<128>, cute::C<4>>>, cute::Layout<cute::tuple<cute::C<128>, cute::C<32>>, cute::tuple<cute::C<36>, cute::C<1>>>, cute::Layout<cute::tuple<cute::C<128>, cute::C<32>>, cute::tuple<cute::C<1>, cute::C<132>>>>(const float *, const float *, float *, int, int, int, T4, T5, T6, T7, T8) (32, 64, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          1.59
    SM Frequency                    Ghz          1.25
    Elapsed Cycles                cycle    55,354,793
    Memory Throughput                 %         73.34
    DRAM Throughput                   %         73.34
    Duration                         ms         44.36
    L1/TEX Cache Throughput           %         42.86
    L2 Cache Throughput               %         68.59
    SM Active Cycles              cycle 52,496,318.09
    Compute (SM) Throughput           %         20.85
    ----------------------- ----------- -------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or
          whether there are values you can (re)compute.

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline
          analysis.

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       114.62
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           12
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.60
    Executed Ipc Elapsed  inst/cycle         0.57
    Issue Slots Busy               %        14.88
    Issued Ipc Active     inst/cycle         0.60
    SM Busy                        %        14.88
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 88.38%
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.

    Section: Memory Workload Analysis
    ---------------------------- ----------- -------------
    Metric Name                  Metric Unit  Metric Value
    ---------------------------- ----------- -------------
    Memory Throughput                Tbyte/s          1.79
    Mem Busy                               %         42.45
    Max Bandwidth                          %         73.34
    L1/TEX Hit Rate                        %         30.91
    L2 Compression Success Rate            %             0
    L2 Compression Ratio                   %             0
    L2 Compression Input Sectors      sector 2,421,436,243
    L2 Hit Rate                            %         84.41
    Mem Pipes Busy                         %         20.85
    ---------------------------- ----------- -------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 16.83%
          Out of the 77485959776.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To
          increase this success rate, consider marking only those memory regions as compressible that contain the most
          zero values and/or expose the most homogeneous values.

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 20.44%
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between
          threads. Check the Source Counters section for uncoalesced global stores.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 39.6%
          The memory access pattern for local loads from L1TEX might not be optimal. On average, only 1.0 of the 32
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between
          threads. Check the Source Counters section for uncoalesced local loads.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 62.97%
          The memory access pattern for local stores to L2 might not be optimal. On average, only 1.0 of the 32 bytes
          transmitted per sector are utilized by each thread. This applies to the 94.8% of sectors missed in L1TEX.
          This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced
          local stores.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 15.83%
          The memory access pattern for shared loads might not be optimal and causes on average a 1.6 - way bank
          conflict across all 536870912 shared load requests.This results in 314271265 bank conflicts,  which
          represent 36.92% of the overall 851142177 wavefronts for shared loads. Check the Source Counters section for
          uncoalesced shared loads.

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        14.90
    Issued Warp Per Scheduler                        0.15
    No Eligible                            %        85.10
    Active Warps Per Scheduler          warp         3.88
    Eligible Warps Per Scheduler        warp         0.16
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.66%
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only
          issues an instruction every 6.7 cycles. This might leave hardware resources underutilized and may lead to
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average
          of 3.88 active warps per scheduler, but only an average of 0.16 warps were eligible per cycle. Eligible
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        26.07
    Warp Cycles Per Executed Instruction           cycle        26.07
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.41
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.66%
          On average, each warp of this workload spends 16.1 cycles being stalled waiting for a scoreboard dependency
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently
          used data to shared memory. This stall type represents about 61.8% of the total average of 26.1 cycles
          between issuing two instructions.
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on
          sampling data. The Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details
          on each stall reason.

    Section: Instruction Statistics
    ---------------------------------------- ----------- -------------
    Metric Name                              Metric Unit  Metric Value
    ---------------------------------------- ----------- -------------
    Avg. Executed Instructions Per Scheduler        inst  7,813,352.73
    Executed Instructions                           inst 4,125,450,240
    Avg. Issued Instructions Per Scheduler          inst  7,813,356.73
    Issued Instructions                             inst 4,125,452,352
    ---------------------------------------- ----------- -------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             128
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           35.30
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         262,144
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                3.88
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 465 thread blocks.
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for
          up to 25.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for
          more details on launch configurations.

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            4
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.27
    Achieved Active Warps Per SM           warp        15.54
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.66%
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required
          registers, and the required amount of shared memory.

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle  51,828,622.17
    Total DRAM Elapsed Cycles        cycle  3,392,280,832
    Average L1 Active Cycles         cycle  52,496,318.09
    Total L1 Elapsed Cycles          cycle  7,266,447,584
    Average L2 Active Cycles         cycle  58,888,059.34
    Total L2 Elapsed Cycles          cycle  5,749,598,304
    Average SM Active Cycles         cycle  52,496,318.09
    Total SM Elapsed Cycles          cycle  7,266,447,584
    Average SMSP Active Cycles       cycle  52,450,434.29
    Total SMSP Elapsed Cycles        cycle 29,065,790,336
    -------------------------- ----------- --------------

    OPT   Est. Speedup: 5.51%
          One or more SMs have a much higher number of active cycles than the average number of active cycles.
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.
          Maximum instance value is 5.78% above the average, while the minimum instance value is 6.54% below the
          average.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.51%
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active
          cycles. Maximum instance value is 5.78% above the average, while the minimum instance value is 6.54% below
          the average.

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.00
    Branch Instructions              inst    4,210,688
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 32.86%
          This kernel has uncoalesced global accesses resulting in a total of 541065216 excessive sectors (33% of the
          total 1619001344 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional
          information on reducing uncoalesced device memory accesses.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 44.5%
          This kernel has uncoalesced shared accesses resulting in a total of 704643072 excessive wavefronts (47% of
          the total 1509949440 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source
          locations. The CUDA Best Practices Guide
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.

  void matmul_cute_wgmma<128, 128, 32, cute::TiledMMA<cute::MMA_Atom<SM90::MMA_64x128x8_F32TF32TF32_SS_TN<1, 1>>, cute::Layout<cute::tuple<cute::C<2>, cute::C<1>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<0>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<32>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<128>, cute::C<1>>, cute::C<32>>>, cute::tuple<cute::C<32>, cute::C<32>>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<32>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<128>, cute::C<1>>, cute::C<32>>>, cute::tuple<cute::C<32>, cute::C<32>>>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>>>>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>>>>>(const float *, const float *, float *, int, int, int, T4, T5, T6, T7, T8) (32, 64, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.24
    Elapsed Cycles                cycle    7,813,581
    Memory Throughput                 %        54.69
    DRAM Throughput                   %        37.14
    Duration                         ms         6.27
    L1/TEX Cache Throughput           %        44.54
    L2 Cache Throughput               %        55.88
    SM Active Cycles              cycle 7,592,509.85
    Compute (SM) Throughput           %        51.70
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential
          reasons.

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline
          analysis.

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        74.51
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            3
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.56
    Executed Ipc Elapsed  inst/cycle         0.54
    Issue Slots Busy               %        14.11
    Issued Ipc Active     inst/cycle         0.56
    SM Busy                        %        53.57
    -------------------- ----------- ------------

    INF   Shared is the highest-utilized pipeline (53.6%) based on active cycles, taking into account the rates of its
          different instructions. It is the logical sum of several other pipelines which can't achieve full
          utilization on their own. It executes 64-bit floating point and tensor operations. It's dominated by its
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       908.78
    Mem Busy                               %        42.98
    Max Bandwidth                          %        54.69
    L1/TEX Hit Rate                        %         0.63
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    8,369,104
    L2 Hit Rate                            %        64.79
    Mem Pipes Busy                         %        29.08
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 35.98%
          Out of the 267811328.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To
          increase this success rate, consider marking only those memory regions as compressible that contain the most
          zero values and/or expose the most homogeneous values.

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 21.49%
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between
          threads. Check the Source Counters section for uncoalesced global stores.

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        13.95
    Issued Warp Per Scheduler                        0.14
    No Eligible                            %        86.05
    Active Warps Per Scheduler          warp         4.05
    Eligible Warps Per Scheduler        warp         0.19
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 45.31%
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only
          issues an instruction every 7.2 cycles. This might leave hardware resources underutilized and may lead to
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average
          of 4.05 active warps per scheduler, but only an average of 0.19 warps were eligible per cycle. Eligible
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.00
    Warp Cycles Per Executed Instruction           cycle        29.00
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.57
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 41.4%
          On average, each warp of this workload spends 12.0 cycles being stalled waiting for a scoreboard dependency
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently
          used data to shared memory. This stall type represents about 41.4% of the total average of 29.0 cycles
          between issuing two instructions.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 38.17%
          On average, each warp of this workload spends 11.1 cycles being stalled waiting for sibling warps at a CTA
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point
          first. This stall type represents about 38.2% of the total average of 29.0 cycles between issuing two
          instructions.
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on
          sampling data. The Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details
          on each stall reason.

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst 1,071,010.91
    Executed Instructions                           inst  565,493,760
    Avg. Issued Instructions Per Scheduler          inst 1,071,024.56
    Issued Instructions                             inst  565,500,970
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread              96
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           32.77
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         524,288
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                7.76
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            4
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.75
    Achieved Active Warps Per SM           warp        15.84
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 45.31%
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required
          registers.

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle  3,711,980.33
    Total DRAM Elapsed Cycles        cycle   479,723,776
    Average L1 Active Cycles         cycle  7,592,509.85
    Total L1 Elapsed Cycles          cycle 1,038,470,014
    Average L2 Active Cycles         cycle  8,476,186.94
    Total L2 Elapsed Cycles          cycle   810,242,208
    Average SM Active Cycles         cycle  7,592,509.85
    Total SM Elapsed Cycles          cycle 1,038,470,014
    Average SMSP Active Cycles       cycle  7,678,583.43
    Total SMSP Elapsed Cycles        cycle 4,153,880,056
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst    8,421,376
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.3893%
          This kernel has uncoalesced global accesses resulting in a total of 4194304 excessive sectors (0% of the
          total 1082130432 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional
          information on reducing uncoalesced device memory accesses.

  void matmul_cute_wgmma_pipe<128, 128, 32, 0, cute::TiledMMA<cute::MMA_Atom<SM90::MMA_64x128x8_F32TF32TF32_SS_TN<1, 1>>, cute::Layout<cute::tuple<cute::C<2>, cute::C<1>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<0>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<32>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<128>, cute::C<1>>, cute::C<32>>>, cute::tuple<cute::C<32>, cute::C<32>>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<32>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<128>, cute::C<1>>, cute::C<32>>>, cute::tuple<cute::C<32>, cute::C<32>>>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>>>>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>>>>>(const float *, const float *, float *, int, int, int, T5, T6, T7, T8, T9) (32, 64, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.24
    Elapsed Cycles                cycle    5,570,074
    Memory Throughput                 %        68.36
    DRAM Throughput                   %        26.49
    Duration                         ms         4.48
    L1/TEX Cache Throughput           %        59.26
    L2 Cache Throughput               %        68.63
    SM Active Cycles              cycle 5,531,535.05
    Compute (SM) Throughput           %        72.85
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced.
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline
          analysis.

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       100.66
    Dropped Samples                sample           71
    Maximum Sampling Interval          us            3
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.96
    Executed Ipc Elapsed  inst/cycle         0.95
    Issue Slots Busy               %        23.99
    Issued Ipc Active     inst/cycle         0.96
    SM Busy                        %        73.53
    -------------------- ----------- ------------

    OPT   Tensor is the highest-utilized pipeline (73.5%) based on active cycles, taking into account the rates of its
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its
          Tensor (FP) sub-pipeline. The pipeline is well-utilized, but might become a bottleneck if more work is
          added. Based on the number of executed instructions, the highest utilized pipeline (16.2%) is ALU. It
          executes integer and logic operations. Comparing the two, the overall pipeline utilization appears to be
          caused by high-latency instructions. See the Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which
          reasons cause warps to stall.

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       648.25
    Mem Busy                               %        58.71
    Max Bandwidth                          %        68.36
    L1/TEX Hit Rate                        %         0.63
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    8,347,476
    L2 Hit Rate                            %        81.29
    Mem Pipes Busy                         %        40.98
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 25.11%
          Out of the 267119232.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To
          increase this success rate, consider marking only those memory regions as compressible that contain the most
          zero values and/or expose the most homogeneous values.

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 29.36%
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between
          threads. Check the Source Counters section for uncoalesced global stores.

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        23.91
    Issued Warp Per Scheduler                        0.24
    No Eligible                            %        76.09
    Active Warps Per Scheduler          warp         4.01
    Eligible Warps Per Scheduler        warp         0.35
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.15%
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only
          issues an instruction every 4.2 cycles. This might leave hardware resources underutilized and may lead to
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average
          of 4.01 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        16.77
    Warp Cycles Per Executed Instruction           cycle        16.77
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    29.70
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.15%
          On average, each warp of this workload spends 6.5 cycles being stalled waiting for sibling warps at a CTA
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point
          first. This stall type represents about 38.6% of the total average of 16.8 cycles between issuing two
          instructions.
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on
          sampling data. The Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details
          on each stall reason.

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst 1,327,072.97
    Executed Instructions                           inst  700,694,528
    Avg. Issued Instructions Per Scheduler          inst 1,327,082.97
    Issued Instructions                             inst  700,699,808
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             124
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           65.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         524,288
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                7.76
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.30
    Achieved Active Warps Per SM           warp        15.55
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.15%
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required
          registers, and the required amount of shared memory.

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle  1,889,108.67
    Total DRAM Elapsed Cycles        cycle   342,259,200
    Average L1 Active Cycles         cycle  5,531,535.05
    Total L1 Elapsed Cycles          cycle   736,959,880
    Average L2 Active Cycles         cycle  6,007,887.67
    Total L2 Elapsed Cycles          cycle   576,477,408
    Average SM Active Cycles         cycle  5,531,535.05
    Total SM Elapsed Cycles          cycle   736,959,880
    Average SMSP Active Cycles       cycle  5,550,100.70
    Total SMSP Elapsed Cycles        cycle 2,947,839,520
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst   50,331,648
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.3878%
          This kernel has uncoalesced global accesses resulting in a total of 4194304 excessive sectors (0% of the
          total 1082130432 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional
          information on reducing uncoalesced device memory accesses.

  void matmul_cute_wgmma_pipe<128, 128, 32, 4, cute::TiledMMA<cute::MMA_Atom<SM90::MMA_64x128x8_F32TF32TF32_SS_TN<1, 1>>, cute::Layout<cute::tuple<cute::C<2>, cute::C<1>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<0>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<32>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<128>, cute::C<1>>, cute::C<32>>>, cute::tuple<cute::C<32>, cute::C<32>>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<32>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<128>, cute::C<1>>, cute::C<32>>>, cute::tuple<cute::C<32>, cute::C<32>>>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>>>>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>>>>>(const float *, const float *, float *, int, int, int, T5, T6, T7, T8, T9) (2048, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.24
    Elapsed Cycles                cycle    5,514,804
    Memory Throughput                 %        69.42
    DRAM Throughput                   %        26.42
    Duration                         ms         4.43
    L1/TEX Cache Throughput           %        59.52
    L2 Cache Throughput               %        68.31
    SM Active Cycles              cycle 5,481,564.91
    Compute (SM) Throughput           %        74.05
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced.
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline
          analysis.

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       100.66
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            3
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.97
    Executed Ipc Elapsed  inst/cycle         0.97
    Issue Slots Busy               %        24.24
    Issued Ipc Active     inst/cycle         0.97
    SM Busy                        %        74.20
    -------------------- ----------- ------------

    OPT   Tensor is the highest-utilized pipeline (74.2%) based on active cycles, taking into account the rates of its
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its
          Tensor (FP) sub-pipeline. The pipeline is well-utilized, but might become a bottleneck if more work is
          added. Based on the number of executed instructions, the highest utilized pipeline (15.2%) is ALU. It
          executes integer and logic operations. Comparing the two, the overall pipeline utilization appears to be
          caused by high-latency instructions. See the Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which
          reasons cause warps to stall.

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       646.55
    Mem Busy                               %        59.40
    Max Bandwidth                          %        69.42
    L1/TEX Hit Rate                        %         0.63
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    8,361,948
    L2 Hit Rate                            %        80.56
    Mem Pipes Busy                         %        41.66
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 25.04%
          Out of the 267582336.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To
          increase this success rate, consider marking only those memory regions as compressible that contain the most
          zero values and/or expose the most homogeneous values.

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 29.7%
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between
          threads. Check the Source Counters section for uncoalesced global stores.

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        24.36
    Issued Warp Per Scheduler                        0.24
    No Eligible                            %        75.64
    Active Warps Per Scheduler          warp         3.91
    Eligible Warps Per Scheduler        warp         0.34
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.95%
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only
          issues an instruction every 4.1 cycles. This might leave hardware resources underutilized and may lead to
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average
          of 3.91 active warps per scheduler, but only an average of 0.34 warps were eligible per cycle. Eligible
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        16.03
    Warp Cycles Per Executed Instruction           cycle        16.03
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    29.70
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.95%
          On average, each warp of this workload spends 6.2 cycles being stalled waiting for sibling warps at a CTA
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point
          first. This stall type represents about 38.8% of the total average of 16.0 cycles between issuing two
          instructions.
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on
          sampling data. The Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details
          on each stall reason.

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst 1,328,562.42
    Executed Instructions                           inst  701,480,960
    Avg. Issued Instructions Per Scheduler          inst 1,328,575.78
    Issued Instructions                             inst  701,488,014
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             124
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           65.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         524,288
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                7.76
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.29
    Achieved Active Warps Per SM           warp        15.54
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.95%
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required
          registers, and the required amount of shared memory.

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle  1,865,364.83
    Total DRAM Elapsed Cycles        cycle   338,843,136
    Average L1 Active Cycles         cycle  5,481,564.91
    Total L1 Elapsed Cycles          cycle   725,026,496
    Average L2 Active Cycles         cycle  5,937,740.98
    Total L2 Elapsed Cycles          cycle   570,924,576
    Average SM Active Cycles         cycle  5,481,564.91
    Total SM Elapsed Cycles          cycle   725,026,496
    Average SMSP Active Cycles       cycle  5,454,289.63
    Total SMSP Elapsed Cycles        cycle 2,900,105,984
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst   50,331,648
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.387%
          This kernel has uncoalesced global accesses resulting in a total of 4194304 excessive sectors (0% of the
          total 1082130432 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional
          information on reducing uncoalesced device memory accesses.

  void matmul_cute_wgmma_tma<cute::tuple<cute::C<128>, cute::C<128>, cute::C<32>>, cute::Copy_Atom<cute::Copy_Traits<cute::SM90_TMA_LOAD, cute::C<131072>, cute::AuxTmaParams<cute::tuple<cute::ScaledBasis<cute::C<1>, 1>, cute::ScaledBasis<cute::C<1>, 0>>, const cute::Layout<cute::tuple<cute::C<32>, cute::C<128>>, cute::tuple<cute::ScaledBasis<cute::C<1>, 1>, cute::ScaledBasis<cute::C<1>, 0>>> &, const cute::Swizzle<3, 4, 3> &>>, float>, cute::Copy_Atom<cute::Copy_Traits<cute::SM90_TMA_LOAD, cute::C<131072>, cute::AuxTmaParams<cute::tuple<cute::ScaledBasis<cute::C<1>, 1>, cute::ScaledBasis<cute::C<1>, 0>>, const cute::Layout<cute::tuple<cute::C<32>, cute::C<128>>, cute::tuple<cute::ScaledBasis<cute::C<1>, 1>, cute::ScaledBasis<cute::C<1>, 0>>> &, const cute::Swizzle<3, 4, 3> &>>, float>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<3>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>, cute::tuple<cute::C<0>, cute::C<4096>>>>>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<3>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>, cute::tuple<cute::C<0>, cute::C<4096>>>>>, cute::TiledMMA<cute::MMA_Atom<SM90::MMA_64x128x8_F32TF32TF32_SS_TN<1, 1>>, cute::Layout<cute::tuple<cute::C<2>, cute::C<1>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<0>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>>(int, int, int, T2, T3, float *, T6) (32, 64, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.24
    Elapsed Cycles                cycle    5,385,701
    Memory Throughput                 %        83.11
    DRAM Throughput                   %        65.22
    Duration                         ms         4.32
    L1/TEX Cache Throughput           %        59.47
    L2 Cache Throughput               %        80.25
    SM Active Cycles              cycle 5,240,563.03
    Compute (SM) Throughput           %        68.17
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.
          Start by analyzing L2 in the Memory Workload Analysis section.

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline
          analysis.

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       100.66
    Dropped Samples                sample           74
    Maximum Sampling Interval          us            3
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.88
    Executed Ipc Elapsed  inst/cycle         0.77
    Issue Slots Busy               %        21.94
    Issued Ipc Active     inst/cycle         0.88
    SM Busy                        %        77.61
    -------------------- ----------- ------------

    OPT   Shared is the highest-utilized pipeline (77.6%) based on active cycles, taking into account the rates of its
          different instructions. It is the logical sum of several other pipelines which can't achieve full
          utilization on their own. It executes 64-bit floating point and tensor operations. It's dominated by its
          Tensor (FP) sub-pipeline. The pipeline is well-utilized, but might become a bottleneck if more work is
          added. Based on the number of executed instructions, the highest utilized pipeline (15.4%) is ALU. It
          executes integer and logic operations. Comparing the two, the overall pipeline utilization appears to be
          caused by high-latency instructions. See the Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which
          reasons cause warps to stall.

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.60
    Mem Busy                               %        64.54
    Max Bandwidth                          %        83.11
    L1/TEX Hit Rate                        %        72.38
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    8,307,661
    L2 Hit Rate                            %        63.12
    Mem Pipes Busy                         %        38.42
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 63.51%
          Out of the 265845152.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To
          increase this success rate, consider marking only those memory regions as compressible that contain the most
          zero values and/or expose the most homogeneous values.

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 26.12%
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between
          threads. Check the Source Counters section for uncoalesced global stores.

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        19.95
    Issued Warp Per Scheduler                        0.20
    No Eligible                            %        80.05
    Active Warps Per Scheduler          warp         3.66
    Eligible Warps Per Scheduler        warp         0.30
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 16.89%
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only
          issues an instruction every 5.0 cycles. This might leave hardware resources underutilized and may lead to
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average
          of 3.66 active warps per scheduler, but only an average of 0.30 warps were eligible per cycle. Eligible
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        18.34
    Warp Cycles Per Executed Instruction           cycle        18.34
    Avg. Active Threads Per Warp                                29.86
    Avg. Not Predicated Off Threads Per Warp                    28.53
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.89%
          On average, each warp of this workload spends 6.4 cycles being stalled waiting for sibling warps at a CTA
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point
          first. This stall type represents about 35.2% of the total average of 18.3 cycles between issuing two
          instructions.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.89%
          On average, each warp of this workload spends 5.8 cycles being stalled waiting for a scoreboard dependency on
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently
          used data to shared memory. This stall type represents about 31.6% of the total average of 18.3 cycles
          between issuing two instructions.
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on
          sampling data. The Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details
          on each stall reason.

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst 1,149,735.28
    Executed Instructions                           inst  607,060,227
    Avg. Issued Instructions Per Scheduler          inst 1,149,895.07
    Issued Instructions                             inst  607,144,598
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread              90
    Shared Memory Configuration Size           Kbyte          200.70
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           98.43
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         524,288
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                3.88
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 465 thread blocks.
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for
          up to 25.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for
          more details on launch configurations.

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster          264
    Max Cluster Size                      block           16
    Overall GPU Occupancy                     %         1.56
    Cluster Occupancy                         %         6.25
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.70
    Achieved Active Warps Per SM           warp        15.81
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.89%
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required
          registers, and the required amount of shared memory.

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle  4,493,224.33
    Total DRAM Elapsed Cycles        cycle   330,673,152
    Average L1 Active Cycles         cycle  5,240,563.03
    Total L1 Elapsed Cycles          cycle   787,502,774
    Average L2 Active Cycles         cycle  5,893,601.59
    Total L2 Elapsed Cycles          cycle   560,298,144
    Average SM Active Cycles         cycle  5,240,563.03
    Total SM Elapsed Cycles          cycle   787,502,774
    Average SMSP Active Cycles       cycle  5,762,492.39
    Total SMSP Elapsed Cycles        cycle 3,150,011,096
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.09
    Branch Instructions              inst   54,248,809
    Branch Efficiency                   %        96.03
    Avg. Divergent Branches                   1,982.06
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 50.49%
          This kernel has uncoalesced global accesses resulting in a total of 4194304 excessive sectors (50% of the
          total 8388608 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional
          information on reducing uncoalesced device memory accesses.

  void matmul_cute_wgmma_tma<cute::tuple<cute::C<128>, cute::C<256>, cute::C<32>>, cute::Copy_Atom<cute::Copy_Traits<cute::SM90_TMA_LOAD, cute::C<131072>, cute::AuxTmaParams<cute::tuple<cute::ScaledBasis<cute::C<1>, 1>, cute::ScaledBasis<cute::C<1>, 0>>, const cute::Layout<cute::tuple<cute::C<32>, cute::C<128>>, cute::tuple<cute::ScaledBasis<cute::C<1>, 1>, cute::ScaledBasis<cute::C<1>, 0>>> &, const cute::Swizzle<3, 4, 3> &>>, float>, cute::Copy_Atom<cute::Copy_Traits<cute::SM90_TMA_LOAD, cute::C<262144>, cute::AuxTmaParams<cute::tuple<cute::ScaledBasis<cute::C<1>, 1>, cute::ScaledBasis<cute::C<1>, 0>>, const cute::Layout<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::ScaledBasis<cute::C<1>, 1>, cute::ScaledBasis<cute::C<1>, 0>>> &, const cute::Swizzle<3, 4, 3> &>>, float>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<3>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>, cute::tuple<cute::C<0>, cute::C<4096>>>>>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<32>>, cute::tuple<cute::C<32>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<3>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>, cute::tuple<cute::C<0>, cute::C<8192>>>>>, cute::TiledMMA<cute::MMA_Atom<SM90::MMA_64x128x8_F32TF32TF32_SS_TN<1, 1>>, cute::Layout<cute::tuple<cute::C<2>, cute::C<2>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<2>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>>(int, int, int, T2, T3, float *, T6) (32, 32, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.24
    Elapsed Cycles                cycle    4,397,850
    Memory Throughput                 %        70.83
    DRAM Throughput                   %        37.32
    Duration                         ms         3.54
    L1/TEX Cache Throughput           %        73.31
    L2 Cache Throughput               %        60.63
    SM Active Cycles              cycle 4,249,296.49
    Compute (SM) Throughput           %        92.47
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.
          Start by analyzing workloads in the Compute Workload Analysis section.

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline
          analysis.

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       100.66
    Dropped Samples                sample            0
    Maximum Sampling Interval          us         1.50
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.90
    Executed Ipc Elapsed  inst/cycle         0.87
    Issue Slots Busy               %        22.59
    Issued Ipc Active     inst/cycle         0.90
    SM Busy                        %        95.71
    -------------------- ----------- ------------

    OPT   Tensor is the highest-utilized pipeline (95.7%) based on active cycles, taking into account the rates of its
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its
          Tensor (FP) sub-pipeline. The pipeline is over-utilized and likely a performance bottleneck. Based on the
          number of executed instructions, the highest utilized pipeline (17.7%) is ALU. It executes integer and logic
          operations. Comparing the two, the overall pipeline utilization appears to be caused by high-latency
          instructions. See the Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which
          reasons cause warps to stall.

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       913.22
    Mem Busy                               %        70.83
    Max Bandwidth                          %        69.36
    L1/TEX Hit Rate                        %        79.89
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    8,392,108
    L2 Hit Rate                            %        72.14
    Mem Pipes Busy                         %        52.02
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 35.47%
          Out of the 268547456.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To
          increase this success rate, consider marking only those memory regions as compressible that contain the most
          zero values and/or expose the most homogeneous values.

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 35.41%
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between
          threads. Check the Source Counters section for uncoalesced global stores.

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        22.56
    Issued Warp Per Scheduler                        0.23
    No Eligible                            %        77.44
    Active Warps Per Scheduler          warp         3.98
    Eligible Warps Per Scheduler        warp         0.25
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.525%
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only
          issues an instruction every 4.4 cycles. This might leave hardware resources underutilized and may lead to
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average
          of 3.98 active warps per scheduler, but only an average of 0.25 warps were eligible per cycle. Eligible
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        17.62
    Warp Cycles Per Executed Instruction           cycle        17.63
    Avg. Active Threads Per Warp                                30.94
    Avg. Not Predicated Off Threads Per Warp                    29.35
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.525%
          On average, each warp of this workload spends 10.8 cycles being stalled waiting for sibling warps at a CTA
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point
          first. This stall type represents about 61.2% of the total average of 17.6 cycles between issuing two
          instructions.
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on
          sampling data. The Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details
          on each stall reason.

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   959,737.17
    Executed Instructions                           inst  506,741,227
    Avg. Issued Instructions Per Scheduler          inst   959,793.71
    Issued Instructions                             inst  506,771,078
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  1,024
    Registers Per Thread             register/thread              90
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block          147.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         524,288
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                7.76
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster          132
    Max Cluster Size                      block           16
    Overall GPU Occupancy                     %         0.78
    Cluster Occupancy                         %         3.12
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.90
    Achieved Active Warps Per SM           warp        15.93
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.525%
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required
          registers, and the required amount of shared memory.

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle  2,103,249.83
    Total DRAM Elapsed Cycles        cycle   270,493,952
    Average L1 Active Cycles         cycle  4,249,296.49
    Total L1 Elapsed Cycles          cycle   580,559,256
    Average L2 Active Cycles         cycle  4,769,173.54
    Total L2 Elapsed Cycles          cycle   457,257,312
    Average SM Active Cycles         cycle  4,249,296.49
    Total SM Elapsed Cycles          cycle   580,559,256
    Average SMSP Active Cycles       cycle  4,253,550.40
    Total SMSP Elapsed Cycles        cycle 2,322,237,024
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst   52,062,052
    Branch Efficiency                   %        97.97
    Avg. Divergent Branches                     991.03
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 50.06%
          This kernel has uncoalesced global accesses resulting in a total of 4194304 excessive sectors (50% of the
          total 8388608 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional
          information on reducing uncoalesced device memory accesses.

  void cutlass::device_kernel<gemm::GemmUniversal<cute::tuple<int, int, int, int>, gemm::CollectiveMma<gemm::MainloopSm90TmaGmmaWarpSpecialized<4, cute::tuple<cute::C<1>, cute::C<1>, cute::C<1>>, gemm::KernelTmaWarpSpecializedCooperative>, cute::tuple<cute::C<128>, cute::C<256>, cute::C<32>>, float, cute::tuple<long, cute::C<1>, long>, float, cute::tuple<long, cute::C<1>, long>, cute::TiledMMA<cute::MMA_Atom<SM90::MMA_64x256x8_F32TF32TF32_SS_TN<1, 1>>, cute::Layout<cute::tuple<cute::C<2>, cute::C<1>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<0>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>, cute::SM90_TMA_LOAD, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::C<8>, cute::C<32>>, cute::tuple<cute::C<32>, cute::C<1>>>>, void, cute::identity, cute::SM90_TMA_LOAD, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::C<8>, cute::C<32>>, cute::tuple<cute::C<32>, cute::C<1>>>>, void, cute::identity>, epilogue::Sm90TmaWarpSpecializedAdapter<epilogue::DefaultEpilogue<float, cute::tuple<long, cute::C<1>, long>, cute::tuple<long, cute::C<1>, long>, epilogue::LinearCombination<float, 1, float, float, 0, 2, float>, gemm::EpilogueDefault>>, void, void>>(T1::Params) (1, 132, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.25
    Elapsed Cycles                cycle    4,936,240
    Memory Throughput                 %        56.92
    DRAM Throughput                   %        50.01
    Duration                         ms         3.93
    L1/TEX Cache Throughput           %        55.29
    L2 Cache Throughput               %        55.28
    SM Active Cycles              cycle 4,716,671.47
    Compute (SM) Throughput           %        83.05
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.
          Start by analyzing workloads in the Compute Workload Analysis section.

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved
          close to 0% of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling
          Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on
          roofline analysis.

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       100.66
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            3
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.46
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        11.53
    Issued Ipc Active     inst/cycle         0.46
    SM Busy                        %        86.23
    -------------------- ----------- ------------

    OPT   Shared is the highest-utilized pipeline (86.2%) based on active cycles, taking into account the rates of its
          different instructions. It is the logical sum of several other pipelines which can't achieve full
          utilization on their own. It executes 64-bit floating point and tensor operations. It's dominated by its
          Tensor (FP) sub-pipeline. The pipeline is over-utilized and likely a performance bottleneck. Based on the
          number of executed instructions, the highest utilized pipeline (7.0%) is Uniform. Comparing the two, the
          overall pipeline utilization appears to be caused by high-latency instructions. See the Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which
          reasons cause warps to stall.

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.22
    Mem Busy                               %        53.24
    Max Bandwidth                          %        56.92
    L1/TEX Hit Rate                        %        62.45
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    8,270,861
    L2 Hit Rate                            %        72.00
    Mem Pipes Busy                         %        44.17
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 48.11%
          Out of the 264667552.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To
          increase this success rate, consider marking only those memory regions as compressible that contain the most
          zero values and/or expose the most homogeneous values.

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 26.62%
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between
          threads. Check the Source Counters section for uncoalesced global stores.

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        11.51
    Issued Warp Per Scheduler                        0.12
    No Eligible                            %        88.49
    Active Warps Per Scheduler          warp         2.25
    Eligible Warps Per Scheduler        warp         0.12
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 16.95%
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only
          issues an instruction every 8.7 cycles. This might leave hardware resources underutilized and may lead to
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average
          of 2.25 active warps per scheduler, but only an average of 0.12 warps were eligible per cycle. Eligible
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons
          on the Warp State Statistics and Source Counters sections.

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.53
    Warp Cycles Per Executed Instruction           cycle        19.65
    Avg. Active Threads Per Warp                                29.01
    Avg. Not Predicated Off Threads Per Warp                    29.00
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.95%
          On average, each warp of this workload spends 12.5 cycles being stalled waiting for sibling warps at a CTA
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point
          first. This stall type represents about 64.1% of the total average of 19.5 cycles between issuing two
          instructions.
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on
          sampling data. The Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details
          on each stall reason.

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   540,584.27
    Executed Instructions                           inst  285,428,496
    Avg. Issued Instructions Per Scheduler          inst   543,752.25
    Issued Instructions                             inst  287,101,190
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 0.2848%
          This kernel executes 0 fused and 1069584 non-fused FP32 instructions. By converting pairs of non-fused
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its
          current performance).

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    132
    Registers Per Thread             register/thread             168
    Shared Memory Configuration Size           Kbyte          200.70
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block          196.86
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread          50,688
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two
          blocks per multiprocessor (compared to the currently executed 1.0 blocks) This way, blocks that aren't
          waiting for __syncthreads() can keep the hardware busy.

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            5
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %        18.75
    Achieved Occupancy                        %        14.06
    Achieved Active Warps Per SM           warp         9.00
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.95%
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required
          registers, and the required amount of shared memory.

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle     3,128,928
    Total DRAM Elapsed Cycles        cycle   300,340,480
    Average L1 Active Cycles         cycle  4,716,671.47
    Total L1 Elapsed Cycles          cycle   646,464,836
    Average L2 Active Cycles         cycle  5,313,198.11
    Total L2 Elapsed Cycles          cycle   508,343,904
    Average SM Active Cycles         cycle  4,716,671.47
    Total SM Elapsed Cycles          cycle   646,464,836
    Average SMSP Active Cycles       cycle  4,723,388.97
    Total SMSP Elapsed Cycles        cycle 2,585,859,344
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst   30,344,648
    Branch Efficiency                   %       100.00
    Avg. Divergent Branches                       0.75
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 50.17%
          This kernel has uncoalesced global accesses resulting in a total of 4194304 excessive sectors (50% of the
          total 8388608 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional
          information on reducing uncoalesced device memory accesses.

  void cutlass::Kernel2<cutlass_80_simt_sgemm_256x128_8x4_nn_align1>(T1::Params) (256, 4, 5)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          1.59
    SM Frequency                    Ghz          1.35
    Elapsed Cycles                cycle    41,790,867
    Memory Throughput                 %         47.49
    DRAM Throughput                   %          5.76
    Duration                         ms         30.96
    L1/TEX Cache Throughput           %         47.99
    L2 Cache Throughput               %         11.53
    SM Active Cycles              cycle 41,358,954.09
    Compute (SM) Throughput           %         87.39
    ----------------------- ----------- -------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.
          Start by analyzing workloads in the Compute Workload Analysis section.

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 78%
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline
          analysis.

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        80.35
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           12
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         3.53
    Executed Ipc Elapsed  inst/cycle         3.50
    Issue Slots Busy               %        88.30
    Issued Ipc Active     inst/cycle         3.53
    SM Busy                        %        88.30
    -------------------- ----------- ------------

    OPT   FMA is the highest-utilized pipeline (79.2%) based on active cycles, taking into account the rates of its
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)
          operations. The pipeline is well-utilized, but might become a bottleneck if more work is added. Based on the
          number of executed instructions, the highest utilized pipeline (79.0%) is FMA. It executes 32-bit floating
          point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD) operations. Comparing the two, the overall pipeline
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows
          the mix of executed instructions for this workload.

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       140.99
    Mem Busy                               %        47.49
    Max Bandwidth                          %        29.38
    L1/TEX Hit Rate                        %         1.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector   20,984,818
    L2 Hit Rate                            %        81.36
    Mem Pipes Busy                         %        19.87
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 4.779%
          Out of the 671514176.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To
          increase this success rate, consider marking only those memory regions as compressible that contain the most
          zero values and/or expose the most homogeneous values.

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 0.01268%
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 32.0 of the 32
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between
          threads. Check the Source Counters section for uncoalesced global loads.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.01014%
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 32.0 of the 32
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between
          threads. Check the Source Counters section for uncoalesced global stores.

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        88.35
    Issued Warp Per Scheduler                        0.88
    No Eligible                            %        11.65
    Active Warps Per Scheduler          warp         2.00
    Eligible Warps Per Scheduler        warp         1.60
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         2.26
    Warp Cycles Per Executed Instruction           cycle         2.26
    Avg. Active Threads Per Warp                                32.00
    Avg. Not Predicated Off Threads Per Warp                    31.86
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- --------------
    Metric Name                              Metric Unit   Metric Value
    ---------------------------------------- ----------- --------------
    Avg. Executed Instructions Per Scheduler        inst  36,520,112.48
    Executed Instructions                           inst 19,282,619,392
    Avg. Issued Instructions Per Scheduler          inst  36,520,159.98
    Issued Instructions                             inst 19,282,644,469
    ---------------------------------------- ----------- --------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  5,120
    Registers Per Thread             register/thread             202
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           49.15
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread       1,310,720
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               38.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.49
    Achieved Active Warps Per SM           warp         8.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 87.5%
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required
          registers, and the required amount of shared memory.

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle   2,841,459.67
    Total DRAM Elapsed Cycles        cycle  2,367,060,480
    Average L1 Active Cycles         cycle  41,358,954.09
    Total L1 Elapsed Cycles          cycle  5,516,448,190
    Average L2 Active Cycles         cycle  41,385,161.61
    Total L2 Elapsed Cycles          cycle  4,009,001,088
    Average SM Active Cycles         cycle  41,358,954.09
    Total SM Elapsed Cycles          cycle  5,516,448,190
    Average SMSP Active Cycles       cycle  41,333,671.15
    Total SMSP Elapsed Cycles        cycle 22,065,792,760
    -------------------------- ----------- --------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.00
    Branch Instructions              inst   17,339,392
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 26.63%
          This kernel has uncoalesced shared accesses resulting in a total of 671089870 excessive wavefronts (27% of
          the total 2494007063 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source
          locations. The CUDA Best Practices Guide
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.

  sm90_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize128x256x32_warpgroupsize2x1x1_execute_segment_k_off_kernel__5x_cublas (66, 2, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.25
    Elapsed Cycles                cycle    4,323,985
    Memory Throughput                 %        71.89
    DRAM Throughput                   %        31.37
    Duration                         ms         3.44
    L1/TEX Cache Throughput           %        74.33
    L2 Cache Throughput               %        57.01
    SM Active Cycles              cycle 4,146,270.97
    Compute (SM) Throughput           %        94.88
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.
          Start by analyzing workloads in the Compute Workload Analysis section.

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved
          close to 0% of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling
          Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on
          roofline analysis.

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       100.66
    Dropped Samples                sample            0
    Maximum Sampling Interval          us         1.50
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.80
    Executed Ipc Elapsed  inst/cycle         0.77
    Issue Slots Busy               %        19.95
    Issued Ipc Active     inst/cycle         0.80
    SM Busy                        %        98.09
    -------------------- ----------- ------------

    OPT   Shared is the highest-utilized pipeline (98.1%) based on active cycles, taking into account the rates of its
          different instructions. It is the logical sum of several other pipelines which can't achieve full
          utilization on their own. It executes 64-bit floating point and tensor operations. It's dominated by its
          Tensor (FP) sub-pipeline. The pipeline is over-utilized and likely a performance bottleneck. Based on the
          number of executed instructions, the highest utilized pipeline (12.6%) is LSU. It executes load/store memory
          operations. Comparing the two, the overall pipeline utilization appears to be caused by high-latency
          instructions. See the Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which
          reasons cause warps to stall.

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       767.59
    Mem Busy                               %        71.89
    Max Bandwidth                          %        59.33
    L1/TEX Hit Rate                        %        88.11
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    4,215,562
    L2 Hit Rate                            %        75.50
    Mem Pipes Busy                         %        50.42
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 29.67%
          Out of the 134897984.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To
          increase this success rate, consider marking only those memory regions as compressible that contain the most
          zero values and/or expose the most homogeneous values.

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 62.91%
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.0 of the 32
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between
          threads. Check the Source Counters section for uncoalesced global stores.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 69.65%
          The memory access pattern for local loads from L1TEX might not be optimal. On average, only 1.0 of the 32
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between
          threads. Check the Source Counters section for uncoalesced local loads.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 69.65%
          The memory access pattern for local stores to L1TEX might not be optimal. On average, only 1.0 of the 32
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between
          threads. Check the Source Counters section for uncoalesced local stores.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.2%
          The memory access pattern for shared loads might not be optimal and causes on average a 3.9 - way bank
          conflict across all 67138410 shared load requests.This results in 67111347 bank conflicts,  which represent
          25.83% of the overall 259804819 wavefronts for shared loads. Check the Source Counters section for
          uncoalesced shared loads.

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        19.95
    Issued Warp Per Scheduler                        0.20
    No Eligible                            %        80.05
    Active Warps Per Scheduler          warp         2.97
    Eligible Warps Per Scheduler        warp         0.21
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.115%
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only
          issues an instruction every 5.0 cycles. This might leave hardware resources underutilized and may lead to
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average
          of 2.97 active warps per scheduler, but only an average of 0.21 warps were eligible per cycle. Eligible
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        14.87
    Warp Cycles Per Executed Instruction           cycle        14.88
    Avg. Active Threads Per Warp                                32.00
    Avg. Not Predicated Off Threads Per Warp                    30.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.115%
          On average, each warp of this workload spends 7.0 cycles being stalled waiting for sibling warps at a CTA
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point
          first. This stall type represents about 47.1% of the total average of 14.9 cycles between issuing two
          instructions.
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.115%
          On average, each warp of this workload spends 4.5 cycles being stalled waiting for a scoreboard dependency on
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently
          used data to shared memory. This stall type represents about 30.2% of the total average of 14.9 cycles
          between issuing two instructions.
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on
          sampling data. The Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details
          on each stall reason.

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   826,798.76
    Executed Instructions                           inst  436,549,745
    Avg. Issued Instructions Per Scheduler          inst   827,106.14
    Issued Instructions                             inst  436,712,044
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   2
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    132
    Registers Per Thread             register/thread             168
    Shared Memory Configuration Size           Kbyte          233.47
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block          231.42
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread          50,688
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two
          blocks per multiprocessor (compared to the currently executed 1.0 blocks) This way, blocks that aren't
          waiting for __syncthreads() can keep the hardware busy.

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster           66
    Max Cluster Size                      block           16
    Overall GPU Occupancy                     %         0.59
    Cluster Occupancy                         %         3.12
    Block Limit Barriers                  block            4
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            5
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %        18.75
    Achieved Occupancy                        %        18.54
    Achieved Active Warps Per SM           warp        11.86
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.115%
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required
          registers, and the required amount of shared memory.

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle  1,719,711.83
    Total DRAM Elapsed Cycles        cycle   263,129,600
    Average L1 Active Cycles         cycle  4,146,270.97
    Total L1 Elapsed Cycles          cycle   565,813,410
    Average L2 Active Cycles         cycle  4,631,411.27
    Total L2 Elapsed Cycles          cycle   443,990,208
    Average SM Active Cycles         cycle  4,146,270.97
    Total SM Elapsed Cycles          cycle   565,813,410
    Average SMSP Active Cycles       cycle  4,146,295.09
    Total SMSP Elapsed Cycles        cycle 2,263,253,640
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst   11,755,218
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                       7.48
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.08%
          This kernel has uncoalesced shared accesses resulting in a total of 67108864 excessive wavefronts (17% of the
          total 403752742 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source
          locations. The CUDA Best Practices Guide
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.
