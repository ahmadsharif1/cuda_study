==PROF== Connected to process 395452 (/home/ahmads/github/cuda_study/matmul/bin/matmul)
==WARNING== Unable to access the following 6 metrics: ctc__rx_bytes_data_user.sum, ctc__rx_bytes_data_user.sum.pct_of_peak_sustained_elapsed, ctc__rx_bytes_data_user.sum.per_second, ctc__tx_bytes_data_user.sum, ctc__tx_bytes_data_user.sum.pct_of_peak_sustained_elapsed, ctc__tx_bytes_data_user.sum.per_second.


==PROF== Profiling "matmul_tiled": 0%
==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.
....50%....100% - 39 passes
==PROF== Profiling "matmul_tiled_reg": 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_tiled_vec": 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_tiled_pipe": 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_simple": 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_smem": 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_smem": 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_persistent": 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_swizzle": 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_pipe": 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_wgmma": 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_wgmma_pipe": 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_wgmma_pipe": 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_wgmma_tma": 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_wgmma_tma": 0%....50%....100% - 39 passes
Running in PROFILING mode (1 iteration, no warmup, no verification).
==PROF== Disconnected from process 395452
[395452] matmul@127.0.0.1
  void matmul_tiled<32>(const float *, const float *, float *, int, int, int) (256, 128, 1)x(32, 32, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- --------------
    Metric Name             Metric Unit   Metric Value
    ----------------------- ----------- --------------
    DRAM Frequency                  Ghz           1.59
    SM Frequency                    Ghz           1.35
    Elapsed Cycles                cycle    238,656,546
    Memory Throughput                 %          89.33
    DRAM Throughput                   %          16.16
    Duration                         ms         176.78
    L1/TEX Cache Throughput           %          89.59
    L2 Cache Throughput               %          14.11
    SM Active Cycles              cycle 237,958,121.05
    Compute (SM) Throughput           %          75.00
    ----------------------- ----------- --------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 14%  
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       111.21
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           48
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.63
    Executed Ipc Elapsed  inst/cycle         1.62
    Issue Slots Busy               %        40.63
    Issued Ipc Active     inst/cycle         1.63
    SM Busy                        %        41.04
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.61%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       395.29
    Mem Busy                               %        89.33
    Max Bandwidth                          %        85.22
    L1/TEX Hit Rate                        %         0.00
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    4,327,890
    L2 Hit Rate                            %        38.70
    Mem Pipes Busy                         %        75.00
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 16.11%                                                                                          
          Out of the 138492480.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.13%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 1.1 - way bank       
          conflict across all 1073741824 shared store requests.This results in 152270473 bank conflicts,  which         
          represent 12.42% of the overall 1226012297 wavefronts for shared stores. Check the Source Counters section    
          for uncoalesced shared stores.                                                                                

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        40.63
    Issued Warp Per Scheduler                        0.41
    No Eligible                            %        59.37
    Active Warps Per Scheduler          warp        15.97
    Eligible Warps Per Scheduler        warp         3.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.67%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 15.97 active warps per scheduler, but only an average of 3.07 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        39.30
    Warp Cycles Per Executed Instruction           cycle        39.30
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    32.00
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.67%                                                                                          
          On average, each warp of this workload spends 19.0 cycles being stalled waiting for the MIO (memory           
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 48.3% of the total average of 39.3 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- --------------
    Metric Name                              Metric Unit   Metric Value
    ---------------------------------------- ----------- --------------
    Avg. Executed Instructions Per Scheduler        inst  96,675,529.70
    Executed Instructions                           inst 51,044,679,680
    Avg. Issued Instructions Per Scheduler          inst  96,675,559.16
    Issued Instructions                             inst 51,044,695,239
    ---------------------------------------- ----------- --------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                 1,024
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 32,768
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            8.19
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread      33,554,432
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                              124.12
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.79
    Achieved Active Warps Per SM           warp        63.87
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ---------------
    Metric Name                Metric Unit    Metric Value
    -------------------------- ----------- ---------------
    Average DRAM Active Cycles       cycle   45,495,209.50
    Total DRAM Elapsed Cycles        cycle  13,517,516,800
    Average L1 Active Cycles         cycle  237,958,121.05
    Total L1 Elapsed Cycles          cycle  31,500,608,752
    Average L2 Active Cycles         cycle  238,393,414.33
    Total L2 Elapsed Cycles          cycle  22,898,024,736
    Average SM Active Cycles         cycle  237,958,121.05
    Total SM Elapsed Cycles          cycle  31,500,608,752
    Average SMSP Active Cycles       cycle  237,956,815.54
    Total SMSP Elapsed Cycles        cycle 126,002,435,008
    -------------------------- ----------- ---------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst  540,016,640
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void matmul_tiled_reg<128, 128, 8, 8, 8>(const float *, const float *, float *, int, int, int) (64, 32, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          1.59
    SM Frequency                    Ghz          1.35
    Elapsed Cycles                cycle    91,426,351
    Memory Throughput                 %         52.76
    DRAM Throughput                   %          5.50
    Duration                         ms         67.72
    L1/TEX Cache Throughput           %         54.45
    L2 Cache Throughput               %          8.36
    SM Active Cycles              cycle 88,585,808.27
    Compute (SM) Throughput           %         43.25
    ----------------------- ----------- -------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 36%  
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        86.11
    Dropped Samples                sample          182
    Maximum Sampling Interval          us           24
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.79
    Executed Ipc Elapsed  inst/cycle         1.73
    Issue Slots Busy               %        44.64
    Issued Ipc Active     inst/cycle         1.79
    SM Busy                        %        44.64
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (38.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       134.67
    Mem Busy                               %        52.76
    Max Bandwidth                          %        28.92
    L1/TEX Hit Rate                        %         0.27
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector   33,413,773
    L2 Hit Rate                            %        61.43
    Mem Pipes Busy                         %        13.36
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 5.278%                                                                                          
          Out of the 1069240736.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To     
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 46.17%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.78%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 5.0 - way bank        
          conflict across all 1073741824 shared load requests.This results in 2147483648 bank conflicts,  which         
          represent 40.00% of the overall 5368709120 wavefronts for shared loads. Check the Source Counters section     
          for uncoalesced shared loads.                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.062%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 1.1 - way bank       
          conflict across all 268435456 shared store requests.This results in 33630479 bank conflicts,  which           
          represent 11.13% of the overall 302065935 wavefronts for shared stores. Check the Source Counters section     
          for uncoalesced shared stores.                                                                                

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.65
    Issued Warp Per Scheduler                        0.45
    No Eligible                            %        55.35
    Active Warps Per Scheduler          warp         2.00
    Eligible Warps Per Scheduler        warp         0.72
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.24%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 2.00 active warps per scheduler, but only an average of 0.72 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         4.48
    Warp Cycles Per Executed Instruction           cycle         4.48
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    32.00
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- --------------
    Metric Name                              Metric Unit   Metric Value
    ---------------------------------------- ----------- --------------
    Avg. Executed Instructions Per Scheduler        inst  39,541,791.03
    Executed Instructions                           inst 20,878,065,664
    Avg. Issued Instructions Per Scheduler          inst  39,541,801.03
    Issued Instructions                             inst 20,878,070,944
    ---------------------------------------- ----------- --------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             162
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            8.19
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         524,288
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               15.52
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.50
    Achieved Active Warps Per SM           warp         8.00
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 47.24%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle   5,937,680.50
    Total DRAM Elapsed Cycles        cycle  5,178,398,720
    Average L1 Active Cycles         cycle  88,585,808.27
    Total L1 Elapsed Cycles          cycle 12,067,212,866
    Average L2 Active Cycles         cycle  88,151,175.38
    Total L2 Elapsed Cycles          cycle  8,766,077,952
    Average SM Active Cycles         cycle  88,585,808.27
    Total SM Elapsed Cycles          cycle 12,067,212,866
    Average SMSP Active Cycles       cycle  88,557,526.39
    Total SMSP Elapsed Cycles        cycle 48,268,851,464
    -------------------------- ----------- --------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.00
    Branch Instructions              inst   33,849,344
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 2.56%                                                                                           
          This kernel has uncoalesced global accesses resulting in a total of 29360128 excessive sectors (3% of the     
          total 1107296256 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 36.91%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 2147483648 excessive wavefronts (38% of   
          the total 5637144576 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source       
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void matmul_tiled_vec<128, 128, 8, 8, 8>(const float *, const float *, float *, int, int, int) (64, 32, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          1.59
    SM Frequency                    Ghz          1.35
    Elapsed Cycles                cycle    67,751,568
    Memory Throughput                 %         74.06
    DRAM Throughput                   %          8.79
    Duration                         ms         50.19
    L1/TEX Cache Throughput           %         76.20
    L2 Cache Throughput               %         12.04
    SM Active Cycles              cycle 65,843,819.52
    Compute (SM) Throughput           %         53.95
    ----------------------- ----------- -------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L1 
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes      
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or        
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 48%  
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       128.32
    Dropped Samples                sample        5,406
    Maximum Sampling Interval          us           12
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.22
    Executed Ipc Elapsed  inst/cycle         2.16
    Issue Slots Busy               %        55.51
    Issued Ipc Active     inst/cycle         2.22
    SM Busy                        %        55.51
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (50.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       215.03
    Mem Busy                               %        74.06
    Max Bandwidth                          %        39.07
    L1/TEX Hit Rate                        %         1.45
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector   21,160,903
    L2 Hit Rate                            %        57.91
    Mem Pipes Busy                         %        14.27
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 8.538%                                                                                          
          Out of the 677148896.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 64.8%                                                                                           
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 30.48%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 5.0 - way bank        
          conflict across all 1073741824 shared load requests.This results in 2147608212 bank conflicts,  which         
          represent 40.00% of the overall 5368833684 wavefronts for shared loads. Check the Source Counters section     
          for uncoalesced shared loads.                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 39.58%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 4.2 - way bank       
          conflict across all 134217728 shared store requests.This results in 290140467 bank conflicts,  which          
          represent 51.94% of the overall 558575923 wavefronts for shared stores. Check the Source Counters section     
          for uncoalesced shared stores.                                                                                

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        55.42
    Issued Warp Per Scheduler                        0.55
    No Eligible                            %        44.58
    Active Warps Per Scheduler          warp         3.89
    Eligible Warps Per Scheduler        warp         1.31
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.94%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 3.89 active warps per scheduler, but only an average of 1.31 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         7.01
    Warp Cycles Per Executed Instruction           cycle         7.01
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    32.00
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- --------------
    Metric Name                              Metric Unit   Metric Value
    ---------------------------------------- ----------- --------------
    Avg. Executed Instructions Per Scheduler        inst  36,552,207.52
    Executed Instructions                           inst 19,299,565,568
    Avg. Issued Instructions Per Scheduler          inst  36,552,226.52
    Issued Instructions                             inst 19,299,575,600
    ---------------------------------------- ----------- --------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             128
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           10.37
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         524,288
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                7.76
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            5
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.31
    Achieved Active Warps Per SM           warp        15.56
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.94%                                                                                          
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle   7,025,739.50
    Total DRAM Elapsed Cycles        cycle  3,837,463,040
    Average L1 Active Cycles         cycle  65,843,819.52
    Total L1 Elapsed Cycles          cycle  8,942,932,078
    Average L2 Active Cycles         cycle  67,243,198.20
    Total L2 Elapsed Cycles          cycle  6,499,847,136
    Average SM Active Cycles         cycle  65,843,819.52
    Total SM Elapsed Cycles          cycle  8,942,932,078
    Average SMSP Active Cycles       cycle  65,955,627.78
    Total SMSP Elapsed Cycles        cycle 35,771,728,312
    -------------------------- ----------- --------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst  168,067,072
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 2.633%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 29360128 excessive sectors (3% of the     
          total 1107296256 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 39.76%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 2415919104 excessive wavefronts (41% of   
          the total 5905580032 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source       
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void matmul_tiled_pipe<128, 128, 8, 8, 8>(const float *, const float *, float *, int, int, int) (64, 32, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          1.59
    SM Frequency                    Ghz          1.35
    Elapsed Cycles                cycle    98,961,231
    Memory Throughput                 %         49.57
    DRAM Throughput                   %          5.08
    Duration                         ms         73.32
    L1/TEX Cache Throughput           %         51.13
    L2 Cache Throughput               %          7.66
    SM Active Cycles              cycle 95,880,989.83
    Compute (SM) Throughput           %         37.54
    ----------------------- ----------- -------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 33%  
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        93.13
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           24
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.55
    Executed Ipc Elapsed  inst/cycle         1.50
    Issue Slots Busy               %        38.72
    Issued Ipc Active     inst/cycle         1.55
    SM Busy                        %        38.72
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       124.39
    Mem Busy                               %        49.57
    Max Bandwidth                          %        26.76
    L1/TEX Hit Rate                        %         0.19
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector   33,583,876
    L2 Hit Rate                            %        50.88
    Mem Pipes Busy                         %         9.26
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 4.842%                                                                                          
          Out of the 1074684032.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To     
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 43.37%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.45%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 5.0 - way bank        
          conflict across all 1073741824 shared load requests.This results in 2147483648 bank conflicts,  which         
          represent 40.00% of the overall 5368709120 wavefronts for shared loads. Check the Source Counters section     
          for uncoalesced shared loads.                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18%                                                                                             
          The memory access pattern for shared stores might not be optimal and causes on average a 6.2 - way bank       
          conflict across all 67108864 shared store requests.This results in 145860737 bank conflicts,  which           
          represent 35.21% of the overall 414296193 wavefronts for shared stores. Check the Source Counters section     
          for uncoalesced shared stores.                                                                                

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        38.72
    Issued Warp Per Scheduler                        0.39
    No Eligible                            %        61.28
    Active Warps Per Scheduler          warp         2.00
    Eligible Warps Per Scheduler        warp         0.60
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.43%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 2.00 active warps per scheduler, but only an average of 0.60 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         5.17
    Warp Cycles Per Executed Instruction           cycle         5.17
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.94
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 37.31%                                                                                          
          On average, each warp of this workload spends 1.9 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 37.3% of the total average of 5.2 cycles         
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- --------------
    Metric Name                              Metric Unit   Metric Value
    ---------------------------------------- ----------- --------------
    Avg. Executed Instructions Per Scheduler        inst  37,123,940.85
    Executed Instructions                           inst 19,601,440,768
    Avg. Issued Instructions Per Scheduler          inst  37,123,948.85
    Issued Instructions                             inst 19,601,444,992
    ---------------------------------------- ----------- --------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             130
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           20.74
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         524,288
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               15.52
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.50
    Achieved Active Warps Per SM           warp         8.00
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 50.43%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle      5,937,819
    Total DRAM Elapsed Cycles        cycle  5,606,524,416
    Average L1 Active Cycles         cycle  95,880,989.83
    Total L1 Elapsed Cycles          cycle 13,053,812,536
    Average L2 Active Cycles         cycle  47,175,376.85
    Total L2 Elapsed Cycles          cycle  9,416,664,864
    Average SM Active Cycles         cycle  95,880,989.83
    Total SM Elapsed Cycles          cycle 13,053,812,536
    Average SMSP Active Cycles       cycle  95,878,121.27
    Total SMSP Elapsed Cycles        cycle 52,215,250,144
    -------------------------- ----------- --------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst  201,621,504
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 1.275%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 29360128 excessive sectors (3% of the     
          total 1107296256 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 38.33%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 2281701376 excessive wavefronts (40% of   
          the total 5771362304 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source       
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void matmul_cute_simple<128, 128, 8, cute::TiledMMA<cute::MMA_Atom<cute::SM80_16x8x8_F32TF32TF32F32_TN>, cute::Layout<cute::tuple<cute::C<2>, cute::C<2>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<2>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>>(const float *, const float *, float *, int, int, int, T4) (32, 64, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          1.59
    SM Frequency                    Ghz          1.24
    Elapsed Cycles                cycle    32,209,154
    Memory Throughput                 %         75.52
    DRAM Throughput                   %          4.91
    Duration                         ms         25.96
    L1/TEX Cache Throughput           %         82.42
    L2 Cache Throughput               %         30.63
    SM Active Cycles              cycle 29,688,285.75
    Compute (SM) Throughput           %         18.83
    ----------------------- ----------- -------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L1 
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes      
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or        
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%   
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       135.66
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            6
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.73
    Executed Ipc Elapsed  inst/cycle         0.67
    Issue Slots Busy               %        18.21
    Issued Ipc Active     inst/cycle         0.73
    SM Busy                        %        20.55
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (20.5%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       120.08
    Mem Busy                               %        75.52
    Max Bandwidth                          %        27.82
    L1/TEX Hit Rate                        %        67.44
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    7,054,087
    L2 Hit Rate                            %        89.20
    Mem Pipes Busy                         %        12.58
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 4.667%                                                                                          
          Out of the 225730784.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 25.17%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 21.3 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 37.76%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.82
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.18
    Active Warps Per Scheduler          warp         2.84
    Eligible Warps Per Scheduler        warp         0.21
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.48%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 2.84 active warps per scheduler, but only an average of 0.21 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        15.93
    Warp Cycles Per Executed Instruction           cycle        15.93
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    32.00
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.48%                                                                                          
          On average, each warp of this workload spends 11.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 73.8% of the total average of 15.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- -------------
    Metric Name                              Metric Unit  Metric Value
    ---------------------------------------- ----------- -------------
    Avg. Executed Instructions Per Scheduler        inst  5,406,254.55
    Executed Instructions                           inst 2,854,502,400
    Avg. Issued Instructions Per Scheduler          inst  5,406,263.55
    Issued Instructions                             inst 2,854,507,152
    ---------------------------------------- ----------- -------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             166
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         262,144
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                5.17
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %        18.75
    Achieved Occupancy                        %        17.64
    Achieved Active Warps Per SM           warp        11.29
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.48%                                                                                          
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle      2,029,446
    Total DRAM Elapsed Cycles        cycle  1,984,917,248
    Average L1 Active Cycles         cycle  29,688,285.75
    Total L1 Elapsed Cycles          cycle  4,276,777,992
    Average L2 Active Cycles         cycle  31,845,786.34
    Total L2 Elapsed Cycles          cycle  3,350,791,488
    Average SM Active Cycles         cycle  29,688,285.75
    Total SM Elapsed Cycles          cycle  4,276,777,992
    Average SMSP Active Cycles       cycle  30,331,596.13
    Total SMSP Elapsed Cycles        cycle 17,107,111,968
    -------------------------- ----------- --------------

    OPT   Est. Speedup: 6.864%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 7.49% above the average, while the minimum instance value is 9.00% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.273%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 6.70% above the average, while the minimum instance value is 9.71% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.864%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 7.49% above the average, while the minimum instance value is 9.00% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst   16,793,600
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 30.45%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 1077936128 excessive sectors (33% of the  
          total 3229614080 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void matmul_cute_smem<128, 128, 8, cute::TiledMMA<cute::MMA_Atom<cute::SM80_16x8x8_F32TF32TF32F32_TN>, cute::Layout<cute::tuple<cute::C<2>, cute::C<2>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<2>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<64>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<256>, cute::C<1>>, cute::C<64>>>, cute::tuple<cute::C<64>, cute::C<8>>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::C<128>, cute::C<4>>, cute::tuple<cute::C<4>, cute::C<1>>>, cute::tuple<cute::C<128>, cute::C<4>>>, cute::Layout<cute::tuple<cute::C<128>, cute::C<8>>, cute::tuple<cute::C<12>, cute::C<1>>>, cute::Layout<cute::tuple<cute::C<128>, cute::C<8>>, cute::tuple<cute::C<1>, cute::C<132>>>>(const float *, const float *, float *, int, int, int, T4, T5, T6, T7, T8) (32, 64, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          1.59
    SM Frequency                    Ghz          1.24
    Elapsed Cycles                cycle    22,236,483
    Memory Throughput                 %         40.67
    DRAM Throughput                   %          6.96
    Duration                         ms         17.86
    L1/TEX Cache Throughput           %         31.28
    L2 Cache Throughput               %         40.67
    SM Active Cycles              cycle 22,121,423.38
    Compute (SM) Throughput           %         27.49
    ----------------------- ----------- -------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%   
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        95.16
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            6
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.90
    Executed Ipc Elapsed  inst/cycle         0.90
    Issue Slots Busy               %        22.57
    Issued Ipc Active     inst/cycle         0.90
    SM Busy                        %        27.58
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (27.6%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       170.42
    Mem Busy                               %        40.67
    Max Bandwidth                          %        33.83
    L1/TEX Hit Rate                        %         0.41
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    5,951,432
    L2 Hit Rate                            %        94.36
    Mem Pipes Busy                         %        23.51
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 6.652%                                                                                          
          Out of the 190445824.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 15.59%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.43%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.5 - way bank        
          conflict across all 536870912 shared load requests.This results in 268501092 bank conflicts,  which           
          represent 33.34% of the overall 805372004 wavefronts for shared loads. Check the Source Counters section for  
          uncoalesced shared loads.                                                                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        22.52
    Issued Warp Per Scheduler                        0.23
    No Eligible                            %        77.48
    Active Warps Per Scheduler          warp         1.94
    Eligible Warps Per Scheduler        warp         0.24
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 59.33%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 4.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.94 active warps per scheduler, but only an average of 0.24 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.61
    Warp Cycles Per Executed Instruction           cycle         8.61
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.39
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 54.07%                                                                                          
          On average, each warp of this workload spends 4.7 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 54.1% of the total average of 8.6 cycles         
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- -------------
    Metric Name                              Metric Unit  Metric Value
    ---------------------------------------- ----------- -------------
    Avg. Executed Instructions Per Scheduler        inst  4,992,930.91
    Executed Instructions                           inst 2,636,267,520
    Avg. Issued Instructions Per Scheduler          inst  4,992,938.91
    Issued Instructions                             inst 2,636,271,744
    ---------------------------------------- ----------- -------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             188
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           10.34
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         262,144
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                7.76
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            5
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.15
    Achieved Active Warps Per SM           warp         7.78
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 59.33%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle   1,981,791.67
    Total DRAM Elapsed Cycles        cycle  1,365,810,944
    Average L1 Active Cycles         cycle  22,121,423.38
    Total L1 Elapsed Cycles          cycle  2,929,898,334
    Average L2 Active Cycles         cycle  23,855,040.72
    Total L2 Elapsed Cycles          cycle  2,306,855,904
    Average SM Active Cycles         cycle  22,121,423.38
    Total SM Elapsed Cycles          cycle  2,929,898,334
    Average SMSP Active Cycles       cycle  22,173,635.70
    Total SMSP Elapsed Cycles        cycle 11,719,593,336
    -------------------------- ----------- --------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst   16,793,600
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 33.18%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 541065216 excessive sectors (33% of the   
          total 1619001344 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 58.78%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 1157627904 excessive wavefronts (59% of   
          the total 1962934272 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source       
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void matmul_cute_smem<128, 128, 32, cute::TiledMMA<cute::MMA_Atom<cute::SM80_16x8x8_F32TF32TF32F32_TN>, cute::Layout<cute::tuple<cute::C<2>, cute::C<2>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<2>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<64>, cute::C<1>>, cute::C<16>>>, cute::tuple<cute::C<16>, cute::C<32>>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::C<128>, cute::C<4>>, cute::tuple<cute::C<4>, cute::C<1>>>, cute::tuple<cute::C<128>, cute::C<4>>>, cute::Layout<cute::tuple<cute::C<128>, cute::C<32>>, cute::tuple<cute::C<36>, cute::C<1>>>, cute::Layout<cute::tuple<cute::C<128>, cute::C<32>>, cute::tuple<cute::C<1>, cute::C<132>>>>(const float *, const float *, float *, int, int, int, T4, T5, T6, T7, T8) (32, 64, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          1.59
    SM Frequency                    Ghz          1.24
    Elapsed Cycles                cycle    14,348,537
    Memory Throughput                 %         53.56
    DRAM Throughput                   %         17.56
    Duration                         ms         11.56
    L1/TEX Cache Throughput           %         45.58
    L2 Cache Throughput               %         53.56
    SM Active Cycles              cycle 13,941,433.38
    Compute (SM) Throughput           %         42.54
    ----------------------- ----------- -------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%   
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       127.21
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            3
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.32
    Executed Ipc Elapsed  inst/cycle         1.29
    Issue Slots Busy               %        33.08
    Issued Ipc Active     inst/cycle         1.32
    SM Busy                        %        43.76
    -------------------- ----------- ------------

    INF   Shared is the highest-utilized pipeline (43.8%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical sum of several other pipelines which can't achieve full             
          utilization on their own. It executes 64-bit floating point and tensor operations. It's dominated by its      
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       429.59
    Mem Busy                               %        52.58
    Max Bandwidth                          %        53.56
    L1/TEX Hit Rate                        %         0.47
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    6,111,234
    L2 Hit Rate                            %        88.92
    Mem Pipes Busy                         %        33.07
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 17.03%                                                                                          
          Out of the 195559488.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 22.15%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 15.19%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.5 - way bank        
          conflict across all 536870912 shared load requests.This results in 268464248 bank conflicts,  which           
          represent 33.34% of the overall 805335160 wavefronts for shared loads. Check the Source Counters section for  
          uncoalesced shared loads.                                                                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        33.08
    Issued Warp Per Scheduler                        0.33
    No Eligible                            %        66.92
    Active Warps Per Scheduler          warp         1.94
    Eligible Warps Per Scheduler        warp         0.40
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.44%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.94 active warps per scheduler, but only an average of 0.40 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         5.87
    Warp Cycles Per Executed Instruction           cycle         5.87
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.83
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- -------------
    Metric Name                              Metric Unit  Metric Value
    ---------------------------------------- ----------- -------------
    Avg. Executed Instructions Per Scheduler        inst  4,612,204.61
    Executed Instructions                           inst 2,435,244,032
    Avg. Issued Instructions Per Scheduler          inst  4,612,212.61
    Issued Instructions                             inst 2,435,248,256
    ---------------------------------------- ----------- -------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             190
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           35.30
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         262,144
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                7.76
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.13
    Achieved Active Warps Per SM           warp         7.77
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 46.44%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle  3,233,843.50
    Total DRAM Elapsed Cycles        cycle   884,109,312
    Average L1 Active Cycles         cycle 13,941,433.38
    Total L1 Elapsed Cycles          cycle 1,893,079,264
    Average L2 Active Cycles         cycle 15,220,684.35
    Total L2 Elapsed Cycles          cycle 1,490,889,312
    Average SM Active Cycles         cycle 13,941,433.38
    Total SM Elapsed Cycles          cycle 1,893,079,264
    Average SMSP Active Cycles       cycle 13,943,464.09
    Total SMSP Elapsed Cycles        cycle 7,572,317,056
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.00
    Branch Instructions              inst    4,210,688
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 32.75%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 541065216 excessive sectors (33% of the   
          total 1619001344 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 45.36%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 704643072 excessive wavefronts (47% of    
          the total 1509949440 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source       
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void matmul_cute_persistent<128, 128, 32, cute::TiledMMA<cute::MMA_Atom<cute::SM80_16x8x8_F32TF32TF32F32_TN>, cute::Layout<cute::tuple<cute::C<2>, cute::C<2>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<2>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<64>, cute::C<1>>, cute::C<16>>>, cute::tuple<cute::C<16>, cute::C<32>>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::C<128>, cute::C<4>>, cute::tuple<cute::C<4>, cute::C<1>>>, cute::tuple<cute::C<128>, cute::C<4>>>, cute::Layout<cute::tuple<cute::C<128>, cute::C<32>>, cute::tuple<cute::C<36>, cute::C<1>>>, cute::Layout<cute::tuple<cute::C<128>, cute::C<32>>, cute::tuple<cute::C<1>, cute::C<132>>>>(const float *, const float *, float *, int, int, int, T4, T5, T6, T7, T8) (132, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          1.59
    SM Frequency                    Ghz          1.22
    Elapsed Cycles                cycle    26,124,335
    Memory Throughput                 %         29.10
    DRAM Throughput                   %         18.73
    Duration                         ms         20.97
    L1/TEX Cache Throughput           %         26.47
    L2 Cache Throughput               %         29.09
    SM Active Cycles              cycle 24,005,321.70
    Compute (SM) Throughput           %         23.89
    ----------------------- ----------- -------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%   
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       114.95
    Dropped Samples                sample           24
    Maximum Sampling Interval          us            6
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.77
    Executed Ipc Elapsed  inst/cycle         0.72
    Issue Slots Busy               %        19.21
    Issued Ipc Active     inst/cycle         0.77
    SM Busy                        %        25.41
    -------------------- ----------- ------------

    INF   Shared is the highest-utilized pipeline (25.4%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical sum of several other pipelines which can't achieve full             
          utilization on their own. It executes 64-bit floating point and tensor operations. It's dominated by its      
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       458.25
    Mem Busy                               %        29.02
    Max Bandwidth                          %        29.10
    L1/TEX Hit Rate                        %         0.33
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    7,658,446
    L2 Hit Rate                            %        81.64
    Mem Pipes Busy                         %        18.57
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 18.39%                                                                                          
          Out of the 245070272.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 12.44%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.824%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.5 - way bank        
          conflict across all 536870912 shared load requests.This results in 268435456 bank conflicts,  which           
          represent 33.33% of the overall 805306368 wavefronts for shared loads. Check the Source Counters section for  
          uncoalesced shared loads.                                                                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        19.22
    Issued Warp Per Scheduler                        0.19
    No Eligible                            %        80.78
    Active Warps Per Scheduler          warp         1.00
    Eligible Warps Per Scheduler        warp         0.19
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 70.9%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.19 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         5.22
    Warp Cycles Per Executed Instruction           cycle         5.22
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 38.74%                                                                                          
          On average, each warp of this workload spends 2.0 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 38.7% of the total average of 5.2 cycles         
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.79%                                                                                          
          On average, each warp of this workload spends 1.8 cycles being stalled waiting on a fixed latency execution   
          dependency. Typically, this stall reason should be very low and only shows up as a top contributor in         
          already highly optimized kernels. Try to hide the corresponding instruction latencies by increasing the       
          number of active warps, restructuring the code or unrolling loops. Furthermore, consider switching to         
          lower-latency instructions, e.g. by making use of fast math compiler options. This stall type represents      
          about 33.8% of the total average of 5.2 cycles between issuing two instructions.                              
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- -------------
    Metric Name                              Metric Unit  Metric Value
    ---------------------------------------- ----------- -------------
    Avg. Executed Instructions Per Scheduler        inst  4,611,833.48
    Executed Instructions                           inst 2,435,048,080
    Avg. Issued Instructions Per Scheduler          inst  4,611,842.48
    Issued Instructions                             inst 2,435,052,832
    ---------------------------------------- ----------- -------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    132
    Registers Per Thread             register/thread             188
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           35.30
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread          16,896
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.50
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.0 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %         6.25
    Achieved Active Warps Per SM           warp         4.00
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 70.9%                                                                                           
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle   6,256,335.67
    Total DRAM Elapsed Cycles        cycle  1,603,469,568
    Average L1 Active Cycles         cycle  24,005,321.70
    Total L1 Elapsed Cycles          cycle  3,371,257,786
    Average L2 Active Cycles         cycle  27,868,058.04
    Total L2 Elapsed Cycles          cycle  2,697,765,600
    Average SM Active Cycles         cycle  24,005,321.70
    Total SM Elapsed Cycles          cycle  3,371,257,786
    Average SMSP Active Cycles       cycle  23,989,375.13
    Total SMSP Elapsed Cycles        cycle 13,485,031,144
    -------------------------- ----------- --------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.00
    Branch Instructions              inst    4,219,408
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 33.14%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 541065216 excessive sectors (33% of the   
          total 1619001344 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 43.86%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 704643072 excessive wavefronts (47% of    
          the total 1509949440 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source       
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void matmul_cute_swizzle<128, 128, 32, 4, cute::TiledMMA<cute::MMA_Atom<cute::SM80_16x8x8_F32TF32TF32F32_TN>, cute::Layout<cute::tuple<cute::C<2>, cute::C<2>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<2>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<64>, cute::C<1>>, cute::C<16>>>, cute::tuple<cute::C<16>, cute::C<32>>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::C<128>, cute::C<4>>, cute::tuple<cute::C<4>, cute::C<1>>>, cute::tuple<cute::C<128>, cute::C<4>>>, cute::Layout<cute::tuple<cute::C<128>, cute::C<32>>, cute::tuple<cute::C<36>, cute::C<1>>>, cute::Layout<cute::tuple<cute::C<128>, cute::C<32>>, cute::tuple<cute::C<1>, cute::C<132>>>>(const float *, const float *, float *, int, int, int, T5, T6, T7, T8, T9) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          1.59
    SM Frequency                    Ghz          1.25
    Elapsed Cycles                cycle    17,796,671
    Memory Throughput                 %         43.26
    DRAM Throughput                   %         10.74
    Duration                         ms         14.27
    L1/TEX Cache Throughput           %         36.22
    L2 Cache Throughput               %         43.29
    SM Active Cycles              cycle 17,542,994.29
    Compute (SM) Throughput           %         34.37
    ----------------------- ----------- -------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%   
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        77.00
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            6
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.05
    Executed Ipc Elapsed  inst/cycle         1.03
    Issue Slots Busy               %        26.16
    Issued Ipc Active     inst/cycle         1.05
    SM Busy                        %        34.78
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (34.8%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       262.70
    Mem Busy                               %        41.36
    Max Bandwidth                          %        43.26
    L1/TEX Hit Rate                        %         0.47
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    6,135,518
    L2 Hit Rate                            %        91.80
    Mem Pipes Busy                         %        26.72
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 10.32%                                                                                          
          Out of the 196336576.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 17.9%                                                                                           
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.08%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.5 - way bank        
          conflict across all 536870912 shared load requests.This results in 268457210 bank conflicts,  which           
          represent 33.34% of the overall 805328122 wavefronts for shared loads. Check the Source Counters section for  
          uncoalesced shared loads.                                                                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        26.16
    Issued Warp Per Scheduler                        0.26
    No Eligible                            %        73.84
    Active Warps Per Scheduler          warp         1.94
    Eligible Warps Per Scheduler        warp         0.30
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 56.74%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 3.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.94 active warps per scheduler, but only an average of 0.30 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         7.43
    Warp Cycles Per Executed Instruction           cycle         7.43
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 30.53%                                                                                          
          On average, each warp of this workload spends 2.3 cycles being stalled waiting on a fixed latency execution   
          dependency. Typically, this stall reason should be very low and only shows up as a top contributor in         
          already highly optimized kernels. Try to hide the corresponding instruction latencies by increasing the       
          number of active warps, restructuring the code or unrolling loops. Furthermore, consider switching to         
          lower-latency instructions, e.g. by making use of fast math compiler options. This stall type represents      
          about 30.5% of the total average of 7.4 cycles between issuing two instructions.                              
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- -------------
    Metric Name                              Metric Unit  Metric Value
    ---------------------------------------- ----------- -------------
    Avg. Executed Instructions Per Scheduler        inst  4,588,993.94
    Executed Instructions                           inst 2,422,988,800
    Avg. Issued Instructions Per Scheduler          inst  4,588,999.94
    Issued Instructions                             inst 2,422,991,968
    ---------------------------------------- ----------- -------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             186
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           35.30
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         262,144
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                7.76
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.16
    Achieved Active Warps Per SM           warp         7.78
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 56.74%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle     2,440,691
    Total DRAM Elapsed Cycles        cycle 1,091,171,456
    Average L1 Active Cycles         cycle 17,542,994.29
    Total L1 Elapsed Cycles          cycle 2,343,044,548
    Average L2 Active Cycles         cycle 18,577,785.10
    Total L2 Elapsed Cycles          cycle 1,835,361,120
    Average SM Active Cycles         cycle 17,542,994.29
    Total SM Elapsed Cycles          cycle 2,343,044,548
    Average SMSP Active Cycles       cycle 17,542,862.82
    Total SMSP Elapsed Cycles        cycle 9,372,178,192
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.00
    Branch Instructions              inst    4,210,688
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 32.47%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 541065216 excessive sectors (33% of the   
          total 1619001344 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 46.12%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 704643072 excessive wavefronts (47% of    
          the total 1509949440 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source       
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void matmul_cute_pipe<128, 128, 32, cute::TiledMMA<cute::MMA_Atom<cute::SM80_16x8x8_F32TF32TF32F32_TN>, cute::Layout<cute::tuple<cute::C<2>, cute::C<2>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<2>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<64>, cute::C<1>>, cute::C<16>>>, cute::tuple<cute::C<16>, cute::C<32>>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::C<128>, cute::C<4>>, cute::tuple<cute::C<4>, cute::C<1>>>, cute::tuple<cute::C<128>, cute::C<4>>>, cute::Layout<cute::tuple<cute::C<128>, cute::C<32>>, cute::tuple<cute::C<36>, cute::C<1>>>, cute::Layout<cute::tuple<cute::C<128>, cute::C<32>>, cute::tuple<cute::C<1>, cute::C<132>>>>(const float *, const float *, float *, int, int, int, T4, T5, T6, T7, T8) (32, 64, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          1.59
    SM Frequency                    Ghz          1.24
    Elapsed Cycles                cycle    39,570,052
    Memory Throughput                 %         33.16
    DRAM Throughput                   %          4.39
    Duration                         ms         31.78
    L1/TEX Cache Throughput           %         34.20
    L2 Cache Throughput               %         24.76
    SM Active Cycles              cycle 38,444,825.03
    Compute (SM) Throughput           %         65.28
    ----------------------- ----------- -------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%   
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        82.51
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           12
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.16
    Executed Ipc Elapsed  inst/cycle         2.09
    Issue Slots Busy               %        54.00
    Issued Ipc Active     inst/cycle         2.16
    SM Busy                        %        67.33
    -------------------- ----------- ------------

    OPT   ALU is the highest-utilized pipeline (67.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. The pipeline is well-utilized, but might    
          become a bottleneck if more work is added. Based on the number of executed instructions, the highest          
          utilized pipeline (67.3%) is ALU. It executes integer and logic operations. Comparing the two, the overall    
          pipeline utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide      
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       107.31
    Mem Busy                               %        33.16
    Max Bandwidth                          %        27.82
    L1/TEX Hit Rate                        %        42.46
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector  349,910,699
    L2 Hit Rate                            %        93.92
    Mem Pipes Busy                         %        27.82
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 3.551%                                                                                          
          Out of the 11197142368.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To    
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 16.58%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 32.13%                                                                                          
          The memory access pattern for local loads from L1TEX might not be optimal. On average, only 1.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced local loads.                                       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 32.13%                                                                                          
          The memory access pattern for local stores to L1TEX might not be optimal. On average, only 1.0 of the 32      
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced local stores.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.73%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.5 - way bank        
          conflict across all 536870912 shared load requests.This results in 280137533 bank conflicts,  which           
          represent 34.29% of the overall 817008445 wavefronts for shared loads. Check the Source Counters section for  
          uncoalesced shared loads.                                                                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        54.08
    Issued Warp Per Scheduler                        0.54
    No Eligible                            %        45.92
    Active Warps Per Scheduler          warp         2.00
    Eligible Warps Per Scheduler        warp         0.71
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 34.72%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 2.00 active warps per scheduler, but only an average of 0.71 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         3.69
    Warp Cycles Per Executed Instruction           cycle         3.69
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    22.80
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 34.72%                                                                                          
          On average, each warp of this workload spends 1.5 cycles being stalled waiting on a fixed latency execution   
          dependency. Typically, this stall reason should be very low and only shows up as a top contributor in         
          already highly optimized kernels. Try to hide the corresponding instruction latencies by increasing the       
          number of active warps, restructuring the code or unrolling loops. Furthermore, consider switching to         
          lower-latency instructions, e.g. by making use of fast math compiler options. This stall type represents      
          about 40.7% of the total average of 3.7 cycles between issuing two instructions.                              
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.77%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 32.0 threads being active per cycle. This is further reduced  
          to 22.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- --------------
    Metric Name                              Metric Unit   Metric Value
    ---------------------------------------- ----------- --------------
    Avg. Executed Instructions Per Scheduler        inst  20,761,025.94
    Executed Instructions                           inst 10,961,821,696
    Avg. Issued Instructions Per Scheduler          inst  20,761,033.94
    Issued Instructions                             inst 10,961,825,920
    ---------------------------------------- ----------- --------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             255
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           70.59
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         262,144
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                7.76
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.45
    Achieved Active Warps Per SM           warp         7.97
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 34.72%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle   2,220,058.33
    Total DRAM Elapsed Cycles        cycle  2,429,762,048
    Average L1 Active Cycles         cycle  38,444,825.03
    Total L1 Elapsed Cycles          cycle  5,234,088,152
    Average L2 Active Cycles         cycle  42,622,107.67
    Total L2 Elapsed Cycles          cycle  4,112,153,280
    Average SM Active Cycles         cycle  38,444,825.03
    Total SM Elapsed Cycles          cycle  5,234,088,152
    Average SMSP Active Cycles       cycle  38,390,128.34
    Total SMSP Elapsed Cycles        cycle 20,936,352,608
    -------------------------- ----------- --------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.00
    Branch Instructions              inst   12,599,296
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 33.25%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 541065216 excessive sectors (33% of the   
          total 1619001344 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 45.25%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 704643072 excessive wavefronts (47% of    
          the total 1509949440 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source       
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void matmul_cute_wgmma<128, 128, 32, cute::TiledMMA<cute::MMA_Atom<SM90::MMA_64x128x8_F32TF32TF32_SS_TN<1, 1>>, cute::Layout<cute::tuple<cute::C<2>, cute::C<1>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<0>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<32>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<128>, cute::C<1>>, cute::C<32>>>, cute::tuple<cute::C<32>, cute::C<32>>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<32>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<128>, cute::C<1>>, cute::C<32>>>, cute::tuple<cute::C<32>, cute::C<32>>>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>>>>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>>>>>(const float *, const float *, float *, int, int, int, T4, T5, T6, T7, T8) (32, 64, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.24
    Elapsed Cycles                cycle    7,943,305
    Memory Throughput                 %        54.21
    DRAM Throughput                   %        44.30
    Duration                         ms         6.37
    L1/TEX Cache Throughput           %        44.69
    L2 Cache Throughput               %        54.63
    SM Active Cycles              cycle 7,571,306.27
    Compute (SM) Throughput           %        50.32
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%   
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        76.09
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            3
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.57
    Executed Ipc Elapsed  inst/cycle         0.53
    Issue Slots Busy               %        14.15
    Issued Ipc Active     inst/cycle         0.57
    SM Busy                        %        53.72
    -------------------- ----------- ------------

    INF   Shared is the highest-utilized pipeline (53.7%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical sum of several other pipelines which can't achieve full             
          utilization on their own. It executes 64-bit floating point and tensor operations. It's dominated by its      
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.08
    Mem Busy                               %        44.12
    Max Bandwidth                          %        54.21
    L1/TEX Hit Rate                        %         0.63
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    8,368,429
    L2 Hit Rate                            %        60.10
    Mem Pipes Busy                         %        28.31
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 43.12%                                                                                          
          Out of the 267789728.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 20.93%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        13.80
    Issued Warp Per Scheduler                        0.14
    No Eligible                            %        86.20
    Active Warps Per Scheduler          warp         3.93
    Eligible Warps Per Scheduler        warp         0.19
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 45.79%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 7.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 3.93 active warps per scheduler, but only an average of 0.19 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        28.47
    Warp Cycles Per Executed Instruction           cycle        28.47
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.57
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 39.41%                                                                                          
          On average, each warp of this workload spends 11.2 cycles being stalled waiting for sibling warps at a CTA    
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 39.4% of the total average of 28.5 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 37.47%                                                                                          
          On average, each warp of this workload spends 10.7 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 37.5% of the total average of 28.5 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst 1,071,010.91
    Executed Instructions                           inst  565,493,760
    Avg. Issued Instructions Per Scheduler          inst 1,071,024.56
    Issued Instructions                             inst  565,500,970
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread              96
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           32.77
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         524,288
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                7.76
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            4
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.56
    Achieved Active Warps Per SM           warp        15.72
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 45.79%                                                                                          
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle  4,496,375.17
    Total DRAM Elapsed Cycles        cycle   487,208,320
    Average L1 Active Cycles         cycle  7,571,306.27
    Total L1 Elapsed Cycles          cycle 1,066,808,712
    Average L2 Active Cycles         cycle  8,307,383.14
    Total L2 Elapsed Cycles          cycle   823,228,992
    Average SM Active Cycles         cycle  7,571,306.27
    Total SM Elapsed Cycles          cycle 1,066,808,712
    Average SMSP Active Cycles       cycle  7,762,685.87
    Total SMSP Elapsed Cycles        cycle 4,267,234,848
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst    8,421,376
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.3755%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 4194304 excessive sectors (0% of the      
          total 1082130432 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void matmul_cute_wgmma_pipe<128, 128, 32, 0, cute::TiledMMA<cute::MMA_Atom<SM90::MMA_64x128x8_F32TF32TF32_SS_TN<1, 1>>, cute::Layout<cute::tuple<cute::C<2>, cute::C<1>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<0>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<32>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<128>, cute::C<1>>, cute::C<32>>>, cute::tuple<cute::C<32>, cute::C<32>>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<32>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<128>, cute::C<1>>, cute::C<32>>>, cute::tuple<cute::C<32>, cute::C<32>>>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>>>>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>>>>>(const float *, const float *, float *, int, int, int, T5, T6, T7, T8, T9) (32, 64, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.24
    Elapsed Cycles                cycle    5,537,628
    Memory Throughput                 %        68.24
    DRAM Throughput                   %        26.30
    Duration                         ms         4.45
    L1/TEX Cache Throughput           %        59.12
    L2 Cache Throughput               %        68.42
    SM Active Cycles              cycle 5,537,166.88
    Compute (SM) Throughput           %        73.15
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%   
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       100.66
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            3
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.96
    Executed Ipc Elapsed  inst/cycle         0.95
    Issue Slots Busy               %        23.97
    Issued Ipc Active     inst/cycle         0.96
    SM Busy                        %        73.45
    -------------------- ----------- ------------

    OPT   Shared is the highest-utilized pipeline (73.5%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical sum of several other pipelines which can't achieve full             
          utilization on their own. It executes 64-bit floating point and tensor operations. It's dominated by its      
          Tensor (FP) sub-pipeline. The pipeline is well-utilized, but might become a bottleneck if more work is        
          added. Based on the number of executed instructions, the highest utilized pipeline (16.2%) is ALU. It         
          executes integer and logic operations. Comparing the two, the overall pipeline utilization appears to be      
          caused by high-latency instructions. See the Profiling Guide                                                  
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       643.62
    Mem Busy                               %        58.88
    Max Bandwidth                          %        68.24
    L1/TEX Hit Rate                        %         0.63
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    8,356,476
    L2 Hit Rate                            %        79.46
    Mem Pipes Busy                         %        41.15
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 24.93%                                                                                          
          Out of the 267407232.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 29.44%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        24.00
    Issued Warp Per Scheduler                        0.24
    No Eligible                            %        76.00
    Active Warps Per Scheduler          warp         3.87
    Eligible Warps Per Scheduler        warp         0.35
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.85%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 4.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 3.87 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        16.13
    Warp Cycles Per Executed Instruction           cycle        16.13
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    29.70
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.85%                                                                                          
          On average, each warp of this workload spends 6.4 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 39.8% of the total average of 16.1 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst 1,327,072.97
    Executed Instructions                           inst  700,694,528
    Avg. Issued Instructions Per Scheduler          inst 1,327,082.97
    Issued Instructions                             inst  700,699,808
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             124
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           65.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         524,288
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                7.76
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.29
    Achieved Active Warps Per SM           warp        15.55
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.85%                                                                                          
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle  1,864,559.33
    Total DRAM Elapsed Cycles        cycle   340,244,224
    Average L1 Active Cycles         cycle  5,537,166.88
    Total L1 Elapsed Cycles          cycle   733,895,264
    Average L2 Active Cycles         cycle  6,342,184.88
    Total L2 Elapsed Cycles          cycle   573,832,896
    Average SM Active Cycles         cycle  5,537,166.88
    Total SM Elapsed Cycles          cycle   733,895,264
    Average SMSP Active Cycles       cycle  5,528,424.94
    Total SMSP Elapsed Cycles        cycle 2,935,581,056
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst   50,331,648
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.4112%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 4194304 excessive sectors (0% of the      
          total 1082130432 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void matmul_cute_wgmma_pipe<128, 128, 32, 4, cute::TiledMMA<cute::MMA_Atom<SM90::MMA_64x128x8_F32TF32TF32_SS_TN<1, 1>>, cute::Layout<cute::tuple<cute::C<2>, cute::C<1>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<0>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<32>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<128>, cute::C<1>>, cute::C<32>>>, cute::tuple<cute::C<32>, cute::C<32>>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<32>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<128>, cute::C<1>>, cute::C<32>>>, cute::tuple<cute::C<32>, cute::C<32>>>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>>>>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>>>>>(const float *, const float *, float *, int, int, int, T5, T6, T7, T8, T9) (2048, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.24
    Elapsed Cycles                cycle    5,517,233
    Memory Throughput                 %        68.41
    DRAM Throughput                   %        26.32
    Duration                         ms         4.44
    L1/TEX Cache Throughput           %        59.43
    L2 Cache Throughput               %        69.10
    SM Active Cycles              cycle 5,491,252.68
    Compute (SM) Throughput           %        73.34
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%   
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       100.66
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            3
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.97
    Executed Ipc Elapsed  inst/cycle         0.96
    Issue Slots Busy               %        24.19
    Issued Ipc Active     inst/cycle         0.97
    SM Busy                        %        74.07
    -------------------- ----------- ------------

    OPT   Shared is the highest-utilized pipeline (74.1%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical sum of several other pipelines which can't achieve full             
          utilization on their own. It executes 64-bit floating point and tensor operations. It's dominated by its      
          Tensor (FP) sub-pipeline. The pipeline is well-utilized, but might become a bottleneck if more work is        
          added. Based on the number of executed instructions, the highest utilized pipeline (15.2%) is ALU. It         
          executes integer and logic operations. Comparing the two, the overall pipeline utilization appears to be      
          caused by high-latency instructions. See the Profiling Guide                                                  
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       643.96
    Mem Busy                               %        58.85
    Max Bandwidth                          %        68.41
    L1/TEX Hit Rate                        %         0.63
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    8,359,092
    L2 Hit Rate                            %        80.87
    Mem Pipes Busy                         %        41.26
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 24.94%                                                                                          
          Out of the 267490944.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 29.42%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        24.22
    Issued Warp Per Scheduler                        0.24
    No Eligible                            %        75.78
    Active Warps Per Scheduler          warp         4.22
    Eligible Warps Per Scheduler        warp         0.34
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.66%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 4.1 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 4.22 active warps per scheduler, but only an average of 0.34 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        17.42
    Warp Cycles Per Executed Instruction           cycle        17.42
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    29.70
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.66%                                                                                          
          On average, each warp of this workload spends 6.2 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 35.4% of the total average of 17.4 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst 1,328,562.42
    Executed Instructions                           inst  701,480,960
    Avg. Issued Instructions Per Scheduler          inst 1,328,575.77
    Issued Instructions                             inst  701,488,004
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             124
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           65.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         524,288
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                7.76
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.28
    Achieved Active Warps Per SM           warp        15.54
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.66%                                                                                          
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle  1,861,424.67
    Total DRAM Elapsed Cycles        cycle   339,490,304
    Average L1 Active Cycles         cycle  5,491,252.68
    Total L1 Elapsed Cycles          cycle   732,024,138
    Average L2 Active Cycles         cycle  5,955,139.52
    Total L2 Elapsed Cycles          cycle   572,116,224
    Average SM Active Cycles         cycle  5,491,252.68
    Total SM Elapsed Cycles          cycle   732,024,138
    Average SMSP Active Cycles       cycle  5,484,982.56
    Total SMSP Elapsed Cycles        cycle 2,928,096,552
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst   50,331,648
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.3873%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 4194304 excessive sectors (0% of the      
          total 1082130432 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void matmul_cute_wgmma_tma<cute::tuple<cute::C<128>, cute::C<128>, cute::C<32>>, cute::Copy_Atom<cute::Copy_Traits<cute::SM90_TMA_LOAD, cute::C<131072>, cute::AuxTmaParams<cute::tuple<cute::ScaledBasis<cute::C<1>, 1>, cute::ScaledBasis<cute::C<1>, 0>>, const cute::Layout<cute::tuple<cute::C<32>, cute::C<128>>, cute::tuple<cute::ScaledBasis<cute::C<1>, 1>, cute::ScaledBasis<cute::C<1>, 0>>> &, const cute::Swizzle<3, 4, 3> &>>, float>, cute::Copy_Atom<cute::Copy_Traits<cute::SM90_TMA_LOAD, cute::C<131072>, cute::AuxTmaParams<cute::tuple<cute::ScaledBasis<cute::C<1>, 1>, cute::ScaledBasis<cute::C<1>, 0>>, const cute::Layout<cute::tuple<cute::C<32>, cute::C<128>>, cute::tuple<cute::ScaledBasis<cute::C<1>, 1>, cute::ScaledBasis<cute::C<1>, 0>>> &, const cute::Swizzle<3, 4, 3> &>>, float>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<3>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>, cute::tuple<cute::C<0>, cute::C<4096>>>>>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<3>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>, cute::tuple<cute::C<0>, cute::C<4096>>>>>, cute::TiledMMA<cute::MMA_Atom<SM90::MMA_64x128x8_F32TF32TF32_SS_TN<1, 1>>, cute::Layout<cute::tuple<cute::C<2>, cute::C<1>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<0>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>>(int, int, int, T2, T3, float *, T6) (32, 64, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.24
    Elapsed Cycles                cycle    5,191,553
    Memory Throughput                 %        84.34
    DRAM Throughput                   %        62.03
    Duration                         ms         4.18
    L1/TEX Cache Throughput           %        58.73
    L2 Cache Throughput               %        85.71
    SM Active Cycles              cycle 5,307,532.64
    Compute (SM) Throughput           %        80.51
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing L2 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%   
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       100.66
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            3
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.87
    Executed Ipc Elapsed  inst/cycle         0.91
    Issue Slots Busy               %        21.67
    Issued Ipc Active     inst/cycle         0.87
    SM Busy                        %        76.63
    -------------------- ----------- ------------

    OPT   Tensor is the highest-utilized pipeline (76.6%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. The pipeline is well-utilized, but might become a bottleneck if more work is        
          added. Based on the number of executed instructions, the highest utilized pipeline (15.2%) is ALU. It         
          executes integer and logic operations. Comparing the two, the overall pipeline utilization appears to be      
          caused by high-latency instructions. See the Profiling Guide                                                  
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.52
    Mem Busy                               %        62.78
    Max Bandwidth                          %        84.34
    L1/TEX Hit Rate                        %        72.47
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    8,311,919
    L2 Hit Rate                            %        65.00
    Mem Pipes Busy                         %        45.37
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 60.28%                                                                                          
          Out of the 265981408.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 30.85%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        23.25
    Issued Warp Per Scheduler                        0.23
    No Eligible                            %        76.75
    Active Warps Per Scheduler          warp         4.27
    Eligible Warps Per Scheduler        warp         0.35
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 15.66%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 4.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 4.27 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        18.38
    Warp Cycles Per Executed Instruction           cycle        18.41
    Avg. Active Threads Per Warp                                29.89
    Avg. Not Predicated Off Threads Per Warp                    28.56
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 15.66%                                                                                          
          On average, each warp of this workload spends 7.1 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 38.6% of the total average of 18.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 15.66%                                                                                          
          On average, each warp of this workload spends 6.5 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 35.3% of the total average of 18.4 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst 1,148,392.23
    Executed Instructions                           inst  606,351,096
    Avg. Issued Instructions Per Scheduler          inst 1,149,957.52
    Issued Instructions                             inst  607,177,572
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread              90
    Shared Memory Configuration Size           Kbyte          200.70
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           98.43
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         524,288
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                3.88
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 465 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 25.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster          264
    Max Cluster Size                      block           16
    Overall GPU Occupancy                     %         1.56
    Cluster Occupancy                         %         6.25
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.72
    Achieved Active Warps Per SM           warp        15.82
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 15.66%                                                                                          
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle  4,126,108.17
    Total DRAM Elapsed Cycles        cycle   319,260,672
    Average L1 Active Cycles         cycle  5,307,532.64
    Total L1 Elapsed Cycles          cycle   666,869,248
    Average L2 Active Cycles         cycle  5,933,713.16
    Total L2 Elapsed Cycles          cycle   540,827,616
    Average SM Active Cycles         cycle  5,307,532.64
    Total SM Elapsed Cycles          cycle   666,869,248
    Average SMSP Active Cycles       cycle  4,945,766.62
    Total SMSP Elapsed Cycles        cycle 2,667,476,992
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.09
    Branch Instructions              inst   54,110,516
    Branch Efficiency                   %        96.04
    Avg. Divergent Branches                   1,982.06
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 52.66%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 4194304 excessive sectors (50% of the     
          total 8388608 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void matmul_cute_wgmma_tma<cute::tuple<cute::C<128>, cute::C<256>, cute::C<32>>, cute::Copy_Atom<cute::Copy_Traits<cute::SM90_TMA_LOAD, cute::C<131072>, cute::AuxTmaParams<cute::tuple<cute::ScaledBasis<cute::C<1>, 1>, cute::ScaledBasis<cute::C<1>, 0>>, const cute::Layout<cute::tuple<cute::C<32>, cute::C<128>>, cute::tuple<cute::ScaledBasis<cute::C<1>, 1>, cute::ScaledBasis<cute::C<1>, 0>>> &, const cute::Swizzle<3, 4, 3> &>>, float>, cute::Copy_Atom<cute::Copy_Traits<cute::SM90_TMA_LOAD, cute::C<262144>, cute::AuxTmaParams<cute::tuple<cute::ScaledBasis<cute::C<1>, 1>, cute::ScaledBasis<cute::C<1>, 0>>, const cute::Layout<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::ScaledBasis<cute::C<1>, 1>, cute::ScaledBasis<cute::C<1>, 0>>> &, const cute::Swizzle<3, 4, 3> &>>, float>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<3>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>, cute::tuple<cute::C<0>, cute::C<4096>>>>>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<32>>, cute::tuple<cute::C<32>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<3>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>, cute::tuple<cute::C<0>, cute::C<8192>>>>>, cute::TiledMMA<cute::MMA_Atom<SM90::MMA_64x128x8_F32TF32TF32_SS_TN<1, 1>>, cute::Layout<cute::tuple<cute::C<2>, cute::C<2>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<2>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>>(int, int, int, T2, T3, float *, T6) (32, 32, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.24
    Elapsed Cycles                cycle    4,405,339
    Memory Throughput                 %        71.00
    DRAM Throughput                   %        36.46
    Duration                         ms         3.54
    L1/TEX Cache Throughput           %        73.35
    L2 Cache Throughput               %        68.92
    SM Active Cycles              cycle 4,246,853.57
    Compute (SM) Throughput           %        92.70
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%   
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       100.66
    Dropped Samples                sample            0
    Maximum Sampling Interval          us         1.50
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.90
    Executed Ipc Elapsed  inst/cycle         0.87
    Issue Slots Busy               %        22.60
    Issued Ipc Active     inst/cycle         0.90
    SM Busy                        %        95.77
    -------------------- ----------- ------------

    OPT   Shared is the highest-utilized pipeline (95.8%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical sum of several other pipelines which can't achieve full             
          utilization on their own. It executes 64-bit floating point and tensor operations. It's dominated by its      
          Tensor (FP) sub-pipeline. The pipeline is over-utilized and likely a performance bottleneck. Based on the     
          number of executed instructions, the highest utilized pipeline (17.7%) is ALU. It executes integer and logic  
          operations. Comparing the two, the overall pipeline utilization appears to be caused by high-latency          
          instructions. See the Profiling Guide                                                                         
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       892.13
    Mem Busy                               %        71.00
    Max Bandwidth                          %        69.53
    L1/TEX Hit Rate                        %        79.94
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    8,428,859
    L2 Hit Rate                            %        72.02
    Mem Pipes Busy                         %        52.15
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 34.63%                                                                                          
          Out of the 269723488.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 35.5%                                                                                           
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        22.61
    Issued Warp Per Scheduler                        0.23
    No Eligible                            %        77.39
    Active Warps Per Scheduler          warp         3.99
    Eligible Warps Per Scheduler        warp         0.25
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.302%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 4.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 3.99 active warps per scheduler, but only an average of 0.25 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        17.63
    Warp Cycles Per Executed Instruction           cycle        17.63
    Avg. Active Threads Per Warp                                30.94
    Avg. Not Predicated Off Threads Per Warp                    29.35
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.302%                                                                                          
          On average, each warp of this workload spends 10.8 cycles being stalled waiting for sibling warps at a CTA    
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 61.1% of the total average of 17.6 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   959,737.09
    Executed Instructions                           inst  506,741,184
    Avg. Issued Instructions Per Scheduler          inst   959,793.09
    Issued Instructions                             inst  506,770,754
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  1,024
    Registers Per Thread             register/thread              90
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block          147.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         524,288
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                7.76
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster          132
    Max Cluster Size                      block           16
    Overall GPU Occupancy                     %         0.78
    Cluster Occupancy                         %         3.12
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.90
    Achieved Active Warps Per SM           warp        15.94
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.302%                                                                                          
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle     2,056,395
    Total DRAM Elapsed Cycles        cycle   270,723,072
    Average L1 Active Cycles         cycle  4,246,853.57
    Total L1 Elapsed Cycles          cycle   579,163,040
    Average L2 Active Cycles         cycle  4,755,332.11
    Total L2 Elapsed Cycles          cycle   456,922,560
    Average SM Active Cycles         cycle  4,246,853.57
    Total SM Elapsed Cycles          cycle   579,163,040
    Average SMSP Active Cycles       cycle  4,244,761.23
    Total SMSP Elapsed Cycles        cycle 2,316,652,160
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst   52,062,033
    Branch Efficiency                   %        97.97
    Avg. Divergent Branches                     991.03
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 49.96%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 4194304 excessive sectors (50% of the     
          total 8388608 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

