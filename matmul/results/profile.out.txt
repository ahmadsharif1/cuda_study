==PROF== Connected to process 970673 (/home/ahmads/github/cuda_study/matmul/bin/matmul)
==WARNING== Unable to access the following 6 metrics: ctc__rx_bytes_data_user.sum, ctc__rx_bytes_data_user.sum.pct_of_peak_sustained_elapsed, ctc__rx_bytes_data_user.sum.per_second, ctc__tx_bytes_data_user.sum, ctc__tx_bytes_data_user.sum.pct_of_peak_sustained_elapsed, ctc__tx_bytes_data_user.sum.per_second.


==PROF== Profiling "matmul_naive" - 0: 0%
==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.
....50%....100% - 39 passes
==PROF== Profiling "matmul_tiled" - 1: 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_tiled_reg" - 2: 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_tiled_vec" - 3: 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_tiled_pipe" - 4: 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_simple" - 5: 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_smem" - 6: 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_smem" - 7: 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_persistent" - 8: 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_swizzle" - 9: 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_pipe" - 10: 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_wgmma" - 11: 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_wgmma_pipe" - 12: 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_wgmma_pipe" - 13: 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_wgmma_tma" - 14: 0%....50%....100% - 39 passes
==PROF== Profiling "matmul_cute_wgmma_tma" - 15: 0%....50%....100% - 39 passes
==PROF== Profiling "device_kernel" - 16: 0%Running in PROFILING mode (1 iteration, no warmup, no verification).
....50%....100% - 39 passes
==PROF== Profiling "Kernel2" - 17: 0%....50%....100% - 39 passes
==PROF== Profiling "sm90_xmma_gemm_f32f32_tf32f32..." - 18: 0%....50%....100% - 39 passes
==PROF== Disconnected from process 970673
[970673] matmul@127.0.0.1
  matmul_naive(const float *, const float *, float *, int, int, int) (512, 256, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- --------------
    Metric Name             Metric Unit   Metric Value
    ----------------------- ----------- --------------
    DRAM Frequency                  Ghz           1.59
    SM Frequency                    Ghz           1.35
    Elapsed Cycles                cycle    399,917,654
    Memory Throughput                 %          97.66
    DRAM Throughput                   %          18.98
    Duration                         ms         296.24
    L1/TEX Cache Throughput           %          97.75
    L2 Cache Throughput               %          21.63
    SM Active Cycles              cycle 399,504,955.38
    Compute (SM) Throughput           %          65.10
    ----------------------- ----------- --------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 8%   
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        93.00
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           96
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.49
    Executed Ipc Elapsed  inst/cycle         1.49
    Issue Slots Busy               %        37.18
    Issued Ipc Active     inst/cycle         1.49
    SM Busy                        %        37.18
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (25.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       464.33
    Mem Busy                               %        97.66
    Max Bandwidth                          %        65.99
    L1/TEX Hit Rate                        %        87.54
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    4,333,328
    L2 Hit Rate                            %        39.99
    Mem Pipes Busy                         %        65.10
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 18.95%                                                                                          
          Out of the 138666496.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 42.73%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        37.18
    Issued Warp Per Scheduler                        0.37
    No Eligible                            %        62.82
    Active Warps Per Scheduler          warp        15.96
    Eligible Warps Per Scheduler        warp         1.45
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 2.339%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 15.96 active warps per scheduler, but only an average of 1.45 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        42.93
    Warp Cycles Per Executed Instruction           cycle        42.93
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    32.00
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 2.339%                                                                                          
          On average, each warp of this workload spends 28.1 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 65.6% of the total average of 42.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- --------------
    Metric Name                              Metric Unit   Metric Value
    ---------------------------------------- ----------- --------------
    Avg. Executed Instructions Per Scheduler        inst 148,550,252.61
    Executed Instructions                           inst 78,434,533,376
    Avg. Issued Instructions Per Scheduler          inst 148,550,281.39
    Issued Instructions                             inst 78,434,548,576
    ---------------------------------------- ----------- --------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                131,072
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread      33,554,432
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                              124.12
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.75
    Achieved Active Warps Per SM           warp        63.84
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ---------------
    Metric Name                Metric Unit    Metric Value
    -------------------------- ----------- ---------------
    Average DRAM Active Cycles       cycle   89,552,329.50
    Total DRAM Elapsed Cycles        cycle  22,651,346,944
    Average L1 Active Cycles         cycle  399,504,955.38
    Total L1 Elapsed Cycles          cycle  52,781,046,582
    Average L2 Active Cycles         cycle  399,785,470.22
    Total L2 Elapsed Cycles          cycle  38,388,858,720
    Average SM Active Cycles         cycle  399,504,955.38
    Total SM Elapsed Cycles          cycle  52,781,046,582
    Average SMSP Active Cycles       cycle  399,539,901.56
    Total SMSP Elapsed Cycles        cycle 211,124,186,328
    -------------------------- ----------- ---------------

    Section: Source Counters
    ------------------------- ----------- -------------
    Metric Name               Metric Unit  Metric Value
    ------------------------- ----------- -------------
    Branch Instructions Ratio           %          0.01
    Branch Instructions              inst 1,083,179,008
    Branch Efficiency                   %           100
    Avg. Divergent Branches                           0
    ------------------------- ----------- -------------

  void matmul_tiled<32>(const float *, const float *, float *, int, int, int) (256, 128, 1)x(32, 32, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- --------------
    Metric Name             Metric Unit   Metric Value
    ----------------------- ----------- --------------
    DRAM Frequency                  Ghz           1.59
    SM Frequency                    Ghz           1.35
    Elapsed Cycles                cycle    238,653,086
    Memory Throughput                 %          89.34
    DRAM Throughput                   %          16.13
    Duration                         ms         176.78
    L1/TEX Cache Throughput           %          89.59
    L2 Cache Throughput               %          14.14
    SM Active Cycles              cycle 237,966,069.58
    Compute (SM) Throughput           %          75.00
    ----------------------- ----------- --------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 14%  
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       111.21
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           48
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.63
    Executed Ipc Elapsed  inst/cycle         1.62
    Issue Slots Busy               %        40.63
    Issued Ipc Active     inst/cycle         1.63
    SM Busy                        %        41.04
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.61%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       394.68
    Mem Busy                               %        89.34
    Max Bandwidth                          %        85.22
    L1/TEX Hit Rate                        %         0.00
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    4,276,315
    L2 Hit Rate                            %        38.94
    Mem Pipes Busy                         %        75.00
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 16.08%                                                                                          
          Out of the 136842080.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.26%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 1.1 - way bank       
          conflict across all 1073741824 shared store requests.This results in 154330928 bank conflicts,  which         
          represent 12.57% of the overall 1228072752 wavefronts for shared stores. Check the Source Counters section    
          for uncoalesced shared stores.                                                                                

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        40.63
    Issued Warp Per Scheduler                        0.41
    No Eligible                            %        59.37
    Active Warps Per Scheduler          warp        15.97
    Eligible Warps Per Scheduler        warp         3.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.66%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 15.97 active warps per scheduler, but only an average of 3.07 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        39.30
    Warp Cycles Per Executed Instruction           cycle        39.30
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    32.00
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.66%                                                                                          
          On average, each warp of this workload spends 19.0 cycles being stalled waiting for the MIO (memory           
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 48.3% of the total average of 39.3 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- --------------
    Metric Name                              Metric Unit   Metric Value
    ---------------------------------------- ----------- --------------
    Avg. Executed Instructions Per Scheduler        inst  96,675,529.70
    Executed Instructions                           inst 51,044,679,680
    Avg. Issued Instructions Per Scheduler          inst  96,675,554.85
    Issued Instructions                             inst 51,044,692,959
    ---------------------------------------- ----------- --------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                 1,024
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 32,768
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            8.19
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread      33,554,432
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                              124.12
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.79
    Achieved Active Warps Per SM           warp        63.87
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ---------------
    Metric Name                Metric Unit    Metric Value
    -------------------------- ----------- ---------------
    Average DRAM Active Cycles       cycle   45,424,254.33
    Total DRAM Elapsed Cycles        cycle  13,517,319,552
    Average L1 Active Cycles         cycle  237,966,069.58
    Total L1 Elapsed Cycles          cycle  31,500,185,802
    Average L2 Active Cycles         cycle     238,428,252
    Total L2 Elapsed Cycles          cycle  22,900,743,264
    Average SM Active Cycles         cycle  237,966,069.58
    Total SM Elapsed Cycles          cycle  31,500,185,802
    Average SMSP Active Cycles       cycle  237,965,928.75
    Total SMSP Elapsed Cycles        cycle 126,000,743,208
    -------------------------- ----------- ---------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst  540,016,640
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void matmul_tiled_reg<128, 128, 8, 8, 8>(const float *, const float *, float *, int, int, int) (64, 32, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          1.59
    SM Frequency                    Ghz          1.35
    Elapsed Cycles                cycle    91,395,669
    Memory Throughput                 %         52.78
    DRAM Throughput                   %          5.51
    Duration                         ms         67.70
    L1/TEX Cache Throughput           %         54.46
    L2 Cache Throughput               %          8.31
    SM Active Cycles              cycle 88,568,141.38
    Compute (SM) Throughput           %         43.27
    ----------------------- ----------- -------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 36%  
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        86.05
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           24
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.79
    Executed Ipc Elapsed  inst/cycle         1.73
    Issue Slots Busy               %        44.65
    Issued Ipc Active     inst/cycle         1.79
    SM Busy                        %        44.65
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (38.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       134.71
    Mem Busy                               %        52.78
    Max Bandwidth                          %        28.93
    L1/TEX Hit Rate                        %         0.27
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector   33,412,206
    L2 Hit Rate                            %        61.29
    Mem Pipes Busy                         %        13.36
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 5.279%                                                                                          
          Out of the 1069190592.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To     
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 46.18%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.78%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 5.0 - way bank        
          conflict across all 1073741824 shared load requests.This results in 2147483648 bank conflicts,  which         
          represent 40.00% of the overall 5368709120 wavefronts for shared loads. Check the Source Counters section     
          for uncoalesced shared loads.                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.039%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 1.1 - way bank       
          conflict across all 268435456 shared store requests.This results in 33480902 bank conflicts,  which           
          represent 11.09% of the overall 301916358 wavefronts for shared stores. Check the Source Counters section     
          for uncoalesced shared stores.                                                                                

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.64
    Issued Warp Per Scheduler                        0.45
    No Eligible                            %        55.36
    Active Warps Per Scheduler          warp         2.00
    Eligible Warps Per Scheduler        warp         0.72
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.22%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 2.00 active warps per scheduler, but only an average of 0.72 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         4.48
    Warp Cycles Per Executed Instruction           cycle         4.48
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    32.00
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- --------------
    Metric Name                              Metric Unit   Metric Value
    ---------------------------------------- ----------- --------------
    Avg. Executed Instructions Per Scheduler        inst  39,541,791.03
    Executed Instructions                           inst 20,878,065,664
    Avg. Issued Instructions Per Scheduler          inst  39,541,801.03
    Issued Instructions                             inst 20,878,070,944
    ---------------------------------------- ----------- --------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             162
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            8.19
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         524,288
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               15.52
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.50
    Achieved Active Warps Per SM           warp         8.00
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 47.22%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle   5,937,268.83
    Total DRAM Elapsed Cycles        cycle  5,176,659,968
    Average L1 Active Cycles         cycle  88,568,141.38
    Total L1 Elapsed Cycles          cycle 12,063,593,782
    Average L2 Active Cycles         cycle  86,787,938.60
    Total L2 Elapsed Cycles          cycle  8,762,374,656
    Average SM Active Cycles         cycle  88,568,141.38
    Total SM Elapsed Cycles          cycle 12,063,593,782
    Average SMSP Active Cycles       cycle  88,572,432.68
    Total SMSP Elapsed Cycles        cycle 48,254,375,128
    -------------------------- ----------- --------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.00
    Branch Instructions              inst   33,849,344
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 2.521%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 29360128 excessive sectors (3% of the     
          total 1107296256 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 36.92%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 2147483648 excessive wavefronts (38% of   
          the total 5637144576 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source       
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void matmul_tiled_vec<128, 128, 8, 8, 8>(const float *, const float *, float *, int, int, int) (64, 32, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          1.59
    SM Frequency                    Ghz          1.35
    Elapsed Cycles                cycle    68,068,625
    Memory Throughput                 %         73.76
    DRAM Throughput                   %          9.52
    Duration                         ms         50.42
    L1/TEX Cache Throughput           %         76.31
    L2 Cache Throughput               %         11.87
    SM Active Cycles              cycle 65,751,421.87
    Compute (SM) Throughput           %         53.74
    ----------------------- ----------- -------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L1 
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes      
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or        
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 48%  
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       128.32
    Dropped Samples                sample        2,958
    Maximum Sampling Interval          us           12
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.22
    Executed Ipc Elapsed  inst/cycle         2.15
    Issue Slots Busy               %        55.59
    Issued Ipc Active     inst/cycle         2.22
    SM Busy                        %        55.59
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (50.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       232.98
    Mem Busy                               %        73.76
    Max Bandwidth                          %        38.92
    L1/TEX Hit Rate                        %         1.46
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector   21,018,244
    L2 Hit Rate                            %        56.52
    Mem Pipes Busy                         %        14.21
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 9.262%                                                                                          
          Out of the 672583808.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 64.54%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 30.52%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 5.0 - way bank        
          conflict across all 1073741824 shared load requests.This results in 2147607616 bank conflicts,  which         
          represent 40.00% of the overall 5368833088 wavefronts for shared loads. Check the Source Counters section     
          for uncoalesced shared loads.                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 39.66%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 4.2 - way bank       
          conflict across all 134217728 shared store requests.This results in 290489468 bank conflicts,  which          
          represent 51.97% of the overall 558924924 wavefronts for shared stores. Check the Source Counters section     
          for uncoalesced shared stores.                                                                                

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        55.50
    Issued Warp Per Scheduler                        0.55
    No Eligible                            %        44.50
    Active Warps Per Scheduler          warp         3.89
    Eligible Warps Per Scheduler        warp         1.31
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.24%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 3.89 active warps per scheduler, but only an average of 1.31 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         7.01
    Warp Cycles Per Executed Instruction           cycle         7.01
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    32.00
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- --------------
    Metric Name                              Metric Unit   Metric Value
    ---------------------------------------- ----------- --------------
    Avg. Executed Instructions Per Scheduler        inst  36,552,207.52
    Executed Instructions                           inst 19,299,565,568
    Avg. Issued Instructions Per Scheduler          inst  36,552,226.52
    Issued Instructions                             inst 19,299,575,600
    ---------------------------------------- ----------- --------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             128
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           10.37
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         524,288
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                7.76
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            5
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.30
    Achieved Active Warps Per SM           warp        15.55
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.24%                                                                                          
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle   7,647,886.67
    Total DRAM Elapsed Cycles        cycle  3,855,425,152
    Average L1 Active Cycles         cycle  65,751,421.87
    Total L1 Elapsed Cycles          cycle  8,978,774,744
    Average L2 Active Cycles         cycle  67,333,000.06
    Total L2 Elapsed Cycles          cycle  6,530,535,264
    Average SM Active Cycles         cycle  65,751,421.87
    Total SM Elapsed Cycles          cycle  8,978,774,744
    Average SMSP Active Cycles       cycle  65,862,495.56
    Total SMSP Elapsed Cycles        cycle 35,915,098,976
    -------------------------- ----------- --------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst  168,067,072
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 2.624%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 29360128 excessive sectors (3% of the     
          total 1107296256 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 39.54%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 2415919104 excessive wavefronts (41% of   
          the total 5905580032 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source       
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void matmul_tiled_pipe<128, 128, 8, 8, 8>(const float *, const float *, float *, int, int, int) (64, 32, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          1.59
    SM Frequency                    Ghz          1.35
    Elapsed Cycles                cycle    98,946,786
    Memory Throughput                 %         49.55
    DRAM Throughput                   %          5.08
    Duration                         ms         73.32
    L1/TEX Cache Throughput           %         51.14
    L2 Cache Throughput               %          7.45
    SM Active Cycles              cycle 95,857,613.36
    Compute (SM) Throughput           %         37.53
    ----------------------- ----------- -------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 33%  
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        93.13
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           24
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.55
    Executed Ipc Elapsed  inst/cycle         1.50
    Issue Slots Busy               %        38.73
    Issued Ipc Active     inst/cycle         1.55
    SM Busy                        %        38.73
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       124.39
    Mem Busy                               %        49.55
    Max Bandwidth                          %        26.75
    L1/TEX Hit Rate                        %         0.19
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector   33,586,392
    L2 Hit Rate                            %        50.80
    Mem Pipes Busy                         %         9.26
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 4.841%                                                                                          
          Out of the 1074764544.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To     
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 43.36%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.46%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 5.0 - way bank        
          conflict across all 1073741824 shared load requests.This results in 2147483648 bank conflicts,  which         
          represent 40.00% of the overall 5368709120 wavefronts for shared loads. Check the Source Counters section     
          for uncoalesced shared loads.                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18%                                                                                             
          The memory access pattern for shared stores might not be optimal and causes on average a 6.2 - way bank       
          conflict across all 67108864 shared store requests.This results in 145748507 bank conflicts,  which           
          represent 35.19% of the overall 414183963 wavefronts for shared stores. Check the Source Counters section     
          for uncoalesced shared stores.                                                                                

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        38.71
    Issued Warp Per Scheduler                        0.39
    No Eligible                            %        61.29
    Active Warps Per Scheduler          warp         2.00
    Eligible Warps Per Scheduler        warp         0.60
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.45%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 2.00 active warps per scheduler, but only an average of 0.60 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         5.16
    Warp Cycles Per Executed Instruction           cycle         5.16
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.94
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 37.35%                                                                                          
          On average, each warp of this workload spends 1.9 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 37.3% of the total average of 5.2 cycles         
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- --------------
    Metric Name                              Metric Unit   Metric Value
    ---------------------------------------- ----------- --------------
    Avg. Executed Instructions Per Scheduler        inst  37,123,940.85
    Executed Instructions                           inst 19,601,440,768
    Avg. Issued Instructions Per Scheduler          inst  37,123,948.85
    Issued Instructions                             inst 19,601,444,992
    ---------------------------------------- ----------- --------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             130
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           20.74
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         524,288
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               15.52
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.50
    Achieved Active Warps Per SM           warp         8.00
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 50.45%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle   5,937,780.17
    Total DRAM Elapsed Cycles        cycle  5,606,215,680
    Average L1 Active Cycles         cycle  95,857,613.36
    Total L1 Elapsed Cycles          cycle 13,058,590,526
    Average L2 Active Cycles         cycle  47,557,199.50
    Total L2 Elapsed Cycles          cycle  9,418,276,896
    Average SM Active Cycles         cycle  95,857,613.36
    Total SM Elapsed Cycles          cycle 13,058,590,526
    Average SMSP Active Cycles       cycle  95,896,682.50
    Total SMSP Elapsed Cycles        cycle 52,234,362,104
    -------------------------- ----------- --------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst  201,621,504
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 1.285%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 29360128 excessive sectors (3% of the     
          total 1107296256 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 38.31%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 2281701376 excessive wavefronts (40% of   
          the total 5771362304 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source       
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void matmul_cute_simple<128, 128, 8, cute::TiledMMA<cute::MMA_Atom<cute::SM80_16x8x8_F32TF32TF32F32_TN>, cute::Layout<cute::tuple<cute::C<2>, cute::C<2>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<2>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>>(const float *, const float *, float *, int, int, int, T4) (32, 64, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          1.59
    SM Frequency                    Ghz          1.23
    Elapsed Cycles                cycle    32,101,076
    Memory Throughput                 %         76.42
    DRAM Throughput                   %          4.15
    Duration                         ms         25.90
    L1/TEX Cache Throughput           %         82.90
    L2 Cache Throughput               %         30.61
    SM Active Cycles              cycle 29,513,033.35
    Compute (SM) Throughput           %         19.06
    ----------------------- ----------- -------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L1 
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes      
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or        
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%   
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       135.46
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            6
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.73
    Executed Ipc Elapsed  inst/cycle         0.68
    Issue Slots Busy               %        18.32
    Issued Ipc Active     inst/cycle         0.73
    SM Busy                        %        20.67
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (20.7%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       101.59
    Mem Busy                               %        76.42
    Max Bandwidth                          %        27.58
    L1/TEX Hit Rate                        %        67.23
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    6,982,651
    L2 Hit Rate                            %        92.26
    Mem Pipes Busy                         %        12.73
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 3.916%                                                                                          
          Out of the 223444832.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 25.47%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 21.3 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 38.21%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.24
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.76
    Active Warps Per Scheduler          warp         2.85
    Eligible Warps Per Scheduler        warp         0.22
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.58%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 2.85 active warps per scheduler, but only an average of 0.22 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        15.62
    Warp Cycles Per Executed Instruction           cycle        15.62
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    32.00
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 23.58%                                                                                          
          On average, each warp of this workload spends 11.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 74.5% of the total average of 15.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- -------------
    Metric Name                              Metric Unit  Metric Value
    ---------------------------------------- ----------- -------------
    Avg. Executed Instructions Per Scheduler        inst  5,406,254.55
    Executed Instructions                           inst 2,854,502,400
    Avg. Issued Instructions Per Scheduler          inst  5,406,263.55
    Issued Instructions                             inst 2,854,507,152
    ---------------------------------------- ----------- -------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             166
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         262,144
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                5.17
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %        18.75
    Achieved Occupancy                        %        17.70
    Achieved Active Warps Per SM           warp        11.32
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 23.58%                                                                                          
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle      1,712,777
    Total DRAM Elapsed Cycles        cycle  1,980,150,016
    Average L1 Active Cycles         cycle  29,513,033.35
    Total L1 Elapsed Cycles          cycle  4,226,046,566
    Average L2 Active Cycles         cycle  32,034,020.70
    Total L2 Elapsed Cycles          cycle  3,341,655,072
    Average SM Active Cycles         cycle  29,513,033.35
    Total SM Elapsed Cycles          cycle  4,226,046,566
    Average SMSP Active Cycles       cycle  29,641,870.81
    Total SMSP Elapsed Cycles        cycle 16,904,186,264
    -------------------------- ----------- --------------

    OPT   Est. Speedup: 7.845%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 8.51% above the average, while the minimum instance value is 8.67% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.116%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 7.69% above the average, while the minimum instance value is 8.82% below    
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.845%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.51% above the average, while the minimum instance value is 8.67% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst   16,793,600
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 30.72%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 1077936128 excessive sectors (33% of the  
          total 3229614080 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void matmul_cute_smem<128, 128, 8, cute::TiledMMA<cute::MMA_Atom<cute::SM80_16x8x8_F32TF32TF32F32_TN>, cute::Layout<cute::tuple<cute::C<2>, cute::C<2>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<2>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<64>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<256>, cute::C<1>>, cute::C<64>>>, cute::tuple<cute::C<64>, cute::C<8>>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::C<128>, cute::C<4>>, cute::tuple<cute::C<4>, cute::C<1>>>, cute::tuple<cute::C<128>, cute::C<4>>>, cute::Layout<cute::tuple<cute::C<128>, cute::C<8>>, cute::tuple<cute::C<12>, cute::C<1>>>, cute::Layout<cute::tuple<cute::C<128>, cute::C<8>>, cute::tuple<cute::C<1>, cute::C<132>>>>(const float *, const float *, float *, int, int, int, T4, T5, T6, T7, T8) (32, 64, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          1.59
    SM Frequency                    Ghz          1.24
    Elapsed Cycles                cycle    22,222,510
    Memory Throughput                 %         40.61
    DRAM Throughput                   %          6.55
    Duration                         ms         17.85
    L1/TEX Cache Throughput           %         31.16
    L2 Cache Throughput               %         40.61
    SM Active Cycles              cycle 22,208,876.24
    Compute (SM) Throughput           %         27.45
    ----------------------- ----------- -------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%   
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        94.96
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            6
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.90
    Executed Ipc Elapsed  inst/cycle         0.90
    Issue Slots Busy               %        22.48
    Issued Ipc Active     inst/cycle         0.90
    SM Busy                        %        27.47
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (27.5%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       160.26
    Mem Busy                               %        40.61
    Max Bandwidth                          %        33.89
    L1/TEX Hit Rate                        %         0.41
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    5,925,655
    L2 Hit Rate                            %        94.36
    Mem Pipes Busy                         %        23.48
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 6.239%                                                                                          
          Out of the 189620960.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 15.57%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.39%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.5 - way bank        
          conflict across all 536870912 shared load requests.This results in 268506806 bank conflicts,  which           
          represent 33.34% of the overall 805377718 wavefronts for shared loads. Check the Source Counters section for  
          uncoalesced shared loads.                                                                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        22.62
    Issued Warp Per Scheduler                        0.23
    No Eligible                            %        77.38
    Active Warps Per Scheduler          warp         1.95
    Eligible Warps Per Scheduler        warp         0.25
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 59.39%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 4.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.95 active warps per scheduler, but only an average of 0.25 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.62
    Warp Cycles Per Executed Instruction           cycle         8.62
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.39
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 54.12%                                                                                          
          On average, each warp of this workload spends 4.7 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 54.1% of the total average of 8.6 cycles         
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- -------------
    Metric Name                              Metric Unit  Metric Value
    ---------------------------------------- ----------- -------------
    Avg. Executed Instructions Per Scheduler        inst  4,992,930.91
    Executed Instructions                           inst 2,636,267,520
    Avg. Issued Instructions Per Scheduler          inst  4,992,938.91
    Issued Instructions                             inst 2,636,271,744
    ---------------------------------------- ----------- -------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             188
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           10.34
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         262,144
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                7.76
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            5
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.13
    Achieved Active Warps Per SM           warp         7.76
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 59.39%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle      1,862,655
    Total DRAM Elapsed Cycles        cycle  1,365,035,264
    Average L1 Active Cycles         cycle  22,208,876.24
    Total L1 Elapsed Cycles          cycle  2,933,609,166
    Average L2 Active Cycles         cycle  23,878,302.47
    Total L2 Elapsed Cycles          cycle  2,306,248,608
    Average SM Active Cycles         cycle  22,208,876.24
    Total SM Elapsed Cycles          cycle  2,933,609,166
    Average SMSP Active Cycles       cycle  22,076,361.98
    Total SMSP Elapsed Cycles        cycle 11,734,436,664
    -------------------------- ----------- --------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst   16,793,600
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 33.22%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 541065216 excessive sectors (33% of the   
          total 1619001344 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 58.93%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 1157627904 excessive wavefronts (59% of   
          the total 1962934272 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source       
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void matmul_cute_smem<128, 128, 32, cute::TiledMMA<cute::MMA_Atom<cute::SM80_16x8x8_F32TF32TF32F32_TN>, cute::Layout<cute::tuple<cute::C<2>, cute::C<2>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<2>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<64>, cute::C<1>>, cute::C<16>>>, cute::tuple<cute::C<16>, cute::C<32>>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::C<128>, cute::C<4>>, cute::tuple<cute::C<4>, cute::C<1>>>, cute::tuple<cute::C<128>, cute::C<4>>>, cute::Layout<cute::tuple<cute::C<128>, cute::C<32>>, cute::tuple<cute::C<36>, cute::C<1>>>, cute::Layout<cute::tuple<cute::C<128>, cute::C<32>>, cute::tuple<cute::C<1>, cute::C<132>>>>(const float *, const float *, float *, int, int, int, T4, T5, T6, T7, T8) (32, 64, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          1.59
    SM Frequency                    Ghz          1.24
    Elapsed Cycles                cycle    14,338,662
    Memory Throughput                 %         53.69
    DRAM Throughput                   %         17.63
    Duration                         ms         11.53
    L1/TEX Cache Throughput           %         45.58
    L2 Cache Throughput               %         53.73
    SM Active Cycles              cycle 13,941,610.38
    Compute (SM) Throughput           %         42.51
    ----------------------- ----------- -------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%   
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       126.94
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            3
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.32
    Executed Ipc Elapsed  inst/cycle         1.29
    Issue Slots Busy               %        33.08
    Issued Ipc Active     inst/cycle         1.32
    SM Busy                        %        43.76
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (43.8%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       431.44
    Mem Busy                               %        52.51
    Max Bandwidth                          %        53.69
    L1/TEX Hit Rate                        %         0.47
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    6,107,416
    L2 Hit Rate                            %        89.04
    Mem Pipes Busy                         %        33.04
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 17.1%                                                                                           
          Out of the 195437312.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 22.14%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 15.19%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.5 - way bank        
          conflict across all 536870912 shared load requests.This results in 268465109 bank conflicts,  which           
          represent 33.34% of the overall 805336021 wavefronts for shared loads. Check the Source Counters section for  
          uncoalesced shared loads.                                                                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        33.06
    Issued Warp Per Scheduler                        0.33
    No Eligible                            %        66.94
    Active Warps Per Scheduler          warp         1.94
    Eligible Warps Per Scheduler        warp         0.40
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.31%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.94 active warps per scheduler, but only an average of 0.40 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         5.87
    Warp Cycles Per Executed Instruction           cycle         5.87
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.83
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- -------------
    Metric Name                              Metric Unit  Metric Value
    ---------------------------------------- ----------- -------------
    Avg. Executed Instructions Per Scheduler        inst  4,612,204.61
    Executed Instructions                           inst 2,435,244,032
    Avg. Issued Instructions Per Scheduler          inst  4,612,212.61
    Issued Instructions                             inst 2,435,248,256
    ---------------------------------------- ----------- -------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             190
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           35.30
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         262,144
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                7.76
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.14
    Achieved Active Warps Per SM           warp         7.77
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 46.31%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle  3,238,583.17
    Total DRAM Elapsed Cycles        cycle   881,614,848
    Average L1 Active Cycles         cycle 13,941,610.38
    Total L1 Elapsed Cycles          cycle 1,894,612,316
    Average L2 Active Cycles         cycle 15,269,490.67
    Total L2 Elapsed Cycles          cycle 1,486,812,384
    Average SM Active Cycles         cycle 13,941,610.38
    Total SM Elapsed Cycles          cycle 1,894,612,316
    Average SMSP Active Cycles       cycle 13,950,083.32
    Total SMSP Elapsed Cycles        cycle 7,578,449,264
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.00
    Branch Instructions              inst    4,210,688
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 32.95%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 541065216 excessive sectors (33% of the   
          total 1619001344 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 45.33%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 704643072 excessive wavefronts (47% of    
          the total 1509949440 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source       
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void matmul_cute_persistent<128, 128, 32, cute::TiledMMA<cute::MMA_Atom<cute::SM80_16x8x8_F32TF32TF32F32_TN>, cute::Layout<cute::tuple<cute::C<2>, cute::C<2>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<2>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<64>, cute::C<1>>, cute::C<16>>>, cute::tuple<cute::C<16>, cute::C<32>>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::C<128>, cute::C<4>>, cute::tuple<cute::C<4>, cute::C<1>>>, cute::tuple<cute::C<128>, cute::C<4>>>, cute::Layout<cute::tuple<cute::C<128>, cute::C<32>>, cute::tuple<cute::C<36>, cute::C<1>>>, cute::Layout<cute::tuple<cute::C<128>, cute::C<32>>, cute::tuple<cute::C<1>, cute::C<132>>>>(const float *, const float *, float *, int, int, int, T4, T5, T6, T7, T8) (132, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          1.59
    SM Frequency                    Ghz          1.23
    Elapsed Cycles                cycle    26,440,454
    Memory Throughput                 %         28.72
    DRAM Throughput                   %         18.31
    Duration                         ms         21.24
    L1/TEX Cache Throughput           %         26.47
    L2 Cache Throughput               %         28.73
    SM Active Cycles              cycle 24,008,899.59
    Compute (SM) Throughput           %         23.32
    ----------------------- ----------- -------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%   
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       113.64
    Dropped Samples                sample        4,209
    Maximum Sampling Interval          us            6
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.77
    Executed Ipc Elapsed  inst/cycle         0.71
    Issue Slots Busy               %        19.21
    Issued Ipc Active     inst/cycle         0.77
    SM Busy                        %        25.41
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (25.4%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       448.01
    Mem Busy                               %        28.53
    Max Bandwidth                          %        28.72
    L1/TEX Hit Rate                        %         0.34
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    7,298,822
    L2 Hit Rate                            %        82.30
    Mem Pipes Busy                         %        18.13
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 17.97%                                                                                          
          Out of the 233562304.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 12.15%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.823%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.5 - way bank        
          conflict across all 536870912 shared load requests.This results in 268435456 bank conflicts,  which           
          represent 33.33% of the overall 805306368 wavefronts for shared loads. Check the Source Counters section for  
          uncoalesced shared loads.                                                                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        19.24
    Issued Warp Per Scheduler                        0.19
    No Eligible                            %        80.76
    Active Warps Per Scheduler          warp         1.00
    Eligible Warps Per Scheduler        warp         0.19
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 71.28%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.19 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         5.21
    Warp Cycles Per Executed Instruction           cycle         5.21
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 38.91%                                                                                          
          On average, each warp of this workload spends 2.0 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 38.9% of the total average of 5.2 cycles         
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.86%                                                                                          
          On average, each warp of this workload spends 1.8 cycles being stalled waiting on a fixed latency execution   
          dependency. Typically, this stall reason should be very low and only shows up as a top contributor in         
          already highly optimized kernels. Try to hide the corresponding instruction latencies by increasing the       
          number of active warps, restructuring the code or unrolling loops. Furthermore, consider switching to         
          lower-latency instructions, e.g. by making use of fast math compiler options. This stall type represents      
          about 33.9% of the total average of 5.2 cycles between issuing two instructions.                              
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- -------------
    Metric Name                              Metric Unit  Metric Value
    ---------------------------------------- ----------- -------------
    Avg. Executed Instructions Per Scheduler        inst  4,611,833.48
    Executed Instructions                           inst 2,435,048,080
    Avg. Issued Instructions Per Scheduler          inst  4,611,842.48
    Issued Instructions                             inst 2,435,052,832
    ---------------------------------------- ----------- -------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    132
    Registers Per Thread             register/thread             188
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           35.30
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread          16,896
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.50
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.0 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %         6.25
    Achieved Active Warps Per SM           warp         4.00
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 71.28%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle   6,196,483.17
    Total DRAM Elapsed Cycles        cycle  1,624,438,272
    Average L1 Active Cycles         cycle  24,008,899.59
    Total L1 Elapsed Cycles          cycle  3,452,742,812
    Average L2 Active Cycles         cycle  28,042,216.35
    Total L2 Elapsed Cycles          cycle  2,734,384,608
    Average SM Active Cycles         cycle  24,008,899.59
    Total SM Elapsed Cycles          cycle  3,452,742,812
    Average SMSP Active Cycles       cycle  23,966,586.44
    Total SMSP Elapsed Cycles        cycle 13,810,971,248
    -------------------------- ----------- --------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.00
    Branch Instructions              inst    4,219,408
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 32.9%                                                                                           
          This kernel has uncoalesced global accesses resulting in a total of 541065216 excessive sectors (33% of the   
          total 1619001344 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 42.83%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 704643072 excessive wavefronts (47% of    
          the total 1509949440 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source       
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void matmul_cute_swizzle<128, 128, 32, 4, cute::TiledMMA<cute::MMA_Atom<cute::SM80_16x8x8_F32TF32TF32F32_TN>, cute::Layout<cute::tuple<cute::C<2>, cute::C<2>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<2>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<64>, cute::C<1>>, cute::C<16>>>, cute::tuple<cute::C<16>, cute::C<32>>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::C<128>, cute::C<4>>, cute::tuple<cute::C<4>, cute::C<1>>>, cute::tuple<cute::C<128>, cute::C<4>>>, cute::Layout<cute::tuple<cute::C<128>, cute::C<32>>, cute::tuple<cute::C<36>, cute::C<1>>>, cute::Layout<cute::tuple<cute::C<128>, cute::C<32>>, cute::tuple<cute::C<1>, cute::C<132>>>>(const float *, const float *, float *, int, int, int, T5, T6, T7, T8, T9) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          1.59
    SM Frequency                    Ghz          1.24
    Elapsed Cycles                cycle    17,766,061
    Memory Throughput                 %         43.27
    DRAM Throughput                   %          9.68
    Duration                         ms         14.27
    L1/TEX Cache Throughput           %         36.23
    L2 Cache Throughput               %         43.28
    SM Active Cycles              cycle 17,542,418.30
    Compute (SM) Throughput           %         34.41
    ----------------------- ----------- -------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%   
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        77.07
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            6
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.05
    Executed Ipc Elapsed  inst/cycle         1.04
    Issue Slots Busy               %        26.16
    Issued Ipc Active     inst/cycle         1.05
    SM Busy                        %        34.78
    -------------------- ----------- ------------

    INF   Shared is the highest-utilized pipeline (34.8%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical sum of several other pipelines which can't achieve full             
          utilization on their own. It executes 64-bit floating point and tensor operations. It's dominated by its      
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       236.89
    Mem Busy                               %        41.19
    Max Bandwidth                          %        43.27
    L1/TEX Hit Rate                        %         0.47
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    6,140,294
    L2 Hit Rate                            %        92.44
    Mem Pipes Busy                         %        26.75
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 9.276%                                                                                          
          Out of the 196489408.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 17.92%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.08%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.5 - way bank        
          conflict across all 536870912 shared load requests.This results in 268453979 bank conflicts,  which           
          represent 33.33% of the overall 805324891 wavefronts for shared loads. Check the Source Counters section for  
          uncoalesced shared loads.                                                                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        26.16
    Issued Warp Per Scheduler                        0.26
    No Eligible                            %        73.84
    Active Warps Per Scheduler          warp         1.95
    Eligible Warps Per Scheduler        warp         0.30
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 56.73%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 3.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.95 active warps per scheduler, but only an average of 0.30 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         7.44
    Warp Cycles Per Executed Instruction           cycle         7.44
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 30.53%                                                                                          
          On average, each warp of this workload spends 2.3 cycles being stalled waiting on a fixed latency execution   
          dependency. Typically, this stall reason should be very low and only shows up as a top contributor in         
          already highly optimized kernels. Try to hide the corresponding instruction latencies by increasing the       
          number of active warps, restructuring the code or unrolling loops. Furthermore, consider switching to         
          lower-latency instructions, e.g. by making use of fast math compiler options. This stall type represents      
          about 30.5% of the total average of 7.4 cycles between issuing two instructions.                              
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- -------------
    Metric Name                              Metric Unit  Metric Value
    ---------------------------------------- ----------- -------------
    Avg. Executed Instructions Per Scheduler        inst  4,588,993.94
    Executed Instructions                           inst 2,422,988,800
    Avg. Issued Instructions Per Scheduler          inst  4,588,999.94
    Issued Instructions                             inst 2,422,991,968
    ---------------------------------------- ----------- -------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             186
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           35.30
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         262,144
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                7.76
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.17
    Achieved Active Warps Per SM           warp         7.79
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 56.73%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle     2,200,885
    Total DRAM Elapsed Cycles        cycle 1,091,170,560
    Average L1 Active Cycles         cycle 17,542,418.30
    Total L1 Elapsed Cycles          cycle 2,340,403,136
    Average L2 Active Cycles         cycle 18,445,806.01
    Total L2 Elapsed Cycles          cycle 1,834,699,296
    Average SM Active Cycles         cycle 17,542,418.30
    Total SM Elapsed Cycles          cycle 2,340,403,136
    Average SMSP Active Cycles       cycle 17,542,458.22
    Total SMSP Elapsed Cycles        cycle 9,361,612,544
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.00
    Branch Instructions              inst    4,210,688
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 32.26%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 541065216 excessive sectors (33% of the   
          total 1619001344 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 46.17%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 704643072 excessive wavefronts (47% of    
          the total 1509949440 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source       
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void matmul_cute_pipe<128, 128, 32, cute::TiledMMA<cute::MMA_Atom<cute::SM80_16x8x8_F32TF32TF32F32_TN>, cute::Layout<cute::tuple<cute::C<2>, cute::C<2>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<2>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<64>, cute::C<1>>, cute::C<16>>>, cute::tuple<cute::C<16>, cute::C<32>>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::C<128>, cute::C<4>>, cute::tuple<cute::C<4>, cute::C<1>>>, cute::tuple<cute::C<128>, cute::C<4>>>, cute::Layout<cute::tuple<cute::C<128>, cute::C<32>>, cute::tuple<cute::C<36>, cute::C<1>>>, cute::Layout<cute::tuple<cute::C<128>, cute::C<32>>, cute::tuple<cute::C<1>, cute::C<132>>>>(const float *, const float *, float *, int, int, int, T4, T5, T6, T7, T8) (32, 64, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          1.59
    SM Frequency                    Ghz          1.24
    Elapsed Cycles                cycle    39,576,846
    Memory Throughput                 %         33.24
    DRAM Throughput                   %          4.18
    Duration                         ms         31.78
    L1/TEX Cache Throughput           %         34.31
    L2 Cache Throughput               %         24.75
    SM Active Cycles              cycle 38,339,882.25
    Compute (SM) Throughput           %         65.41
    ----------------------- ----------- -------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%   
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        82.51
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           12
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.17
    Executed Ipc Elapsed  inst/cycle         2.10
    Issue Slots Busy               %        54.15
    Issued Ipc Active     inst/cycle         2.17
    SM Busy                        %        67.51
    -------------------- ----------- ------------

    OPT   ALU is the highest-utilized pipeline (67.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. The pipeline is well-utilized, but might    
          become a bottleneck if more work is added. Based on the number of executed instructions, the highest          
          utilized pipeline (67.5%) is ALU. It executes integer and logic operations. Comparing the two, the overall    
          pipeline utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide      
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       102.30
    Mem Busy                               %        33.24
    Max Bandwidth                          %        27.88
    L1/TEX Hit Rate                        %        42.46
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector  349,963,709
    L2 Hit Rate                            %        93.93
    Mem Pipes Busy                         %        27.88
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 3.378%                                                                                          
          Out of the 11198838688.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To    
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 16.62%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 32.2%                                                                                           
          The memory access pattern for local loads from L1TEX might not be optimal. On average, only 1.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced local loads.                                       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 32.2%                                                                                           
          The memory access pattern for local stores to L1TEX might not be optimal. On average, only 1.0 of the 32      
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced local stores.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.77%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.5 - way bank        
          conflict across all 536870912 shared load requests.This results in 280228473 bank conflicts,  which           
          represent 34.30% of the overall 817099385 wavefronts for shared loads. Check the Source Counters section for  
          uncoalesced shared loads.                                                                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        54.02
    Issued Warp Per Scheduler                        0.54
    No Eligible                            %        45.98
    Active Warps Per Scheduler          warp         1.99
    Eligible Warps Per Scheduler        warp         0.71
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 34.59%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.99 active warps per scheduler, but only an average of 0.71 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         3.69
    Warp Cycles Per Executed Instruction           cycle         3.69
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    22.80
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 34.59%                                                                                          
          On average, each warp of this workload spends 1.5 cycles being stalled waiting on a fixed latency execution   
          dependency. Typically, this stall reason should be very low and only shows up as a top contributor in         
          already highly optimized kernels. Try to hide the corresponding instruction latencies by increasing the       
          number of active warps, restructuring the code or unrolling loops. Furthermore, consider switching to         
          lower-latency instructions, e.g. by making use of fast math compiler options. This stall type represents      
          about 40.7% of the total average of 3.7 cycles between issuing two instructions.                              
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.81%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 32.0 threads being active per cycle. This is further reduced  
          to 22.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- --------------
    Metric Name                              Metric Unit   Metric Value
    ---------------------------------------- ----------- --------------
    Avg. Executed Instructions Per Scheduler        inst  20,761,025.94
    Executed Instructions                           inst 10,961,821,696
    Avg. Issued Instructions Per Scheduler          inst  20,761,033.94
    Issued Instructions                             inst 10,961,825,920
    ---------------------------------------- ----------- --------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             255
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           70.59
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         262,144
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                7.76
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.49
    Achieved Active Warps Per SM           warp         7.99
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 34.59%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle   2,116,867.33
    Total DRAM Elapsed Cycles        cycle  2,430,374,400
    Average L1 Active Cycles         cycle  38,339,882.25
    Total L1 Elapsed Cycles          cycle  5,223,410,870
    Average L2 Active Cycles         cycle  42,611,962.94
    Total L2 Elapsed Cycles          cycle  4,113,685,056
    Average SM Active Cycles         cycle  38,339,882.25
    Total SM Elapsed Cycles          cycle  5,223,410,870
    Average SMSP Active Cycles       cycle  38,434,398.75
    Total SMSP Elapsed Cycles        cycle 20,893,643,480
    -------------------------- ----------- --------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.00
    Branch Instructions              inst   12,599,296
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 33.23%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 541065216 excessive sectors (33% of the   
          total 1619001344 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 45.21%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 704643072 excessive wavefronts (47% of    
          the total 1509949440 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source       
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void matmul_cute_wgmma<128, 128, 32, cute::TiledMMA<cute::MMA_Atom<SM90::MMA_64x128x8_F32TF32TF32_SS_TN<1, 1>>, cute::Layout<cute::tuple<cute::C<2>, cute::C<1>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<0>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<32>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<128>, cute::C<1>>, cute::C<32>>>, cute::tuple<cute::C<32>, cute::C<32>>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<32>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<128>, cute::C<1>>, cute::C<32>>>, cute::tuple<cute::C<32>, cute::C<32>>>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>>>>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>>>>>(const float *, const float *, float *, int, int, int, T4, T5, T6, T7, T8) (32, 64, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.24
    Elapsed Cycles                cycle    8,134,662
    Memory Throughput                 %        52.63
    DRAM Throughput                   %        48.11
    Duration                         ms         6.52
    L1/TEX Cache Throughput           %        44.62
    L2 Cache Throughput               %        53.12
    SM Active Cycles              cycle 7,576,449.30
    Compute (SM) Throughput           %        52.52
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%   
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        71.76
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            3
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.57
    Executed Ipc Elapsed  inst/cycle         0.55
    Issue Slots Busy               %        14.14
    Issued Ipc Active     inst/cycle         0.57
    SM Busy                        %        53.68
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (53.7%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.18
    Mem Busy                               %        44.48
    Max Bandwidth                          %        52.63
    L1/TEX Hit Rate                        %         0.63
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    8,369,954
    L2 Hit Rate                            %        67.67
    Mem Pipes Busy                         %        29.55
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 46.92%                                                                                          
          Out of the 267838528.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 21.83%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        14.13
    Issued Warp Per Scheduler                        0.14
    No Eligible                            %        85.87
    Active Warps Per Scheduler          warp         4.02
    Eligible Warps Per Scheduler        warp         0.19
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.37%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 7.1 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 4.02 active warps per scheduler, but only an average of 0.19 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        28.44
    Warp Cycles Per Executed Instruction           cycle        28.44
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.57
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 39.52%                                                                                          
          On average, each warp of this workload spends 11.2 cycles being stalled waiting for sibling warps at a CTA    
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 39.5% of the total average of 28.4 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 35.26%                                                                                          
          On average, each warp of this workload spends 10.0 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 35.3% of the total average of 28.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst 1,071,010.91
    Executed Instructions                           inst  565,493,760
    Avg. Issued Instructions Per Scheduler          inst 1,071,024.55
    Issued Instructions                             inst  565,500,962
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread              96
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           32.77
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         524,288
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                7.76
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            4
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.76
    Achieved Active Warps Per SM           warp        15.85
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 47.37%                                                                                          
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle  4,999,689.17
    Total DRAM Elapsed Cycles        cycle   498,843,648
    Average L1 Active Cycles         cycle  7,576,449.30
    Total L1 Elapsed Cycles          cycle 1,022,194,750
    Average L2 Active Cycles         cycle  8,579,252.88
    Total L2 Elapsed Cycles          cycle   843,553,440
    Average SM Active Cycles         cycle  7,576,449.30
    Total SM Elapsed Cycles          cycle 1,022,194,750
    Average SMSP Active Cycles       cycle  7,577,274.07
    Total SMSP Elapsed Cycles        cycle 4,088,779,000
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst    8,421,376
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.3784%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 4194304 excessive sectors (0% of the      
          total 1082130432 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void matmul_cute_wgmma_pipe<128, 128, 32, 0, cute::TiledMMA<cute::MMA_Atom<SM90::MMA_64x128x8_F32TF32TF32_SS_TN<1, 1>>, cute::Layout<cute::tuple<cute::C<2>, cute::C<1>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<0>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<32>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<128>, cute::C<1>>, cute::C<32>>>, cute::tuple<cute::C<32>, cute::C<32>>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<32>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<128>, cute::C<1>>, cute::C<32>>>, cute::tuple<cute::C<32>, cute::C<32>>>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>>>>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>>>>>(const float *, const float *, float *, int, int, int, T5, T6, T7, T8, T9) (32, 64, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.24
    Elapsed Cycles                cycle    5,578,308
    Memory Throughput                 %        68.36
    DRAM Throughput                   %        26.38
    Duration                         ms         4.48
    L1/TEX Cache Throughput           %        58.65
    L2 Cache Throughput               %        68.60
    SM Active Cycles              cycle 5,578,786.79
    Compute (SM) Throughput           %        72.87
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%   
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       100.66
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            3
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.95
    Executed Ipc Elapsed  inst/cycle         0.95
    Issue Slots Busy               %        23.79
    Issued Ipc Active     inst/cycle         0.95
    SM Busy                        %        72.90
    -------------------- ----------- ------------

    OPT   Shared is the highest-utilized pipeline (72.9%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical sum of several other pipelines which can't achieve full             
          utilization on their own. It executes 64-bit floating point and tensor operations. It's dominated by its      
          Tensor (FP) sub-pipeline. The pipeline is well-utilized, but might become a bottleneck if more work is        
          added. Based on the number of executed instructions, the highest utilized pipeline (16.1%) is ALU. It         
          executes integer and logic operations. Comparing the two, the overall pipeline utilization appears to be      
          caused by high-latency instructions. See the Profiling Guide                                                  
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       645.44
    Mem Busy                               %        58.62
    Max Bandwidth                          %        68.36
    L1/TEX Hit Rate                        %         0.63
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    8,350,426
    L2 Hit Rate                            %        79.85
    Mem Pipes Busy                         %        40.99
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 25%                                                                                             
          Out of the 267213632.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 29.31%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        23.94
    Issued Warp Per Scheduler                        0.24
    No Eligible                            %        76.06
    Active Warps Per Scheduler          warp         3.87
    Eligible Warps Per Scheduler        warp         0.35
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.13%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 4.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 3.87 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        16.15
    Warp Cycles Per Executed Instruction           cycle        16.15
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    29.70
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.13%                                                                                          
          On average, each warp of this workload spends 6.5 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 40.1% of the total average of 16.2 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst 1,327,072.97
    Executed Instructions                           inst  700,694,528
    Avg. Issued Instructions Per Scheduler          inst 1,327,082.97
    Issued Instructions                             inst  700,699,808
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             124
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           65.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         524,288
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                7.76
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.29
    Achieved Active Warps Per SM           warp        15.55
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.13%                                                                                          
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle  1,883,539.50
    Total DRAM Elapsed Cycles        cycle   342,737,408
    Average L1 Active Cycles         cycle  5,578,786.79
    Total L1 Elapsed Cycles          cycle   736,793,456
    Average L2 Active Cycles         cycle  5,971,041.22
    Total L2 Elapsed Cycles          cycle   577,866,816
    Average SM Active Cycles         cycle  5,578,786.79
    Total SM Elapsed Cycles          cycle   736,793,456
    Average SMSP Active Cycles       cycle  5,542,987.00
    Total SMSP Elapsed Cycles        cycle 2,947,173,824
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst   50,331,648
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.3845%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 4194304 excessive sectors (0% of the      
          total 1082130432 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void matmul_cute_wgmma_pipe<128, 128, 32, 4, cute::TiledMMA<cute::MMA_Atom<SM90::MMA_64x128x8_F32TF32TF32_SS_TN<1, 1>>, cute::Layout<cute::tuple<cute::C<2>, cute::C<1>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<0>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<32>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<128>, cute::C<1>>, cute::C<32>>>, cute::tuple<cute::C<32>, cute::C<32>>>, cute::TiledCopy<cute::Copy_Atom<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, float>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<32>>, cute::C<4>>, cute::tuple<cute::tuple<cute::C<128>, cute::C<1>>, cute::C<32>>>, cute::tuple<cute::C<32>, cute::C<32>>>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>>>>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>>>>>(const float *, const float *, float *, int, int, int, T5, T6, T7, T8, T9) (2048, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.24
    Elapsed Cycles                cycle    5,521,430
    Memory Throughput                 %        68.60
    DRAM Throughput                   %        26.72
    Duration                         ms         4.44
    L1/TEX Cache Throughput           %        59.12
    L2 Cache Throughput               %        68.73
    SM Active Cycles              cycle 5,508,564.17
    Compute (SM) Throughput           %        73.59
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%   
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       100.66
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            3
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.96
    Executed Ipc Elapsed  inst/cycle         0.96
    Issue Slots Busy               %        24.12
    Issued Ipc Active     inst/cycle         0.96
    SM Busy                        %        73.83
    -------------------- ----------- ------------

    OPT   Shared is the highest-utilized pipeline (73.8%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical sum of several other pipelines which can't achieve full             
          utilization on their own. It executes 64-bit floating point and tensor operations. It's dominated by its      
          Tensor (FP) sub-pipeline. The pipeline is well-utilized, but might become a bottleneck if more work is        
          added. Based on the number of executed instructions, the highest utilized pipeline (15.2%) is ALU. It         
          executes integer and logic operations. Comparing the two, the overall pipeline utilization appears to be      
          caused by high-latency instructions. See the Profiling Guide                                                  
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       653.75
    Mem Busy                               %        58.92
    Max Bandwidth                          %        68.60
    L1/TEX Hit Rate                        %         0.63
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    8,361,698
    L2 Hit Rate                            %        80.23
    Mem Pipes Busy                         %        41.40
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 25.32%                                                                                          
          Out of the 267574336.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 29.46%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        24.18
    Issued Warp Per Scheduler                        0.24
    No Eligible                            %        75.82
    Active Warps Per Scheduler          warp         3.89
    Eligible Warps Per Scheduler        warp         0.34
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.41%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 4.1 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 3.89 active warps per scheduler, but only an average of 0.34 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        16.11
    Warp Cycles Per Executed Instruction           cycle        16.11
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    29.70
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.41%                                                                                          
          On average, each warp of this workload spends 6.2 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 38.4% of the total average of 16.1 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst 1,328,562.42
    Executed Instructions                           inst  701,480,960
    Avg. Issued Instructions Per Scheduler          inst 1,328,575.77
    Issued Instructions                             inst  701,488,006
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread             124
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           65.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         524,288
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                7.76
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.29
    Achieved Active Warps Per SM           warp        15.55
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.41%                                                                                          
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle  1,891,521.83
    Total DRAM Elapsed Cycles        cycle   339,819,392
    Average L1 Active Cycles         cycle  5,508,564.17
    Total L1 Elapsed Cycles          cycle   729,577,294
    Average L2 Active Cycles         cycle  5,915,213.66
    Total L2 Elapsed Cycles          cycle   572,951,328
    Average SM Active Cycles         cycle  5,508,564.17
    Total SM Elapsed Cycles          cycle   729,577,294
    Average SMSP Active Cycles       cycle  5,495,626.36
    Total SMSP Elapsed Cycles        cycle 2,918,309,176
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst   50,331,648
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.3842%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 4194304 excessive sectors (0% of the      
          total 1082130432 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void matmul_cute_wgmma_tma<cute::tuple<cute::C<128>, cute::C<128>, cute::C<32>>, cute::Copy_Atom<cute::Copy_Traits<cute::SM90_TMA_LOAD, cute::C<131072>, cute::AuxTmaParams<cute::tuple<cute::ScaledBasis<cute::C<1>, 1>, cute::ScaledBasis<cute::C<1>, 0>>, const cute::Layout<cute::tuple<cute::C<32>, cute::C<128>>, cute::tuple<cute::ScaledBasis<cute::C<1>, 1>, cute::ScaledBasis<cute::C<1>, 0>>> &, const cute::Swizzle<3, 4, 3> &>>, float>, cute::Copy_Atom<cute::Copy_Traits<cute::SM90_TMA_LOAD, cute::C<131072>, cute::AuxTmaParams<cute::tuple<cute::ScaledBasis<cute::C<1>, 1>, cute::ScaledBasis<cute::C<1>, 0>>, const cute::Layout<cute::tuple<cute::C<32>, cute::C<128>>, cute::tuple<cute::ScaledBasis<cute::C<1>, 1>, cute::ScaledBasis<cute::C<1>, 0>>> &, const cute::Swizzle<3, 4, 3> &>>, float>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<3>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>, cute::tuple<cute::C<0>, cute::C<4096>>>>>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<3>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>, cute::tuple<cute::C<0>, cute::C<4096>>>>>, cute::TiledMMA<cute::MMA_Atom<SM90::MMA_64x128x8_F32TF32TF32_SS_TN<1, 1>>, cute::Layout<cute::tuple<cute::C<2>, cute::C<1>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<0>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>>(int, int, int, T2, T3, float *, T6) (32, 64, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.24
    Elapsed Cycles                cycle    5,612,550
    Memory Throughput                 %        78.78
    DRAM Throughput                   %        68.98
    Duration                         ms         4.51
    L1/TEX Cache Throughput           %        59.42
    L2 Cache Throughput               %        74.97
    SM Active Cycles              cycle 5,243,550.02
    Compute (SM) Throughput           %        74.66
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%   
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       100.66
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            3
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.88
    Executed Ipc Elapsed  inst/cycle         0.84
    Issue Slots Busy               %        21.96
    Issued Ipc Active     inst/cycle         0.88
    SM Busy                        %        77.57
    -------------------- ----------- ------------

    OPT   Tensor is the highest-utilized pipeline (77.6%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. The pipeline is well-utilized, but might become a bottleneck if more work is        
          added. Based on the number of executed instructions, the highest utilized pipeline (15.4%) is ALU. It         
          executes integer and logic operations. Comparing the two, the overall pipeline utilization appears to be      
          caused by high-latency instructions. See the Profiling Guide                                                  
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.69
    Mem Busy                               %        61.81
    Max Bandwidth                          %        78.78
    L1/TEX Hit Rate                        %        72.50
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    8,303,318
    L2 Hit Rate                            %        70.58
    Mem Pipes Busy                         %        42.07
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 67.27%                                                                                          
          Out of the 265706176.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 28.6%                                                                                           
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        21.89
    Issued Warp Per Scheduler                        0.22
    No Eligible                            %        78.11
    Active Warps Per Scheduler          warp         3.97
    Eligible Warps Per Scheduler        warp         0.33
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.22%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 4.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 3.97 active warps per scheduler, but only an average of 0.33 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        18.13
    Warp Cycles Per Executed Instruction           cycle        18.16
    Avg. Active Threads Per Warp                                29.86
    Avg. Not Predicated Off Threads Per Warp                    28.53
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 21.22%                                                                                          
          On average, each warp of this workload spends 6.4 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 35.1% of the total average of 18.1 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.22%                                                                                          
          On average, each warp of this workload spends 6.1 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 33.9% of the total average of 18.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst 1,149,663.14
    Executed Instructions                           inst  607,022,138
    Avg. Issued Instructions Per Scheduler          inst 1,151,353.53
    Issued Instructions                             inst  607,914,664
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread              90
    Shared Memory Configuration Size           Kbyte          200.70
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           98.43
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         524,288
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                3.88
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 465 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 25.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster          264
    Max Cluster Size                      block           16
    Overall GPU Occupancy                     %         1.56
    Cluster Occupancy                         %         6.25
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.81
    Achieved Active Warps Per SM           warp        15.88
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 21.22%                                                                                          
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle  4,956,204.33
    Total DRAM Elapsed Cycles        cycle   344,881,152
    Average L1 Active Cycles         cycle  5,243,550.02
    Total L1 Elapsed Cycles          cycle   719,057,864
    Average L2 Active Cycles         cycle  5,935,187.85
    Total L2 Elapsed Cycles          cycle   584,346,720
    Average SM Active Cycles         cycle  5,243,550.02
    Total SM Elapsed Cycles          cycle   719,057,864
    Average SMSP Active Cycles       cycle  5,259,936.83
    Total SMSP Elapsed Cycles        cycle 2,876,231,456
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.09
    Branch Instructions              inst   54,164,476
    Branch Efficiency                   %        96.04
    Avg. Divergent Branches                   1,982.06
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 48.75%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 4194304 excessive sectors (50% of the     
          total 8388608 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void matmul_cute_wgmma_tma<cute::tuple<cute::C<128>, cute::C<256>, cute::C<32>>, cute::Copy_Atom<cute::Copy_Traits<cute::SM90_TMA_LOAD, cute::C<131072>, cute::AuxTmaParams<cute::tuple<cute::ScaledBasis<cute::C<1>, 1>, cute::ScaledBasis<cute::C<1>, 0>>, const cute::Layout<cute::tuple<cute::C<32>, cute::C<128>>, cute::tuple<cute::ScaledBasis<cute::C<1>, 1>, cute::ScaledBasis<cute::C<1>, 0>>> &, const cute::Swizzle<3, 4, 3> &>>, float>, cute::Copy_Atom<cute::Copy_Traits<cute::SM90_TMA_LOAD, cute::C<262144>, cute::AuxTmaParams<cute::tuple<cute::ScaledBasis<cute::C<1>, 1>, cute::ScaledBasis<cute::C<1>, 0>>, const cute::Layout<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::ScaledBasis<cute::C<1>, 1>, cute::ScaledBasis<cute::C<1>, 0>>> &, const cute::Swizzle<3, 4, 3> &>>, float>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<32>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<3>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>, cute::tuple<cute::C<0>, cute::C<4096>>>>>, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<32>>, cute::tuple<cute::C<32>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<3>>>, cute::tuple<cute::tuple<cute::C<32>, cute::C<256>>, cute::tuple<cute::C<1>, cute::C<0>>, cute::tuple<cute::C<0>, cute::C<8192>>>>>, cute::TiledMMA<cute::MMA_Atom<SM90::MMA_64x128x8_F32TF32TF32_SS_TN<1, 1>>, cute::Layout<cute::tuple<cute::C<2>, cute::C<2>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<2>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>>(int, int, int, T2, T3, float *, T6) (32, 32, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.24
    Elapsed Cycles                cycle    4,415,788
    Memory Throughput                 %        70.83
    DRAM Throughput                   %        36.96
    Duration                         ms         3.55
    L1/TEX Cache Throughput           %        73.32
    L2 Cache Throughput               %        66.60
    SM Active Cycles              cycle 4,248,647.95
    Compute (SM) Throughput           %        92.48
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 0%   
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       100.66
    Dropped Samples                sample            0
    Maximum Sampling Interval          us         1.50
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.90
    Executed Ipc Elapsed  inst/cycle         0.87
    Issue Slots Busy               %        22.59
    Issued Ipc Active     inst/cycle         0.90
    SM Busy                        %        95.73
    -------------------- ----------- ------------

    OPT   Tensor is the highest-utilized pipeline (95.7%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. The pipeline is over-utilized and likely a performance bottleneck. Based on the     
          number of executed instructions, the highest utilized pipeline (17.7%) is ALU. It executes integer and logic  
          operations. Comparing the two, the overall pipeline utilization appears to be caused by high-latency          
          instructions. See the Profiling Guide                                                                         
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       904.37
    Mem Busy                               %        70.83
    Max Bandwidth                          %        69.65
    L1/TEX Hit Rate                        %        79.91
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    8,389,685
    L2 Hit Rate                            %        74.04
    Mem Pipes Busy                         %        52.02
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 35.11%                                                                                          
          Out of the 268469920.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 35.42%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        22.60
    Issued Warp Per Scheduler                        0.23
    No Eligible                            %        77.40
    Active Warps Per Scheduler          warp         3.99
    Eligible Warps Per Scheduler        warp         0.25
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.521%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 4.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 3.99 active warps per scheduler, but only an average of 0.25 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        17.64
    Warp Cycles Per Executed Instruction           cycle        17.64
    Avg. Active Threads Per Warp                                30.94
    Avg. Not Predicated Off Threads Per Warp                    29.35
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.521%                                                                                          
          On average, each warp of this workload spends 10.7 cycles being stalled waiting for sibling warps at a CTA    
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 60.9% of the total average of 17.6 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   959,738.12
    Executed Instructions                           inst  506,741,726
    Avg. Issued Instructions Per Scheduler          inst   959,794.16
    Issued Instructions                             inst  506,771,318
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  1,024
    Registers Per Thread             register/thread              90
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block          147.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread         524,288
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                7.76
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster          132
    Max Cluster Size                      block           16
    Overall GPU Occupancy                     %         0.78
    Cluster Occupancy                         %         3.12
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.90
    Achieved Active Warps Per SM           warp        15.93
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.521%                                                                                          
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle  2,091,611.17
    Total DRAM Elapsed Cycles        cycle   271,627,776
    Average L1 Active Cycles         cycle  4,248,647.95
    Total L1 Elapsed Cycles          cycle   580,530,644
    Average L2 Active Cycles         cycle  4,774,907.65
    Total L2 Elapsed Cycles          cycle   459,225,888
    Average SM Active Cycles         cycle  4,248,647.95
    Total SM Elapsed Cycles          cycle   580,530,644
    Average SMSP Active Cycles       cycle  4,247,317.09
    Total SMSP Elapsed Cycles        cycle 2,322,122,576
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst   52,062,089
    Branch Efficiency                   %        97.97
    Avg. Divergent Branches                     991.03
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 49.91%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 4194304 excessive sectors (50% of the     
          total 8388608 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void cutlass::device_kernel<gemm::GemmUniversal<cute::tuple<int, int, int, int>, gemm::CollectiveMma<gemm::MainloopSm90TmaGmmaWarpSpecialized<4, cute::tuple<cute::C<1>, cute::C<1>, cute::C<1>>, gemm::KernelTmaWarpSpecializedCooperative>, cute::tuple<cute::C<128>, cute::C<256>, cute::C<32>>, float, cute::tuple<long, cute::C<1>, long>, float, cute::tuple<long, cute::C<1>, long>, cute::TiledMMA<cute::MMA_Atom<SM90::MMA_64x256x8_F32TF32TF32_SS_TN<1, 1>>, cute::Layout<cute::tuple<cute::C<2>, cute::C<1>, cute::C<1>>, cute::tuple<cute::C<1>, cute::C<0>, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>, cute::SM90_TMA_LOAD, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::C<8>, cute::C<32>>, cute::tuple<cute::C<32>, cute::C<1>>>>, void, cute::identity, cute::SM90_TMA_LOAD, cute::ComposedLayout<cute::Swizzle<3, 4, 3>, cute::smem_ptr_flag_bits<32>, cute::Layout<cute::tuple<cute::C<8>, cute::C<32>>, cute::tuple<cute::C<32>, cute::C<1>>>>, void, cute::identity>, epilogue::Sm90TmaWarpSpecializedAdapter<epilogue::DefaultEpilogue<float, cute::tuple<long, cute::C<1>, long>, cute::tuple<long, cute::C<1>, long>, epilogue::LinearCombination<float, 1, float, float, 0, 2, float>, gemm::EpilogueDefault>>, void, void>>(T1::Params) (1, 132, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.25
    Elapsed Cycles                cycle    4,928,793
    Memory Throughput                 %        55.21
    DRAM Throughput                   %        49.99
    Duration                         ms         3.93
    L1/TEX Cache Throughput           %        55.30
    L2 Cache Throughput               %        56.93
    SM Active Cycles              cycle 4,715,583.40
    Compute (SM) Throughput           %        83.16
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved      
          close to 0% of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling     
          Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on         
          roofline analysis.                                                                                            

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       100.66
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            3
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.46
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        11.53
    Issued Ipc Active     inst/cycle         0.46
    SM Busy                        %        86.25
    -------------------- ----------- ------------

    OPT   Tensor is the highest-utilized pipeline (86.3%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. The pipeline is over-utilized and likely a performance bottleneck. Based on the     
          number of executed instructions, the highest utilized pipeline (7.0%) is Uniform. Comparing the two, the      
          overall pipeline utilization appears to be caused by high-latency instructions. See the Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Tbyte/s         1.22
    Mem Busy                               %        53.32
    Max Bandwidth                          %        55.21
    L1/TEX Hit Rate                        %        62.47
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    8,268,261
    L2 Hit Rate                            %        63.82
    Mem Pipes Busy                         %        44.23
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 48.08%                                                                                          
          Out of the 264584352.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 26.66%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 16.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        11.51
    Issued Warp Per Scheduler                        0.12
    No Eligible                            %        88.49
    Active Warps Per Scheduler          warp         2.25
    Eligible Warps Per Scheduler        warp         0.12
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 16.84%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 2.25 active warps per scheduler, but only an average of 0.12 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.51
    Warp Cycles Per Executed Instruction           cycle        19.61
    Avg. Active Threads Per Warp                                29.01
    Avg. Not Predicated Off Threads Per Warp                    29.00
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.84%                                                                                          
          On average, each warp of this workload spends 12.5 cycles being stalled waiting for sibling warps at a CTA    
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 64.2% of the total average of 19.5 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   540,606.13
    Executed Instructions                           inst  285,440,037
    Avg. Issued Instructions Per Scheduler          inst   543,597.99
    Issued Instructions                             inst  287,019,741
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 0.2849%                                                                                         
          This kernel executes 0 fused and 1069584 non-fused FP32 instructions. By converting pairs of non-fused        
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    132
    Registers Per Thread             register/thread             168
    Shared Memory Configuration Size           Kbyte          200.70
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block          196.86
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread          50,688
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.0 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            5
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %        18.75
    Achieved Occupancy                        %        14.06
    Achieved Active Warps Per SM           warp         9.00
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.84%                                                                                          
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle  3,128,862.67
    Total DRAM Elapsed Cycles        cycle   300,406,272
    Average L1 Active Cycles         cycle  4,715,583.40
    Total L1 Elapsed Cycles          cycle   645,557,260
    Average L2 Active Cycles         cycle  5,325,508.11
    Total L2 Elapsed Cycles          cycle   508,486,560
    Average SM Active Cycles         cycle  4,715,583.40
    Total SM Elapsed Cycles          cycle   645,557,260
    Average SMSP Active Cycles       cycle  4,721,943.37
    Total SMSP Elapsed Cycles        cycle 2,582,229,040
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst   30,314,861
    Branch Efficiency                   %       100.00
    Avg. Divergent Branches                       0.75
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 50.27%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 4194304 excessive sectors (50% of the     
          total 8388608 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void cutlass::Kernel2<cutlass_80_simt_sgemm_256x128_8x4_nn_align1>(T1::Params) (256, 4, 5)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          1.59
    SM Frequency                    Ghz          1.35
    Elapsed Cycles                cycle    41,792,624
    Memory Throughput                 %         47.49
    DRAM Throughput                   %          5.77
    Duration                         ms         30.96
    L1/TEX Cache Throughput           %         47.99
    L2 Cache Throughput               %         11.53
    SM Active Cycles              cycle 41,355,228.43
    Compute (SM) Throughput           %         87.37
    ----------------------- ----------- -------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 78%  
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        80.35
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           12
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         3.53
    Executed Ipc Elapsed  inst/cycle         3.49
    Issue Slots Busy               %        88.31
    Issued Ipc Active     inst/cycle         3.53
    SM Busy                        %        88.31
    -------------------- ----------- ------------

    OPT   FMA is the highest-utilized pipeline (79.2%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. The pipeline is well-utilized, but might become a bottleneck if more work is added. Based on the  
          number of executed instructions, the highest utilized pipeline (79.0%) is FMA. It executes 32-bit floating    
          point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD) operations. Comparing the two, the overall pipeline    
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload.                                                           

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       141.22
    Mem Busy                               %        47.49
    Max Bandwidth                          %        29.37
    L1/TEX Hit Rate                        %         1.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector   20,985,139
    L2 Hit Rate                            %        81.40
    Mem Pipes Busy                         %        19.87
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 4.788%                                                                                          
          Out of the 671524448.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 0.01268%                                                                                        
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 32.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.01014%                                                                                        
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 32.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        88.35
    Issued Warp Per Scheduler                        0.88
    No Eligible                            %        11.65
    Active Warps Per Scheduler          warp         2.00
    Eligible Warps Per Scheduler        warp         1.60
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         2.26
    Warp Cycles Per Executed Instruction           cycle         2.26
    Avg. Active Threads Per Warp                                32.00
    Avg. Not Predicated Off Threads Per Warp                    31.86
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- --------------
    Metric Name                              Metric Unit   Metric Value
    ---------------------------------------- ----------- --------------
    Avg. Executed Instructions Per Scheduler        inst  36,520,112.48
    Executed Instructions                           inst 19,282,619,392
    Avg. Issued Instructions Per Scheduler          inst  36,520,160.06
    Issued Instructions                             inst 19,282,644,513
    ---------------------------------------- ----------- --------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  5,120
    Registers Per Thread             register/thread             202
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           49.15
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread       1,310,720
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               38.79
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.49
    Achieved Active Warps Per SM           warp         8.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 87.5%                                                                                     
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- --------------
    Metric Name                Metric Unit   Metric Value
    -------------------------- ----------- --------------
    Average DRAM Active Cycles       cycle      2,846,223
    Total DRAM Elapsed Cycles        cycle  2,367,164,928
    Average L1 Active Cycles         cycle  41,355,228.43
    Total L1 Elapsed Cycles          cycle  5,517,408,998
    Average L2 Active Cycles         cycle  41,417,559.74
    Total L2 Elapsed Cycles          cycle  4,009,201,824
    Average SM Active Cycles         cycle  41,355,228.43
    Total SM Elapsed Cycles          cycle  5,517,408,998
    Average SMSP Active Cycles       cycle  41,336,355.14
    Total SMSP Elapsed Cycles        cycle 22,069,635,992
    -------------------------- ----------- --------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.00
    Branch Instructions              inst   17,339,392
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 26.62%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 671089820 excessive wavefronts (27% of    
          the total 2494007024 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source       
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  sm90_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize128x256x32_warpgroupsize2x1x1_execute_segment_k_off_kernel__5x_cublas (66, 2, 1)x(384, 1, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.24
    Elapsed Cycles                cycle    4,321,743
    Memory Throughput                 %        71.83
    DRAM Throughput                   %        31.21
    Duration                         ms         3.46
    L1/TEX Cache Throughput           %        74.34
    L2 Cache Throughput               %        57.66
    SM Active Cycles              cycle 4,145,638.30
    Compute (SM) Throughput           %        94.80
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved      
          close to 0% of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling     
          Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on         
          roofline analysis.                                                                                            

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte       100.66
    Dropped Samples                sample            0
    Maximum Sampling Interval          us         1.50
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.80
    Executed Ipc Elapsed  inst/cycle         0.77
    Issue Slots Busy               %        19.95
    Issued Ipc Active     inst/cycle         0.80
    SM Busy                        %        98.11
    -------------------- ----------- ------------

    OPT   Tensor is the highest-utilized pipeline (98.1%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. The pipeline is over-utilized and likely a performance bottleneck. Based on the     
          number of executed instructions, the highest utilized pipeline (12.6%) is LSU. It executes load/store memory  
          operations. Comparing the two, the overall pipeline utilization appears to be caused by high-latency          
          instructions. See the Profiling Guide                                                                         
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       763.62
    Mem Busy                               %        71.83
    Max Bandwidth                          %        59.27
    L1/TEX Hit Rate                        %        88.11
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                   %            0
    L2 Compression Input Sectors      sector    4,223,889
    L2 Hit Rate                            %        74.25
    Mem Pipes Busy                         %        50.37
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 29.52%                                                                                          
          Out of the 135164448.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To      
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 62.85%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 69.59%                                                                                          
          The memory access pattern for local loads from L1TEX might not be optimal. On average, only 1.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced local loads.                                       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 69.59%                                                                                          
          The memory access pattern for local stores to L1TEX might not be optimal. On average, only 1.0 of the 32      
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced local stores.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.2%                                                                                           
          The memory access pattern for shared loads might not be optimal and causes on average a 3.9 - way bank        
          conflict across all 67138410 shared load requests.This results in 67111455 bank conflicts,  which represent   
          25.83% of the overall 259798732 wavefronts for shared loads. Check the Source Counters section for            
          uncoalesced shared loads.                                                                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        19.95
    Issued Warp Per Scheduler                        0.20
    No Eligible                            %        80.05
    Active Warps Per Scheduler          warp         2.97
    Eligible Warps Per Scheduler        warp         0.21
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.202%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 2.97 active warps per scheduler, but only an average of 0.21 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        14.87
    Warp Cycles Per Executed Instruction           cycle        14.87
    Avg. Active Threads Per Warp                                32.00
    Avg. Not Predicated Off Threads Per Warp                    30.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.202%                                                                                          
          On average, each warp of this workload spends 7.0 cycles being stalled waiting for sibling warps at a CTA     
          barrier. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a      
          barrier. This causes some warps to wait a long time until other warps reach the synchronization point.        
          Whenever possible, try to divide up the work into blocks of uniform workloads. If the block size is 512       
          threads or greater, consider splitting it into smaller groups. This can increase eligible warps without       
          affecting occupancy, unless shared memory becomes a new occupancy limiter. Also, try to identify which        
          barrier instruction causes the most stalls, and optimize the code executed before that synchronization point  
          first. This stall type represents about 47.1% of the total average of 14.9 cycles between issuing two         
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.202%                                                                                          
          On average, each warp of this workload spends 4.5 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 30.2% of the total average of 14.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   826,798.74
    Executed Instructions                           inst  436,549,733
    Avg. Issued Instructions Per Scheduler          inst   827,106.31
    Issued Instructions                             inst  436,712,134
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   2
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    132
    Registers Per Thread             register/thread             168
    Shared Memory Configuration Size           Kbyte          233.47
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block          231.42
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             132
    Stack Size                                                 1,024
    Threads                                   thread          50,688
    # TPCs                                                        66
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.0 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster           66
    Max Cluster Size                      block           16
    Overall GPU Occupancy                     %         0.59
    Cluster Occupancy                         %         3.12
    Block Limit Barriers                  block            4
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            5
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %        18.75
    Achieved Occupancy                        %        18.54
    Achieved Active Warps Per SM           warp        11.86
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.202%                                                                                          
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle  1,719,583.17
    Total DRAM Elapsed Cycles        cycle   264,478,848
    Average L1 Active Cycles         cycle  4,145,638.30
    Total L1 Elapsed Cycles          cycle   566,331,138
    Average L2 Active Cycles         cycle  4,654,919.25
    Total L2 Elapsed Cycles          cycle   446,599,488
    Average SM Active Cycles         cycle  4,145,638.30
    Total SM Elapsed Cycles          cycle   566,331,138
    Average SMSP Active Cycles       cycle  4,146,103.30
    Total SMSP Elapsed Cycles        cycle 2,265,324,552
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst   11,755,228
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                       7.48
    ------------------------- ----------- ------------

    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.06%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 67108864 excessive wavefronts (17% of the 
          total 403752742 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

